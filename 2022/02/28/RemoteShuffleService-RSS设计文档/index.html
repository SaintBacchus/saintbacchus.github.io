<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"yoursite.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Shuffle Service设计文档 该文档写于2021年1月  项目背景业务场景目前公司内部有3个集群, 最大集群规模已经达到8000多台机器, 总共有近45万核, 1.23PB的内存.待补完… Spark业务负载目前集群上, 每天有60万个离线任务在运行, 其中Spark任务有10万个. 公司未来整体战略会将所有HiveMR的任务替换Spark任务, 今年将整个数据平台上的SQL任务基本上都">
<meta property="og:type" content="article">
<meta property="og:title" content="[RemoteShuffleService]: RSS设计文档 ">
<meta property="og:url" content="http://yoursite.com/2022/02/28/RemoteShuffleService-RSS%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/index.html">
<meta property="og:site_name" content="Carlmartin&#39; Blog">
<meta property="og:description" content="Shuffle Service设计文档 该文档写于2021年1月  项目背景业务场景目前公司内部有3个集群, 最大集群规模已经达到8000多台机器, 总共有近45万核, 1.23PB的内存.待补完… Spark业务负载目前集群上, 每天有60万个离线任务在运行, 其中Spark任务有10万个. 公司未来整体战略会将所有HiveMR的任务替换Spark任务, 今年将整个数据平台上的SQL任务基本上都">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172007077.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172021282.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172040055.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172054924.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172110777.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172158952.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172216824.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172232885.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172249213.png">
<meta property="og:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172327830.png">
<meta property="article:published_time" content="2022-02-28T09:19:01.000Z">
<meta property="article:modified_time" content="2024-03-06T08:19:03.856Z">
<meta property="article:author" content="Carlmartin">
<meta property="article:tag" content="技术">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172007077.png">


<link rel="canonical" href="http://yoursite.com/2022/02/28/RemoteShuffleService-RSS%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://yoursite.com/2022/02/28/RemoteShuffleService-RSS%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/","path":"2022/02/28/RemoteShuffleService-RSS设计文档/","title":"[RemoteShuffleService]: RSS设计文档 "}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>[RemoteShuffleService]: RSS设计文档  | Carlmartin' Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Carlmartin' Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">In me the tiger sniffs the rose.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Shuffle-Service%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3"><span class="nav-number">1.</span> <span class="nav-text">Shuffle Service设计文档</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF"><span class="nav-number">1.1.</span> <span class="nav-text">项目背景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%9A%E5%8A%A1%E5%9C%BA%E6%99%AF"><span class="nav-number">1.1.1.</span> <span class="nav-text">业务场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark%E4%B8%9A%E5%8A%A1%E8%B4%9F%E8%BD%BD"><span class="nav-number">1.1.2.</span> <span class="nav-text">Spark业务负载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%93%E5%89%8D%E9%9D%A2%E4%B8%B4%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.3.</span> <span class="nav-text">当前面临的问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%9A%E7%95%8C%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F"><span class="nav-number">1.1.4.</span> <span class="nav-text">业界的解决方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87%E5%8F%8A%E8%8C%83%E5%9B%B4"><span class="nav-number">1.1.5.</span> <span class="nav-text">设计目标及范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shuffle-Service%E9%A1%B6%E5%B1%82%E8%AE%BE%E8%AE%A1"><span class="nav-number">1.2.</span> <span class="nav-text">Shuffle Service顶层设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="nav-number">1.2.1.</span> <span class="nav-text">服务架构图</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Remote-Shuffle-Service-Manager"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">Remote Shuffle Service Manager</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Remote-Shuffle-Service-Server"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Remote Shuffle Service Server</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-App"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">Spark App</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Zookeeper"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">Zookeeper</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DFS"><span class="nav-number">1.2.1.5.</span> <span class="nav-text">DFS</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%A4%E4%BA%92%E5%9B%BE%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="nav-number">1.2.2.</span> <span class="nav-text">交互图架构图</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6%E6%B5%81%E5%9B%BE"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">控制流图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">数据流图</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Remote-Shuffle-Service%E8%AE%BE%E8%AE%A1%E8%A6%81%E7%82%B9"><span class="nav-number">1.3.</span> <span class="nav-text">Remote Shuffle Service设计要点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="nav-number">1.3.1.</span> <span class="nav-text">可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">任务可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#PushData%E5%A4%B1%E8%B4%A5-%E5%A4%9A%E6%AC%A1%E5%A4%B1%E8%B4%A5"><span class="nav-number">1.3.1.1.0.1.</span> <span class="nav-text">PushData失败&#x2F;多次失败</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%85%B3%E4%BA%8EBlock%E9%87%8D%E8%AF%95%E7%9A%84%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E"><span class="nav-number">1.3.1.1.0.2.</span> <span class="nav-text">关于Block重试的详细说明</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%87%8D%E5%A4%8D"><span class="nav-number">1.3.1.1.1.</span> <span class="nav-text">数据重复</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8B%89%E5%8F%96%E5%A4%B1%E8%B4%A5"><span class="nav-number">1.3.1.1.2.</span> <span class="nav-text">拉取失败</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Server%E5%A4%B1%E8%B4%A5"><span class="nav-number">1.3.1.1.3.</span> <span class="nav-text">Server失败</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">数据一致性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">数据可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%B0%81%E6%9D%A5%E5%A4%87%E4%BB%BD"><span class="nav-number">1.3.1.3.1.</span> <span class="nav-text">谁来备份</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E5%A4%87%E4%BB%BD"><span class="nav-number">1.3.1.3.2.</span> <span class="nav-text">怎么备份</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%87%E4%BB%BD%E5%88%B0%E5%93%AA%E5%84%BF"><span class="nav-number">1.3.1.3.3.</span> <span class="nav-text">备份到哪儿</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E7%94%A8%E6%80%A7"><span class="nav-number">1.3.2.</span> <span class="nav-text">可用性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">主备切换</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2-%E4%BB%BB%E5%8A%A1%E4%B8%8D%E4%B8%AD%E6%96%AD"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">主备切换, 任务不中断</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7"><span class="nav-number">1.3.3.</span> <span class="nav-text">监控</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A1%B5%E9%9D%A2"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">页面</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%85%A2%E8%8A%82%E7%82%B9%E5%91%8A%E8%AD%A6"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">慢节点告警</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2"><span class="nav-number">1.3.4.</span> <span class="nav-text">部署</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%BC%E5%AE%B9%E6%80%A7%E6%96%B9%E6%A1%88"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">兼容性方案</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E7%89%88%E6%9C%AC%E6%94%AF%E6%8C%81"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">多版本支持</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88"><span class="nav-number">1.3.4.3.</span> <span class="nav-text">部署方案</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%87%E7%BA%A7%E6%96%B9%E6%A1%88"><span class="nav-number">1.3.4.4.</span> <span class="nav-text">升级方案</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Zookeeper%E8%8A%82%E7%82%B9%E5%88%87%E6%8D%A2"><span class="nav-number">1.3.4.4.1.</span> <span class="nav-text">Zookeeper节点切换</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E5%85%A8"><span class="nav-number">1.3.5.</span> <span class="nav-text">安全</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%89%B4%E6%9D%83"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">鉴权</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">数据完整性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86"><span class="nav-number">1.3.5.3.</span> <span class="nav-text">数据清理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E5%BA%A6"><span class="nav-number">1.3.6.</span> <span class="nav-text">调度</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.6.1.</span> <span class="nav-text">调度算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataLocation"><span class="nav-number">1.3.6.2.</span> <span class="nav-text">DataLocation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD"><span class="nav-number">1.3.7.</span> <span class="nav-text">性能</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%A4%9A%E7%A3%81%E7%9B%98%E5%86%99%E5%85%A5"><span class="nav-number">1.3.7.1.</span> <span class="nav-text">支持多磁盘写入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E6%95%B0%E6%8D%AE%E5%9D%97%E5%90%88%E5%B9%B6%E5%8F%91%E9%80%81"><span class="nav-number">1.3.7.2.</span> <span class="nav-text">小数据块合并发送</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E6%B5%81%E5%BC%8F%E8%AF%BB%E5%8F%96"><span class="nav-number">1.3.7.3.</span> <span class="nav-text">文件流式读取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B0%8F%E6%95%B0%E6%8D%AE%E5%9D%97%E5%90%88%E5%B9%B6%E8%AF%BB%E5%8F%96"><span class="nav-number">1.3.7.4.</span> <span class="nav-text">小数据块合并读取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2GB%E9%99%90%E5%88%B6"><span class="nav-number">1.3.7.5.</span> <span class="nav-text">2GB限制</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Carlmartin</p>
  <div class="site-description" itemprop="description">A place for codeing break.</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">57</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/02/28/RemoteShuffleService-RSS%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Carlmartin">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Carlmartin' Blog">
      <meta itemprop="description" content="A place for codeing break.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="[RemoteShuffleService]: RSS设计文档  | Carlmartin' Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [RemoteShuffleService]: RSS设计文档 
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-28 17:19:01" itemprop="dateCreated datePublished" datetime="2022-02-28T17:19:01+08:00">2022-02-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-06 16:19:03" itemprop="dateModified" datetime="2024-03-06T16:19:03+08:00">2024-03-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/" itemprop="url" rel="index"><span itemprop="name">技术文章</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Shuffle-Service设计文档"><a href="#Shuffle-Service设计文档" class="headerlink" title="Shuffle Service设计文档"></a>Shuffle Service设计文档</h1><blockquote>
<p>该文档写于2021年1月</p>
</blockquote>
<h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><h3 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h3><p>目前公司内部有3个集群, 最大集群规模已经达到8000多台机器, 总共有近45万核, 1.23PB的内存.<br>待补完…</p>
<h3 id="Spark业务负载"><a href="#Spark业务负载" class="headerlink" title="Spark业务负载"></a>Spark业务负载</h3><p>目前集群上, 每天有60万个离线任务在运行, 其中Spark任务有10万个. 公司未来整体战略会将所有HiveMR的任务替换Spark任务, 今年将整个数据平台上的SQL任务基本上都迁移到Spark上.<br>整体的资源利用率得到了巨大的提升, 但随着Spark应用越来越多, 任务的稳定性受到了越来越多的挑战, 尤其是Spark Shuffle这块的问题, 总是让运维人员头疼不已, 一旦当天数据量超过历史, 会导致任务失败, 就需要隔离机器重新运行, 等业务量降低时候, 再释放空闲资源, 给整个团队巨大的运维成本.</p>
<h3 id="当前面临的问题"><a href="#当前面临的问题" class="headerlink" title="当前面临的问题"></a>当前面临的问题</h3><p>我们分析Spark Shuffle的问题, 发现目前Shuffle机制存在如下几个问题:</p>
<ol>
<li>计算存储不分离, Spark计算和Shuffle的IO操作混在一起, 极易因为CPU过高导致Shuffle超时失败</li>
<li>机器超卖, 虽然提高了整体使用率, 但是会因为调度不均匀, 导致失败CPU飙升</li>
<li>Yarn Shuffle Service内嵌于NodeManager之中, 无法给全部集群替换SSD加速IO效率</li>
<li>Pull模式的Shuffle, Reduce阶段有大量的随机读取过程, 导致磁盘IO飙升</li>
<li>Reduce阶段数据未提前聚会, 导致导致大量网络小包产生, 造成网络IO飙升</li>
<li>调度器无法识别CPU/IO繁忙状态, 导致任务依然下发到近乎满载的机器</li>
</ol>
<h3 id="业界的解决方式"><a href="#业界的解决方式" class="headerlink" title="业界的解决方式"></a>业界的解决方式</h3><p>随着Spark成为业界的批处理的标准, 在各大互联网公司之中, Spark Shuffle以上的问题都慢慢出现, 无法通过简单的调优解决这个问题也慢慢成为这个大家的共识. 因此各个公司都推出了自己的解决方案:</p>
<ol>
<li>Spark社区和LinkedIn: 领英在VLDB2020发了一篇Paper, 提出<code>Push based Shuffle</code>来解决4和5点, 该方案依然嵌入在原有的Spark框架之中, 因此该方案最容易被社区接受, 现在也慢慢的在合入代码</li>
<li>Uber: 在今年的Spark Meetup里提出了他们的独立的Shuffle Service的解决方案, 也是目前唯一开源的独立Shuffle Service服务, 意图解决1-5点问题, 并引入SSD解决IO问题. 但目前开源出来的代码健壮性不强, Task重试的时候, 数据会出现冗余,还需要很长的优化之路</li>
<li>Facebook: 也有名为cosico的独立Shuffle Service, 从架构图上看他们解决了4-6的问题, 但容错使用了DFS系统, Reduce数据也是从DFS直接获取的, 因此性能是打问号的, 且未公开源码, 并不确定具体如何实现</li>
<li>阿里云: 20年12月份, 阿里云EMR团队公布了他们的Shuffle Service的方案, 同样未开源, 从公开文章上看, 他们除了解决Shuffle稳定性问题(1-5点), 更重要的点是为了解决Spark on k8s在磁盘上性能解决方案. 同样阿里云的方案也未开源</li>
<li>Intel: 作为一家硬件厂商, 推出了自己的Shuffle Service, 目的是为来卖RDMA产品, 暂时应该不会去采购硬件, 因此略过</li>
</ol>
<h3 id="设计目标及范围"><a href="#设计目标及范围" class="headerlink" title="设计目标及范围"></a>设计目标及范围</h3><p>简单调研几家产品之后, 我们打算博采众长, 并找到符合自己的场景的方案, 主要参考Uber和阿里云:</p>
<ul>
<li>独立的Shuffle服务, 存储计算分离(解决1-2)</li>
<li>SSD存储, 加速IO效率, 减少计算资源, 提高整体资源利用率(解决3)</li>
<li>Push based Shuffle, Server端聚合后再Reduce(解决4-5)</li>
</ul>
<p>相对于阿里云, 我们不需要做的:</p>
<ul>
<li>不需要全部替换原有Shuffle Service, 能用Adaptive的方式, 决策使用哪种Shuffle模式</li>
<li>最好是CloudNative的方案, 但是不强制</li>
</ul>
<p>而相对于他们两个的方案, 我需要:</p>
<ul>
<li>IO敏感型调度, 解决热点问题</li>
</ul>
<h2 id="Shuffle-Service顶层设计"><a href="#Shuffle-Service顶层设计" class="headerlink" title="Shuffle Service顶层设计"></a>Shuffle Service顶层设计</h2><h3 id="服务架构图"><a href="#服务架构图" class="headerlink" title="服务架构图"></a>服务架构图</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172007077.png" alt="image-20220228172007077"></p>
<p>整体架构会采用典型的Master-Slave模式, 其中Client为Spark App, master为Remote Shuffle Service Manager(RssManager), Slave为Remote Shuffle Service Server(RssServer), 还有DFS作为数据备份, Zookeeper作为元数据存储.</p>
<h4 id="Remote-Shuffle-Service-Manager"><a href="#Remote-Shuffle-Service-Manager" class="headerlink" title="Remote Shuffle Service Manager"></a>Remote Shuffle Service Manager</h4><p>作为整个集群的控制节点, 负责整个集群与客户端的任务调度, 元数据管理等主要工作. </p>
<ul>
<li>Manager会采用主备方式完成服务高可用, 主备实例会通过Zookeeper进行选主工作</li>
<li>Manager会采取无状态应用模式, 整体状态都会同步到Zookeeper之中, Slave可以在重启时重建全部的状态</li>
<li>为了加速备实例的启动速度, 主备实例会保持心跳, 并定时同步全量或增量状态给备节点</li>
<li>Manager与Server会保持心跳状态, Server通过心跳上报资源容量信息. </li>
<li>Manager会主动想Server下发RPC通信</li>
<li>客户端会与Manager建立连接发送RPC请求, 但是Manager不会主动给客户端发送RPC请求</li>
<li>Manager与资源调取器(Yarn/K8s)保持连接, 查询任务状态, 用以退出清理</li>
<li>Manager内部设有调度器组件, 可以插件式的设置调度策略</li>
<li>Manager内部有元数据管理, 主要是App级别的信息, Task级别的信息由Driver持有, 通过RPC请求送达Manager</li>
<li>Manager服务有WebUI接口, 统计App级别监控信息</li>
<li>Manager服务有jmx接口, 指标方便指标统计</li>
</ul>
<h4 id="Remote-Shuffle-Service-Server"><a href="#Remote-Shuffle-Service-Server" class="headerlink" title="Remote Shuffle Service Server"></a>Remote Shuffle Service Server</h4><p>作为整个集群的数据节点, 负责与客户端Spark App进行数据交互</p>
<ul>
<li>Server是数据节点, 只会跟客户端进行数据通讯, 不参与控制通信</li>
<li>Server与Manager会保持心跳, 同时会与Manager有控制流的RPC请求</li>
<li>Server会将数据首先写入到本地的SSD, 如果开启了备份的话, 也会以pipeline的方式, 同步给其他备份节点, 最后开启了慢写入的话, 才会备份给DFS. 由于DFS一般有HDD磁盘组成, 因此性能会受到巨大的影响</li>
<li>默认情况下, 一个Reduce任务会开一个文件句柄, 用于写入Map的数据, 同时还有一个index文件记录数据块位置与TaskId的对应关系</li>
<li>读取数据时候, 会根据TaskId过滤无效的数据块</li>
</ul>
<h4 id="Spark-App"><a href="#Spark-App" class="headerlink" title="Spark App"></a>Spark App</h4><p>作为整个集群的客户端, 负责实现Spark对外的接口, 并与RSS交互, 该模块除了实现SparkShuffleManager的接口之外, 还要实现Spark App的事件处理, 还需要在调度上处理任务失败的情况</p>
<ul>
<li>Shuffle接口: 实现Shuffle注册, 数据读取, 数据写入等工作</li>
<li>Event接口: 实现整体控制流的处理, 例如App启停, Stage启停</li>
<li>Schedule接口: 当任务失败时的, 需要重试, push Shuffle的重试方式不同, 目前Spark并没有有接口暴露, 可能会侵入式的修改源码</li>
</ul>
<h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><p>作为整个集群的元数据中心, ZK承担着元数据备份, 服务发现, 主备切换等功能</p>
<ul>
<li>Manager会将App信息写入到ZK, 方便备实例恢复</li>
<li>Manager通过ZK进行主备切换和监控</li>
<li>Client会通过ZK的地址获取到Manager的地址</li>
</ul>
<h4 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h4><p>作为整个集群的数据备份中心, 目前公司内部只有HDFS作为DFS实现, 但是性能堪忧.<br>但当RSS集群也是以HDD磁盘为主时, Reduce任务直接获取DFS上的数据, 是个好的选择.<br>另外DFS方式的稳定性会更加好</p>
<h3 id="交互图架构图"><a href="#交互图架构图" class="headerlink" title="交互图架构图"></a>交互图架构图</h3><p>下面考虑一下RSS是如何和Spark的各个实例交互的, 先一个Map和Reduce任务是如何进行控制流通信的, 然后再看数据是如何写入的</p>
<h4 id="控制流图"><a href="#控制流图" class="headerlink" title="控制流图"></a>控制流图</h4><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172021282.png" alt="image-20220228172021282"></p>
<ol>
<li>Spark Driver向Manager发送申请Shuffle资源的请求</li>
<li>Manager返回结果, 并指明ReduceTaskId对应的Server地址</li>
<li>Driver根据位置分配Spark任务</li>
<li>SparkTask计算各自的数据, 发送到对应Server</li>
<li>Task写完所有的Map数据之后, Executor向DriverCommit任务</li>
<li>完成所有任务的时候, Driver向Manager发送CommitStage请求, 目的是传递最后成功的TaskId给到ShuffleManager</li>
<li>Manager将TaskId等消息,下发到各个Server</li>
<li>Driver下发Reduce任务(如果也是ShuffleMapTask的话, 依然需要先申请资源)</li>
<li>ReduceTask向对应的Server拉去数据, Server需要根据TaskId过滤重复数据</li>
</ol>
<h4 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h4><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172040055.png" alt="image-20220228172040055"></p>
<ul>
<li>MapTask先会将数据完成预聚合, 按照Partition分区</li>
<li>然后两个MapTask都将将P0,P1的数据推送到第一个Server</li>
<li>Server会将两个MapTask的数据都写入到一个文件之中, 因此第一个Server有P0和P1两个文件, 第二个Server有P2,P3,P4三个文件</li>
<li>启动ReduceTask时候, 直接会去对应Server流式拉取数据</li>
</ul>
<h2 id="Remote-Shuffle-Service设计要点"><a href="#Remote-Shuffle-Service设计要点" class="headerlink" title="Remote Shuffle Service设计要点"></a>Remote Shuffle Service设计要点</h2><h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>可靠性从三个方面来阐述, 一个是任务可靠性, 指异常发生的整体重试机制, 这里的重试指流程的重试, 而不是消息的重试, 因为RPC消息大多数可以做到幂等, 做不到幂等的就会出现不一致场景, 就是数据一致性问题. 另外一点数据可靠性, 只Shuffle数据文件的备份问题.</p>
<h4 id="任务可靠性"><a href="#任务可靠性" class="headerlink" title="任务可靠性"></a>任务可靠性</h4><p>首先我们还是回到上面的数据流图, 并标识其中可能失败的场景<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172054924.png" alt="image-20220228172054924"><br>下面分别解释一下错误的具体含义:</p>
<ol>
<li>MapDataPush失败值, MapTask之中单个Partition数据无法推送到对应的Server, 造成的原因一般为网络问题等</li>
<li>慢节点/坏节点, 当出现这种场景的时候, 是多个MapTask推送到同一个Server时候的出现</li>
<li>MapTask失败, 这个场景为部分Partition数据写入成功, 部分写入失败场景, 造成错误的原因很多, 比如Executor OOM</li>
<li>Map推测执行, 指启动两个Task计算同一份数据, 会导致所有数据写了2份</li>
<li>Reduce拉取失败, 指单次失败, 可能网络问题导致</li>
<li>Reduce持续失败, 拉取多次失败, 可能是慢节点等</li>
<li>Server失败重启, 进程级别的重启, 服务多一会时间会恢复</li>
<li>Server丢失数据, 一般是磁盘问题, 导致数据丢失了</li>
</ol>
<h6 id="PushData失败-多次失败"><a href="#PushData失败-多次失败" class="headerlink" title="PushData失败/多次失败"></a>PushData失败/多次失败</h6><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172110777.png" alt="image-20220228172110777"></p>
<ol>
<li>启动MapTask, Task之中携带Reduce对应Server地址, 一个Reduce对应2个地址, 正常情况只会使用前面一个</li>
<li>MapTask计算Partition任务会将数据写入到临时</li>
<li>向Server写入数据</li>
<li>写入失败,返回错误</li>
<li>从临时文件之中获取Partition数据</li>
<li>重试写入(第一次失败的时候, 依然写入老地址, 第二次失败时, 写入备用地址)</li>
<li>重试, 直到达到达到最大次数</li>
<li>如果重试未成功, 将失败的节点加入黑名单, 然后重新运行整个Stage</li>
<li>如果重试成功, 启动Reduce任务, 此时需要指明备节点是否存有数据.</li>
<li>此时对应整个Reduce可能两个节点都存在数据, 因此需要向两个节点拉取数据</li>
</ol>
<blockquote>
<p>PushData的Block粒度为单个Partition, 如果再小, 那么数据一致性就无法保证了, 这里编程时候需要注意</p>
</blockquote>
<h6 id="关于Block重试的详细说明"><a href="#关于Block重试的详细说明" class="headerlink" title="关于Block重试的详细说明"></a>关于Block重试的详细说明</h6><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172158952.png" alt="image-20220228172158952"></p>
<ol>
<li>网络写失败, 无妨, 重试即可</li>
<li>写本地失败, 写了一些, 需要Stage失败</li>
<li>写commit文件失败, 需要Stage失败</li>
<li>网络返回失败, Server做到幂等</li>
</ol>
<p>2和3的问题, 大致是因为服务节点问题导致的, 这个目前比较难以处理, 先不管了[TODO]<br>如果做到幂等, 需要在元数据信息里面记载对应的taskAttemptId</p>
<h5 id="数据重复"><a href="#数据重复" class="headerlink" title="数据重复"></a>数据重复</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172216824.png" alt="image-20220228172216824"></p>
<ol>
<li>Driver启动Map任务</li>
<li>开始写Partition数据, 成功写入P0</li>
<li>但P1数据还没有完成写入</li>
<li>此时Task任务遇到异常退出</li>
<li>Driver启动Task任务重试</li>
<li>Task又重复写入一遍P0数据</li>
<li>同时也完成P1数据的写入, 此时系统之中有2份P0, 1份P1. 由于写入数据时, 含有Task信息, 因此Server知道2份P0数据, 分别由哪个Task写入</li>
<li>Executor向Driver 确认成功的Task</li>
<li>Driver向Manager发送Map任务也完成了, 并将正常完成任务的TaskId信息发给Manager</li>
<li>Manager直接将这些信息下发到Server</li>
<li>Driver启动Reduce任务</li>
<li>Reduce任务拉取数据的时候, 如果发现数据块对应的TaskId不在commitTask列表之中, 就会自动跳过这个数据块</li>
</ol>
<blockquote>
<p>commitTask下发给RSS的话, 容易造成元数据膨胀问题, 但如果将这些元数据放在Spark里面的话, 就会造成代码耦合程度太大了. 因此先看看编程时候能不能将代码耦合去除, 如果去除的话, 直接在ReduceTask测过滤</p>
<p>推测执行的难点在于, 两个独立进程写入同一个数据块, 那么在Server端就必须加锁来防止竞争, 但是一旦加锁, 性能就会收到影响</p>
</blockquote>
<h5 id="拉取失败"><a href="#拉取失败" class="headerlink" title="拉取失败"></a>拉取失败</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172232885.png" alt="image-20220228172232885"></p>
<ol>
<li>Driver启动Reduce任务</li>
<li>ReduceTask开始拉取数据, 但是返回异常</li>
<li>此时跟MapTask一样, 先开始Partition级别拉取重试, 如果第二次失败的时候, 开始拉取备份节点数据. 为了防止重复, 拉取成功的数据块, 需要记录对应的Task号, 拉取备份节点的时候, 需要重新过滤</li>
<li>如果Partition级别的重试未成功</li>
<li>上报给Driver, 准备重试Task</li>
<li>下发重试任务到新的Executor</li>
<li>此时如果一直失败的话, 有两个选择, 一个是直接宣布App失败了, 另外就是重选上一个Stage, 先选第一种</li>
</ol>
<h5 id="Server失败"><a href="#Server失败" class="headerlink" title="Server失败"></a>Server失败</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172249213.png" alt="image-20220228172249213"></p>
<ol>
<li>以MapTask为例, ReduceTask其实也类似</li>
<li>MapTask开始写入数据</li>
<li>此时对应Server进程shutdown了, 对应Manager来就是心跳消失</li>
<li>MapTask首先会失败, 失败后会再次重试, 一般partition级别重试会一直失败, 那么就会换一个节点写入.</li>
<li>此时如果Shuffle Server能马上重启, 重新接入心跳, 那么Manager就会当没事发送. 如果超过一定时间, 还没有重启, 那么Manager会告之Driver数据丢失(这里可以被动的)</li>
<li>Driver发现DataLost之后, 先查看是否开启数据备份功能, 如果有, 则继续. 如果没有开启数据备份, 那么就开始停止整个Stage, 等待重新下发全部的任务(如果有Reduce任务, 且数据没有开启备份功能, 则直接APP失败退出)</li>
</ol>
<blockquote>
<p>如果Server重启且不能恢复的话, 直接让APP失败, 由上层平台重试也许是个最好的方案</p>
</blockquote>
<p>另外一种方式, Service一旦失败, 重试这个Stage, 上一个Stage的数据必须在DFS中有备份, 这样才能重试Stage.</p>
<h4 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a>数据一致性</h4><p>数据的一致性问题, 其实在上面已经单独说过了, 但这里还是要单独拿出来看的, 因为数据不一致, 结果一般就不对了.<br>不一致情况的原有有以下几种:</p>
<ul>
<li>MapTask重试, 一些数据写了两遍</li>
<li>推测执行数据有两份</li>
<li>Server失败, 导致一份数据写到两个Server, 这时有些数据重复了, 有些数据没重复<br>解决上面的问题的思路, 就是读的时候, 需要过滤多余的数据.<br>对于同一个Server写了2遍, 这时通过CommittedTaskId过滤掉多余Task产生的数据<br>对于多个Server数据, 读取的时候, 要记录哪些Task任务已经被消费了, 如果被消费了就不需要重新读取了</li>
</ul>
<h4 id="数据可靠性"><a href="#数据可靠性" class="headerlink" title="数据可靠性"></a>数据可靠性</h4><p>数据备份功能, 一般有几个问题要选择: 谁来备份, 怎么备份, 备份到哪儿</p>
<h5 id="谁来备份"><a href="#谁来备份" class="headerlink" title="谁来备份"></a>谁来备份</h5><p>一般有客户端和服务端备份, 客户端备份,就是客户端直接写多份数据, 服务端备份的话, 客户端只写一份数据, 然后服务端自己写到备份地方去<br>客户端备份的优点是简单, 缺点点网络连接多些, 并且与后端耦合<br>服务端是反过来的<br>我们这儿选服务端备份, 因为备份到哪儿还不是很确定</p>
<h5 id="怎么备份"><a href="#怎么备份" class="headerlink" title="怎么备份"></a>怎么备份</h5><p>这儿有同步和异步的选项, 因为是临时数据, 所以Shuffle的备份最多应该就2个, 不会有半异步的选项<br>同步的特点是, 速度慢, 但是能保证数据一致<br>异步的话, 性能会好, 但是容易导致数据并没有完全ready<br>我们这儿选同步, 因为如果是异步的话, 上面的任务可靠性的处理会更加复杂一些, 这个性能等后续再优化吧</p>
<h5 id="备份到哪儿"><a href="#备份到哪儿" class="headerlink" title="备份到哪儿"></a>备份到哪儿</h5><p>在我们的系统里面有两个选项, 其他Server或者DFS.<br>如果放到其他实例里面的话, 因为机器也是SSD的, 所以性能会好一些, 但是DFS性能会比较差, 不过稳定性会比较好, 不需要考虑数据丢失的问题了<br>这里优先DFS, 因为DFS有人维护.</p>
<h3 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h3><h4 id="主备切换"><a href="#主备切换" class="headerlink" title="主备切换"></a>主备切换</h4><p>主备切换的能力依靠ZK来完成:</p>
<ul>
<li>客户端和Server会监控ZK的地址, 如果Manager地址变化之后, 会主动切换地址和端口</li>
<li>主Manager会在ZK上写入数据, 而备Manager会一直监控着, 如果发现节点丢失, 即主Manager失联, 备Manager会读取Zookeeper上的元数据, 然后变成主Manager写入数据.</li>
</ul>
<h4 id="主备切换-任务不中断"><a href="#主备切换-任务不中断" class="headerlink" title="主备切换, 任务不中断"></a>主备切换, 任务不中断</h4><p>做到任务不中断, 需要有以下条件</p>
<ul>
<li>切换时间短</li>
<li>无任何不一致状态, 或者状态都可以恢复</li>
<li>连接重试<br>要做到切换时间短, 需要备节点时刻监控主节点的状态, 不能落后太多, 不然就会启动延迟<br>尽量将必要的元数据信息都写入到ZK之中, 但ZK并非更新友好型存储系统, 因此也需要实现中间状态重构的能力,而这个重构不能出现状态丢失<br>客户端的所有RPC请求, 都要有重试功能, 一旦发现主备切换, RPC失败之后, 迅速切换到另外的节点.</li>
</ul>
<h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><h4 id="页面"><a href="#页面" class="headerlink" title="页面"></a>页面</h4><p>参考Livy的实现, 只实现到APP级别即可<br>APP: Id, 提交时间, 结束时间, 时长, 当前Shuffle文件个数, 当前shuffle存储总量</p>
<h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><p>App总量: 个数, shuffle存储量, 文件总个数<br>资源容量: 磁盘,内存slot, cpu<br>JVM相关: 内存, 线程</p>
<h4 id="慢节点告警"><a href="#慢节点告警" class="headerlink" title="慢节点告警"></a>慢节点告警</h4><p>如何定义慢节点?</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="兼容性方案"><a href="#兼容性方案" class="headerlink" title="兼容性方案"></a>兼容性方案</h4><p>兼容性包含客户端和服务端之间,以及服务端Manager和Server时间.<br>客户端目前只会支持Spark2.4.3版本以及Spark3.0版本.服务端需要同时支持这两个版本的接入, 因此整体差异只会在于Spark与Client接口层次, RssServer的Client和Server之间的通信兼容性必然要遵循. 可以适当的在控制流监控预留部分json字段来保持未来的兼容.<br>服务端的Manager和Server的RPC也同上诉方式.<br>如果后续改动实在无法, 通过灰度升级的方案处理</p>
<h4 id="多版本支持"><a href="#多版本支持" class="headerlink" title="多版本支持"></a>多版本支持</h4><p>由于兼容性和容错的考虑, Push方式的Shuffle Service很难实现滚动升级, 原因在于每个实例都有很多的网络连接, 一旦重启, SparkTask就会出现异常, 造成大规模的任务重试, 对集群会产生巨大的压力. 因此只能采用灰度升级方案, 发布也必须支持多版本特性.</p>
<p>每个版本发布包都有一个版本号, 进程启动是会将版本号写入到Zookeeper的元数据之中, 客户端会根据自身的版本号, 选择对应的路径, 然后获取到ShuffleManager的地址, 完成整个任务的启动环境.</p>
<p>所以, 整个客户端/RssManager/RssServer的版本都是配套的. 整套环境之中, 目标最多只能存在3个版本.<br>为了防止用户客户端死活不升级情况, Spark加载版本和jar包方式, 将通过Zookeeper获取元数据, 然后启动的时候, 远程加载配置项和jar文件, 这段代码需要注入到Spark之中, 而非RSS.</p>
<h4 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a>部署方案</h4><div class="table-container">
<table>
<thead>
<tr>
<th>实例角色</th>
<th>最少个数</th>
<th>推荐个数</th>
<th>部署位置</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zookeeper</td>
<td>3</td>
<td>5</td>
<td>单独节点, 至少独立磁盘</td>
</tr>
<tr>
<td>RssManager</td>
<td>2</td>
<td>2</td>
<td>Master单独一个节点, Slave单独一个节点</td>
</tr>
<tr>
<td>RssServer</td>
<td>5</td>
<td>30</td>
<td>单独节点</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>每个节点, 对于单个版本, 只部署单个实例</li>
<li>机器尽量在同一机房, 网络带宽尽量大, 因为需要内部传输备份数据</li>
<li>RssManager的资源尽量在8核16G以上, 防止RPC和元数据处理成为集群瓶颈</li>
<li>RssServer机器尽量是SSD盘, 每个节点过挂一些数据盘, 尽量满足1核2G1TB的配置方式</li>
</ol>
<h4 id="升级方案"><a href="#升级方案" class="headerlink" title="升级方案"></a>升级方案</h4><p>整体采用灰度升级方案, 即每个节点会部署多个实例, <code>RssManager</code>和<code>RssServer</code>各自部署一套, 两者元数据分开存储, 但调度器会感知多个版本的任务情况.<br>整体升级步骤如下:</p>
<ol>
<li>部署新版本的<code>RssManager</code>, 在standby的机器上部署另外一个备<code>RssManager</code></li>
<li>逐个安装新的版本的<code>RssServer</code>, 完成后并添加新版本的监控告警</li>
<li>更新新版本客户端到HDFS</li>
<li>更新Zookeeper上最新RSS客户端版本</li>
<li>过两天, 查看老版本的<code>RssManager</code>的负载, 如果任务数已经降低为0, 则准备下线版本</li>
<li>关闭监控告警, 逐个关闭Server, 最后关闭两个Manager.</li>
</ol>
<h5 id="Zookeeper节点切换"><a href="#Zookeeper节点切换" class="headerlink" title="Zookeeper节点切换"></a>Zookeeper节点切换</h5><p>整个集群之中, Zookeeper虽然不是单点的, 但是ZK集群确实单点的, 一旦节点老旧必须替换升级或者IP切换的时候, ZK地址已经变换, RSS和Spark配置都要相对的变化, 这时就必须重启完成, 此时必须<strong>容忍任务大规模重试</strong>.</p>
<h3 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h3><h4 id="鉴权"><a href="#鉴权" class="headerlink" title="鉴权"></a>鉴权</h4><p>内部集群可以先不考虑</p>
<h4 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h4><p>Shuffle数据作为临时数据, 用户即可清理, 可以先不做完整性校验(目前Spark也没有做)</p>
<h4 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h4><p>Shuffle数据根据App粒度清理, Yarn Shuffle Service之中, Driver在退出的时候, 会清理对应的数据, 但不删除目录. 目录有Yarn感知到App状态为完成之后, 下发NodeManager完成清理工作.<br>但在Shuffle Service之中, 由于无法感知App状态, 因此需要Driver来主动清理. </p>
<ul>
<li>在Driver退出的过程之中, 会向Manager发送AppEnd的消息, 接收到请求之后, 开始清理App对应的Shuffle, 想各个Service发送完毕异步请求之后, 将App的状态标记为Deleted, 过一段时间后删除该状态</li>
<li>如果Driver异常退出, 度过静默期(无RPC往来的时长)之后ShuffleManager主动查询Yarn上App的状态, 如果为退出状态, 则主动删除数据.</li>
<li>ShuffleManager如果主备切换, 由于App的信息保存在Zookeeper之中, 因此备节点依然会完成清理工作</li>
<li>ShuffleService实例如果发重启, 那么它会主动询问是否有资源未清理, 如果发现本地APP的在状态为Deleted, 或者查询不到.</li>
</ul>
<h3 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h3><h4 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h4><p>资源的类型有以下几类:</p>
<ol>
<li>磁盘容量</li>
<li>内存Buffer量</li>
<li>IO负载压力</li>
<li>CPU负载压力</li>
</ol>
<h4 id="DataLocation"><a href="#DataLocation" class="headerlink" title="DataLocation"></a>DataLocation</h4><p>分配Server节点的时候, 需要返回机架信息, 方便Reduce任务决策启动在哪些几点之上.</p>
<h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><h4 id="支持多磁盘写入"><a href="#支持多磁盘写入" class="headerlink" title="支持多磁盘写入"></a>支持多磁盘写入</h4><p>文件磁盘目录, 也需要根据Spark一样规划为多级目录, 同时支持多个SSD目录<br>文件也会根据AppId + ShuffleId + PartitionId的hash值计算出对应的目录路径</p>
<h4 id="小数据块合并发送"><a href="#小数据块合并发送" class="headerlink" title="小数据块合并发送"></a>小数据块合并发送</h4><p>Map写入, 如果数据块比较小, 且写入同一个Server, 则可以合并发送.</p>
<h4 id="文件流式读取"><a href="#文件流式读取" class="headerlink" title="文件流式读取"></a>文件流式读取</h4><p>对于Reduce任务, 如果Partition实在太大, 可以根据流式方式读取</p>
<h4 id="小数据块合并读取"><a href="#小数据块合并读取" class="headerlink" title="小数据块合并读取"></a>小数据块合并读取</h4><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172327830.png" alt="image-20220228172327830"><br>例如上图, 在shuffleRead的时候,需要花费近6个小时, 主要原因在于网络小io太多了</p>
<h4 id="2GB限制"><a href="#2GB限制" class="headerlink" title="2GB限制"></a>2GB限制</h4><p>Spark在Shuffle的时候, 已经通过<code>DownloadManager</code>, 通过文件的方式解决了2GB的限制, 但目前这儿依然由这个限制.<br>如何解决问题?  仿照<code>Spark</code>的处理方式, 如果发现数据量大于2GB, 则启动文件发送, 服务端需要重新定义<code>handler</code>, 整个数据也写入到临时文件之中, 不要写在原有RSS的数据文件之中.<br><code>ShuffleDataWrapper</code>需要加入一个新的字段, 或者将<code>data_length</code>设置为负值</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%8A%80%E6%9C%AF/" rel="tag"># 技术</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/02/28/Clickhouse%E7%A0%94%E7%A9%B6-Clickhouse%E7%9A%84%E5%90%8C%E6%AD%A5%E9%94%81/" rel="prev" title="[Clickhouse研究]: Clickhouse的同步锁">
                  <i class="fa fa-angle-left"></i> [Clickhouse研究]: Clickhouse的同步锁
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/02/28/RemoteShuffleService-RSS%E5%AE%9E%E7%8E%B0%E6%83%85%E5%86%B5/" rel="next" title="[RemoteShuffleService]: RSS实现情况">
                  [RemoteShuffleService]: RSS实现情况 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Carlmartin</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">186k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:49</span>
  </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
