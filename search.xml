<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>[Clickhouse研究]: TooManyPart问题解决思路</title>
      <link href="/2022/03/04/Clickhouse%E7%A0%94%E7%A9%B6-TooManyPart%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF/"/>
      <url>/2022/03/04/Clickhouse%E7%A0%94%E7%A9%B6-TooManyPart%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h1> <figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">2021-12-08 10:20:52,561 ERROR org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat [] - JDBC executeBatch error, retry times = 1</span><br><span class="line">org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.except.ClickHouseException: ClickHouse exception, code: 252, host: 10.88.129.186, port: 8024; Code: 252, e.displayText() = DB::Exception: Too many parts (315). Merges are processing significantly slower than inserts.: while write prefix to view Storage omega_analyse_project_stream.dwm_log_pub_event_pvuv_hi_view_local (version 206.1.1)</span><br><span class="line"> </span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:59) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.except.ClickHouseExceptionSpecifier.specify(ClickHouseExceptionSpecifier.java:29) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.ClickHouseStatementImpl.checkForErrorAndThrow(ClickHouseStatementImpl.java:1094) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.ClickHouseStatementImpl.sendStream(ClickHouseStatementImpl.java:1061) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.ClickHouseStatementImpl.sendStream(ClickHouseStatementImpl.java:1026) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.ClickHouseStatementImpl.sendStream(ClickHouseStatementImpl.java:1019) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.ClickHousePreparedStatementImpl.executeBatch(ClickHousePreparedStatementImpl.java:381) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.streaming.connectors.clickhouse.shaded.ru.yandex.clickhouse.ClickHousePreparedStatementImpl.executeBatch(ClickHousePreparedStatementImpl.java:364) ~[flink-connector-clickhouse_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.connector.jdbc.internal.executor.SimpleBatchStatementExecutor.executeBatch(SimpleBatchStatementExecutor.java:71) ~[flink-connector-jdbc_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.attemptFlush(JdbcBatchingOutputFormat.java:207) ~[flink-connector-jdbc_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.flush(JdbcBatchingOutputFormat.java:174) ~[flink-connector-jdbc_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at org.apache.flink.connector.jdbc.internal.JdbcBatchingOutputFormat.lambda$open$0(JdbcBatchingOutputFormat.java:123) ~[flink-connector-jdbc_2.11-1.12.0-700.jar:1.12.0-700]</span><br><span class="line">    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_77]</span><br><span class="line">    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_77]</span><br><span class="line">    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_77]</span><br><span class="line">    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_77]</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]</span><br><span class="line">    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]</span><br><span class="line">    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]</span><br></pre></td></tr></table></figure><p>Flink任务写入CK时, 出现以上异常, 并开始重试, 如果重试次数过多, 就会触发flink的重启, flink重启达到5次, 用户就会收到告警.</p><p>目前Flink重试的时候, 会造成Checkpoint期间的数据, 全部重新写入, 即能够保障AT_LEAST_ONCE语义.</p><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220304152821844.png" alt="image-20220304152821844"></p><p>在数据每次写入的时候, 都会调用该方法, 判断数据写入频率是否过快了, 如果太快了, 就会执行delay操作.</p><p>如何判断过快呢? 通过以下判断</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parts_count_in_partition &gt;= settings-&gt;parts_to_throw_insert</span><br></pre></td></tr></table></figure><p>parts_count_in_partition表示所有partition中DataPart个数最多的一个, parts_to_throw_insert是一个配置项, 默认是300</p><p>CK每次写入一批数据, 就会生成一个DataPart, 然后会有Merge线程merge多个DP合并为一个.</p><p>当<strong>写入太过频繁</strong>或者<strong>Merge过慢</strong>, 就会出现Too Many Parts的问题.</p><h2 id="写入频繁"><a href="#写入频繁" class="headerlink" title="写入频繁"></a>写入频繁</h2><p>写入频繁分两种情况:</p><p><strong>第一种</strong>, 用户数据量不大, 但flink的参数写入不当, 由于现在Flink多是采用jdbc的接口, 攒批的参数需要用户手动设置, 默认值不随我们配置, 因此会导致批过小的问题</p><p><strong>第二种</strong>, 用户数据量较大, flink参数已经被值班人员调整过, 但由于数据量巨大, 依然出现写入过快的问题. 这种问题比较容易出现在”小宽表”上, 即每行数据量比较少, 但是行数特别多的场景.</p><h2 id="Merge过慢"><a href="#Merge过慢" class="headerlink" title="Merge过慢"></a>Merge过慢</h2><p>Clickhouse一个节点的Merge线程为48个, 而且由于集群区分读写节点, 写节点就一个, 因此实际上单个shard内, 合并是单点的.</p><p>因此解决merge过慢的问题, 可以从<strong>增加写节点个数</strong>方式出发.</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>针对以上的问题, 我们可以对问题, case by case制定改进方案</p><h2 id="MemoryTable方式"><a href="#MemoryTable方式" class="headerlink" title="MemoryTable方式"></a>MemoryTable方式</h2><p>这个方案是针对写入频繁中的第一种情况, 这种情况下, 在CK内部完成攒批, 可以解决用户Flink配置问题</p><p>MemoryTable可以参考CK中的Buffer引擎来实现, 但逻辑需要内嵌于MergeTree引擎中, 否则上线成本会增加很多.</p><p>但Buffer引擎和MemoryTable还是有一些区别</p><ol><li>Buffer并不保证数据的一致性, 而在MemoryTable中, 需要实现WAL方式, 支持数据恢复</li><li>Buffer引擎是单机的, 而MergeTree需要考虑分布式的能力, 防止节点异常的情况, 需要实现WAL文件的复制能力</li></ol><p>除去以上的问题, 还有两个细节问题, 在实现上要非常注意:</p><p>第一, 如果用户的SQL中有final等特别的关键字, 那么需要跟内存的数据做merge</p><p>第二, 如果用户准备修改表结构, 那么内存中的结构也会跟着磁盘的结构一起修改, 此时还需要注意新写入的数据情况</p><h3 id="难点一"><a href="#难点一" class="headerlink" title="难点一"></a>难点一</h3><p>final等关键字</p><h3 id="难点二"><a href="#难点二" class="headerlink" title="难点二"></a>难点二</h3><p>alter时, 数据写入</p><h2 id="列式格式写入"><a href="#列式格式写入" class="headerlink" title="列式格式写入"></a>列式格式写入</h2><p>这种情况可以解决写入频繁中的第二种情况, 如果列存数据的压缩能力, 将批次变大.</p><p>由于列存格式的适配, 需要引入新的Flink Connect, 那么在修改过程中, 我们可以在新的Connector中, 实现对于写入频繁的第一种场景的参数进行有效的空,</p><p>目前已经支持Parquet格式的写入, 但是Parquet不支持复杂类型, 例如Array/Map等, 最好能够引进ORC或者CK本身的Native的Block数据结构, 减去转换的开销.</p><h3 id="难点三"><a href="#难点三" class="headerlink" title="难点三"></a>难点三</h3><p>Native格式是CK最有效的写入格式, CK主要将数据按照自己读起来就行, 省却了压缩编码的工作, 会极大的增加导入的性能.</p><p>但Native格式是CK的内部的格式, 可能需要实现在Java中用JNI接口调用C++源码.</p><p>该方案在其他列存格式有问题的情况下, 再来实现吧.</p><h2 id="读写交替"><a href="#读写交替" class="headerlink" title="读写交替"></a>读写交替</h2><p>下面的三个方面的目标, 都是增加写节点.</p><p>目前集群由读写节点确定, 读写节点是固定, 只有故障的时候, 才会变化, 从监控上看, 读写节点压力非常不一致, 写节点的网络写入只有读的一半左右. </p><p>因此, 在公共集群上, 可以实现读写能力的交替, 即对同一张表来说, 数据永远只写一个节点(通过表的hash计算), 但对于不同表的写入, 可以是不同的节点, 因此称为读写交替.</p><h2 id="多写集群"><a href="#多写集群" class="headerlink" title="多写集群"></a>多写集群</h2><p>比上面的方案更好的, 是完成表级别的多写能力, CK原生也是支持的. 但这种情况下, 在变更时候的数据一致性, 还是需要重点测试的.</p><h3 id="难点四"><a href="#难点四" class="headerlink" title="难点四"></a>难点四</h3><p>表多写的影响, Clickhouse原生支持Multi Write, 但不确定是否在复杂环境下, 有异常场景, 可能需要做一次严格的测试来论证这个问题.</p><h2 id="集群扩容"><a href="#集群扩容" class="headerlink" title="集群扩容"></a>集群扩容</h2><p>最后一种方案, 是最简单的. 就是shard扩容, 但我们目前一直都没有一个扩shard的方案, 因此该方案, 依赖辉哥的读写分离的方案, 等读写分离实现后, 实现集群扩容会简化很多.</p><h3 id="难点五"><a href="#难点五" class="headerlink" title="难点五"></a>难点五</h3><p>读写分离方案, 支持扩容shard实现写入加数</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RemoteShuffleService]: RSS实现情况</title>
      <link href="/2022/02/28/RemoteShuffleService-RSS%E5%AE%9E%E7%8E%B0%E6%83%85%E5%86%B5/"/>
      <url>/2022/02/28/RemoteShuffleService-RSS%E5%AE%9E%E7%8E%B0%E6%83%85%E5%86%B5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>该文档写于2021年4月, 当时按照设计文档实现了RSS的原型(基于Uber的开源RSS), 但是后续应该人事调整, 我开始从事OLAP工作后,就不在开发RSS了.</p></blockquote><h2 id="Spark-Shuffle任务的现状阐述"><a href="#Spark-Shuffle任务的现状阐述" class="headerlink" title="Spark Shuffle任务的现状阐述"></a>Spark Shuffle任务的现状阐述</h2><p>目前公司内部有最大集群规模已经达到8000多台机器, 总共有近45万核, 1.23PB的内存, 其中每天有10万+个Spark的离线任务在运行.</p><p>目前团队正在将HiveSQL的任务都迁移到Spark引擎之上, 提升整体性能和资源利用率, Spark将承接越来越多的业务压力. </p><p>但随着Spark应用越来越多, 任务的稳定性受到了越来越多的挑战, 尤其是Spark Shuffle这块的问题, 一旦数据量超过一定阈值, 就会出现大量的Shuffle Fetch失败的情况, 由于Spark做了很好的重试机制, 因此有时候依然能跑出来结果, 但大量计算资源浪费在重试步骤之中; 而有时候因为失败太多, 导致整个App失败退出.</p><p>举个真实的案例, 下图是某业务运行的一个UI监控图, 从图中可以看到, 在Failed Stages里面出现了大量的<code>FetchFailedException</code>, 整个程序在不停的Stage重试, 而最后这个case由于失败过多整个APP失败了<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172502068.png" alt="image-20220228172502068"></p><blockquote><p>在上图中的19个Failed Stages都是由于<code>FetchFailedException</code>造成的</p></blockquote><h2 id="Spark-Shuffle任务的瓶颈分析"><a href="#Spark-Shuffle任务的瓶颈分析" class="headerlink" title="Spark Shuffle任务的瓶颈分析"></a>Spark Shuffle任务的瓶颈分析</h2><h3 id="监控发现"><a href="#监控发现" class="headerlink" title="监控发现"></a>监控发现</h3><p>在分析这类问题的时候, 我们发现了经典的”三高”现象, 即网络,磁盘, CPU都很高.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172518083.png" alt="image-20220228172518083"><br>如上图所示, 一般出现<code>FetchFailedException</code>时, 此时机器上的监控一般会出现这三者的峰值.<br>磁盘和网络的峰值一般一起出现, 因为Shuffle时会有大量的数据交换<br>CPU出现峰值时有两种情况, 一种<code>Wait</code>比较高(图中黄色部分), 说明CPU忙于处理IO操作,一种<code>Wait</code>不高, 但总体CPU比较高, 这种情况往往是该节点有其他任务运行导致的, <strong>进程之间没做到CPU隔离</strong> 或者一些<strong>热点计算问题</strong>.</p><h3 id="Shuffle原理"><a href="#Shuffle原理" class="headerlink" title="Shuffle原理"></a>Shuffle原理</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172616029.png" alt="image-20220228172616029"><br>从监控看到了, Shuffle过程是个”三高”过程, 同时是IO和CPU密集型的过程, 这一节从Shuffle的原理上, 解释一下为什么Shuffle过程为什么会出现瓶颈.</p><p>Shuffle本质是一种数据交换机制, 在MR模型中数据是按照(key, value)的元组方式组织的, 经过MapTask之后, Key的数值已经变化了, Shuffle的目标将这些数据重新组合, 相同新Key的数据放到一起处理.</p><p>因此, 如果所示, MapTask按照新Key产生了3个不同的数据块, 每个数据块与Reduce的个数对应, ReduceTask则直接去获取对应的数据块(图中用颜色表示)</p><p>在Spark的实现之中, MapTask将这些数据重排序后, 写入到同一个文件之中, 通过元数据标识数据所在offset和length. 例如在文件的(offset=0, length=512)的数据为P0, (offset=512, length=1024)的数据为P1. Map阶段要写完所有数据, <strong>磁盘吞吐非常大</strong>.</p><p>ReduceTask拿数据的时候, 要获取所有MapTask的输出, 所以必须跟所有MapTask文件所在Server产生网络连接, <strong>网络连接会非常多</strong>. 需要传输所有的MapTask输出到其他节点, 那么<strong>网络吞吐也会非常大</strong>. 其次, 我们拿P1数据的时候, 是直接读取文件的中间位置的, 因此<strong>随机IO会非常多, 磁盘繁忙</strong>. 最后为了减少网络和磁盘的压力, Spark加入了<strong>Shuffle过程的Merge和Shuffle数据压缩能力</strong>, 这就加重CPU的压力, 但一般情况下CPU压力没那么大, 除非<strong>被别的任务影响</strong></p><p>如果有兴趣了解更加详细的Shuffle原理的同学, 可以参考<a href="http://lionheartwang.github.io/blog/2018/03/11/spark-shuffle-implementation/">这篇文章</a></p><h3 id="原因归纳"><a href="#原因归纳" class="headerlink" title="原因归纳"></a>原因归纳</h3><p>综上所述, 我们简单归纳原因:</p><ul><li>大量网络连接, 且非常多的网络小包</li><li>磁盘大吞吐, 且随机读取, 触发IO瓶颈</li><li>计算存储耦合, CPU密集型任务影响Shuffle</li></ul><h3 id="目前的解决方式"><a href="#目前的解决方式" class="headerlink" title="目前的解决方式"></a>目前的解决方式</h3><p>针对以上的原因, 目前团队也有一些处理措施:</p><ul><li>增加超时配置项, 缓解网络问题</li><li>增加Yarn Shuffle Service处理IO的线程个数,缓解IO问题</li><li>隔离集群, 给队列打上独立NodeLable, 去除其他进程影响</li></ul><p>以上措施在一定程度上缓解了<code>FetchFailedException</code>问题, 但这些措施都治标不治本:</p><ul><li>增加网络超时, 会加大重试的时间, 导致异常情况下时间更长</li><li>Yarn节点的IO已经达到硬件上线, 无法再通过多线程加速</li><li>隔离集群, 使得集群资源的使用率低, 平均CPU使用率在30%以下 </li></ul><p>但随着业务数据量越来越多, 该问题又开始出现了, 是时候提出一种根治的方案了</p><h2 id="Spark-Shuffle任务的改进方案"><a href="#Spark-Shuffle任务的改进方案" class="headerlink" title="Spark Shuffle任务的改进方案"></a>Spark Shuffle任务的改进方案</h2><h3 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h3><p>这个章节, 主要针对上文提到的”三高”问题, 看看从设计上如何解决这些问题</p><h4 id="网络问题"><a href="#网络问题" class="headerlink" title="网络问题"></a>网络问题</h4><p>在原有的网络模型之中, 网络连接个数为M*R, 而M和R都在Executor之中, 可以复用网络连接, 实际的连接为E^2. 如果executor个数为2000个, 那么会有4百万的连接, 一些排在后面网络连接经常出现超时.</p><blockquote><p>为了性能, Spark在Reduce启动的时候, 会尽可能的建立所有的链接</p></blockquote><p>另外一个问题是网络传输的时候小包太多了, 一个分区的平均大小&lt;1MB, 大量的时间消耗在网络连接的创建过程之中, 能不能提前聚合数据, Reduce启动的时候, 直接读取数据就行了.</p><p>对于这两个问题, <strong>Push-Shuffle</strong>可以很好的解决这两个问题<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172633521.png" alt="image-20220228172633521"></p><p>该方案数据的流程, 并不是由ReduceTask主动Pull, 而是由MapTask主动Push到ShuffleService里.<br>如图所示, 2个MapTask计算完分区数据P0后, 将两者的P0都push到第一个Shuffle Service.<br>ReduceTask启动后, 直接获取第一个Shuffle Service之中P0数据即可.</p><p>回看原来的问题, 该方案的连接个数为(E*S + E), S为RSS的个数, 一般在10-50之间, 远远小于E的个数, 有效减少连接;ShuffleService会合并数据(只要读取的时候, 多读几个块就好), 可以方便的处理小包问题.</p><blockquote><p>发送时的小包, 可以通过同步发送P0和P1到同一个节点来解决</p></blockquote><p>此外<strong>Push-Shuffle</strong>将随机IO变成了顺序IO, 解决了随机IO问题.</p><h4 id="磁盘问题"><a href="#磁盘问题" class="headerlink" title="磁盘问题"></a>磁盘问题</h4><p>磁盘问题一个在于随机读写, <strong>Push-Shuffle</strong>将Reduce阶段的随机读写变成顺序读写<br>另外一个问题就是读写速度, 我们的解决思路就是使用<strong>SSD替换HDD</strong>, 同时SSD对于随机读写也非常友好, 因此硬件加速完全好于预期.<br>那么用Spark原来的External Shuffle Service是否也能上SSD解决这个问题呢?<br>答案是否定的, 原因在于ESS是和Yarn强制绑定的, 代码逻辑在NodeManager的进程之中, 替换全部Yarn集群的磁盘, 成本上根本无法接受. 因此RSS必须是<strong>存储计算分离</strong>的, 也就是需要是个完全<strong>独立服务</strong>.<br>总结来说, 解决磁盘问题方法是: <strong>Push-Shuffle</strong>, <strong>SSD加速</strong> 和 <strong>独立服务</strong></p><h4 id="CPU问题"><a href="#CPU问题" class="headerlink" title="CPU问题"></a>CPU问题</h4><p>如&lt;瓶颈分析&gt;中提到的, CPU问题并不是Shuffle的核心问题, 主要问题在于热点和隔离.<br>为了解决热点问题, 我们引入了<strong>Shuffle 调度器</strong>, 通过监控App里面磁盘网络内存压力, 防止出现单服务器热点, 拖慢计算或者导致失败.<br>对于隔离问题, 我们采用<strong>独立部署</strong>, 解除对Yarn集群的依赖</p><h3 id="Remote-Shuffle-Service方案"><a href="#Remote-Shuffle-Service方案" class="headerlink" title="Remote Shuffle Service方案"></a>Remote Shuffle Service方案</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172650782.png" alt="image-20220228172650782"><br>由上文的铺垫, 整体架构也呼之欲出了, 整体采用典型的Master-Slave模式, 其中Client为Spark App, Master为Remote Shuffle Service Manager(RssManager), Slave为Remote Shuffle Service Server(RssServer), 还有DFS作为数据备份, Zookeeper作为元数据存储.</p><h4 id="Remote-Shuffle-Service-Manager"><a href="#Remote-Shuffle-Service-Manager" class="headerlink" title="Remote Shuffle Service Manager"></a>Remote Shuffle Service Manager</h4><p>作为整个集群的控制节点, 负责整个集群与客户端的任务调度, 元数据管理等主要工作. </p><ul><li>Manager会采用主备方式完成服务高可用, 主备实例会通过Zookeeper进行选主工作</li><li>Manager会采取无状态应用模式, 整体状态都会同步到Zookeeper之中, Slave可以在重启时重建全部的状态</li><li>为了加速备实例的启动速度, 主备实例会保持心跳, 并定时同步全量或增量状态给备节点</li><li>Manager与Server会保持心跳状态, Server通过心跳上报资源容量信息. </li><li>Manager会主动想Server下发RPC通信</li><li>客户端会与Manager建立连接发送RPC请求, 但是Manager不会主动给客户端发送RPC请求</li><li>Manager与资源调取器(Yarn/K8s)保持连接, 查询任务状态, 用以退出清理</li><li>Manager内部设有调度器组件, 可以插件式的设置调度策略</li><li>Manager内部有元数据管理, 主要是App级别的信息, Task级别的信息由Driver持有, 通过RPC请求送达Manager</li><li>Manager服务有WebUI接口, 统计App级别监控信息</li><li>Manager服务有jmx接口, 指标方便指标统计</li></ul><h4 id="Remote-Shuffle-Service-Server"><a href="#Remote-Shuffle-Service-Server" class="headerlink" title="Remote Shuffle Service Server"></a>Remote Shuffle Service Server</h4><p>作为整个集群的数据节点, 负责与客户端Spark App进行数据交互</p><ul><li>Server是数据节点, 只会跟客户端进行数据通讯, 不参与控制通信</li><li>Server与Manager会保持心跳, 同时会与Manager有控制流的RPC请求</li><li>Server会将数据首先写入到本地的SSD, 如果开启了备份的话, 也会以pipeline的方式, 同步给其他备份节点, 最后开启了慢写入的话, 才会备份给DFS. 由于DFS一般有HDD磁盘组成, 因此性能会受到巨大的影响</li><li>默认情况下, 一个Reduce任务会开一个文件句柄, 用于写入Map的数据, 同时还有一个index文件记录数据块位置与TaskId的对应关系</li><li>读取数据时候, 会根据TaskId过滤无效的数据块</li></ul><h4 id="Spark-App"><a href="#Spark-App" class="headerlink" title="Spark App"></a>Spark App</h4><p>作为整个集群的客户端, 负责实现Spark对外的接口, 并与RSS交互, 该模块除了实现SparkShuffleManager的接口之外, 还要实现Spark App的事件处理, 还需要在调度上处理任务失败的情况</p><ul><li>Shuffle接口: 实现Shuffle注册, 数据读取, 数据写入等工作</li><li>Event接口: 实现整体控制流的处理, 例如App启停, Stage启停</li><li>Schedule接口: 当任务失败时的, 需要重试, push Shuffle的重试方式不同, 目前Spark并没有有接口暴露, 可能会侵入式的修改源码</li></ul><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><p>作为整个集群的元数据中心, ZK承担着元数据备份, 服务发现, 主备切换等功能</p><ul><li>Manager会将App信息写入到ZK, 方便备实例恢复</li><li>Manager通过ZK进行主备切换和监控</li><li>Client会通过ZK的地址获取到Manager的地址</li></ul><h4 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h4><p>作为整个集群的数据备份中心, 目前公司内部只有HDFS作为DFS实现, 但是性能堪忧.<br>但当RSS集群也是以HDD磁盘为主时, Reduce任务直接获取DFS上的数据, 是个好的选择.<br>另外DFS方式的稳定性会更加好</p><h3 id="方案效果"><a href="#方案效果" class="headerlink" title="方案效果"></a>方案效果</h3><p>我们选了两个典型的案例: </p><h4 id="业务1"><a href="#业务1" class="headerlink" title="业务1"></a>业务1</h4><p>使用默认的Shuffle方式, 总耗时7.3h, 有23个<code>Failed Stage</code>,错误类型全是<code>FetchFailed</code><br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172706050.png" alt="image-20220228172706050"></p><p>使用RemoteShuffleService模式, 总耗时2.9小时, 无任何失败的Stage, 总Shuffle Write数据量在15TB左右, 最大ShuffleStage为5.6TB<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172721493.png" alt="image-20220228172721493"></p><h3 id="业务2"><a href="#业务2" class="headerlink" title="业务2"></a>业务2</h3><p>使用默认的Shuffle方式, 总耗时1.6h, 有21个<code>Failed Stage</code></p><p>使用RemoteShuffleService模式, 总耗时38分钟, 无任何失败的Stage, Shuffle Write数据量在8TB左右, 最大ShuffleStage为0.3TB</p><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><div class="table-container"><table><thead><tr><th>业务</th><th>地图</th><th>服务分</th></tr></thead><tbody><tr><td>总Shuffle量</td><td>15TB</td><td>8TB</td></tr><tr><td>最大ShuffleStage</td><td>5.6TB</td><td>0.3TB</td></tr><tr><td>默认Shuffle耗时</td><td>7.3小时</td><td>1.6小时</td></tr><tr><td>默认Shuffle失败Stage个数</td><td>23个</td><td>21个</td></tr><tr><td>RSS耗时</td><td>2.9小时</td><td>0.75小时</td></tr></tbody></table></div><h2 id="Spark-Shuffle任务的未来展望"><a href="#Spark-Shuffle任务的未来展望" class="headerlink" title="Spark Shuffle任务的未来展望"></a>Spark Shuffle任务的未来展望</h2><p>目前RSS属于刚起步阶段, 一期上线时(21年Q2)只会覆盖部分业务场景, 未来还需要在以下方向做深度的优化:</p><ol><li>持续优化性能和功能, 减少小文件传输的等问题</li><li>完善服务的可运维性, 增加运维的页面, 提供各种监控指标</li><li>提高服务的鲁棒性, 能够处理各种服务异常, 在绝大多数场景下能够正常完成计算</li><li>提高服务可测试性, 完备各种测试用例, 防止异常bug对数据正确性的影响</li><li>智能决策Shuffle类型, Spark引擎能够自动决策Shuffle类型, 一旦RSS失败后能够回退到默认的Shuffle</li><li>优化Shuffle数据的调度策略, 解决Shuffle热点瓶颈问题, 优化集群整体使用率</li><li>Cloud Native支持, 提供较好的弹性伸缩能力, 提高集群利用率</li><li>支持与大数据其他组件混部, 提高集群利用率</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RemoteShuffleService]: RSS设计文档 </title>
      <link href="/2022/02/28/RemoteShuffleService-RSS%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/"/>
      <url>/2022/02/28/RemoteShuffleService-RSS%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Shuffle-Service设计文档"><a href="#Shuffle-Service设计文档" class="headerlink" title="Shuffle Service设计文档"></a>Shuffle Service设计文档</h1><blockquote><p>该文档写于2021年1月</p></blockquote><h2 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h2><h3 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h3><p>目前公司内部有3个集群, 最大集群规模已经达到8000多台机器, 总共有近45万核, 1.23PB的内存.<br>待补完…</p><h3 id="Spark业务负载"><a href="#Spark业务负载" class="headerlink" title="Spark业务负载"></a>Spark业务负载</h3><p>目前集群上, 每天有60万个离线任务在运行, 其中Spark任务有10万个. 公司未来整体战略会将所有HiveMR的任务替换Spark任务, 今年将整个数据平台上的SQL任务基本上都迁移到Spark上.<br>整体的资源利用率得到了巨大的提升, 但随着Spark应用越来越多, 任务的稳定性受到了越来越多的挑战, 尤其是Spark Shuffle这块的问题, 总是让运维人员头疼不已, 一旦当天数据量超过历史, 会导致任务失败, 就需要隔离机器重新运行, 等业务量降低时候, 再释放空闲资源, 给整个团队巨大的运维成本.</p><h3 id="当前面临的问题"><a href="#当前面临的问题" class="headerlink" title="当前面临的问题"></a>当前面临的问题</h3><p>我们分析Spark Shuffle的问题, 发现目前Shuffle机制存在如下几个问题:</p><ol><li>计算存储不分离, Spark计算和Shuffle的IO操作混在一起, 极易因为CPU过高导致Shuffle超时失败</li><li>机器超卖, 虽然提高了整体使用率, 但是会因为调度不均匀, 导致失败CPU飙升</li><li>Yarn Shuffle Service内嵌于NodeManager之中, 无法给全部集群替换SSD加速IO效率</li><li>Pull模式的Shuffle, Reduce阶段有大量的随机读取过程, 导致磁盘IO飙升</li><li>Reduce阶段数据未提前聚会, 导致导致大量网络小包产生, 造成网络IO飙升</li><li>调度器无法识别CPU/IO繁忙状态, 导致任务依然下发到近乎满载的机器</li></ol><h3 id="业界的解决方式"><a href="#业界的解决方式" class="headerlink" title="业界的解决方式"></a>业界的解决方式</h3><p>随着Spark成为业界的批处理的标准, 在各大互联网公司之中, Spark Shuffle以上的问题都慢慢出现, 无法通过简单的调优解决这个问题也慢慢成为这个大家的共识. 因此各个公司都推出了自己的解决方案:</p><ol><li>Spark社区和LinkedIn: 领英在VLDB2020发了一篇Paper, 提出<code>Push based Shuffle</code>来解决4和5点, 该方案依然嵌入在原有的Spark框架之中, 因此该方案最容易被社区接受, 现在也慢慢的在合入代码</li><li>Uber: 在今年的Spark Meetup里提出了他们的独立的Shuffle Service的解决方案, 也是目前唯一开源的独立Shuffle Service服务, 意图解决1-5点问题, 并引入SSD解决IO问题. 但目前开源出来的代码健壮性不强, Task重试的时候, 数据会出现冗余,还需要很长的优化之路</li><li>Facebook: 也有名为cosico的独立Shuffle Service, 从架构图上看他们解决了4-6的问题, 但容错使用了DFS系统, Reduce数据也是从DFS直接获取的, 因此性能是打问号的, 且未公开源码, 并不确定具体如何实现</li><li>阿里云: 20年12月份, 阿里云EMR团队公布了他们的Shuffle Service的方案, 同样未开源, 从公开文章上看, 他们除了解决Shuffle稳定性问题(1-5点), 更重要的点是为了解决Spark on k8s在磁盘上性能解决方案. 同样阿里云的方案也未开源</li><li>Intel: 作为一家硬件厂商, 推出了自己的Shuffle Service, 目的是为来卖RDMA产品, 暂时应该不会去采购硬件, 因此略过</li></ol><h3 id="设计目标及范围"><a href="#设计目标及范围" class="headerlink" title="设计目标及范围"></a>设计目标及范围</h3><p>简单调研几家产品之后, 我们打算博采众长, 并找到符合自己的场景的方案, 主要参考Uber和阿里云:</p><ul><li>独立的Shuffle服务, 存储计算分离(解决1-2)</li><li>SSD存储, 加速IO效率, 减少计算资源, 提高整体资源利用率(解决3)</li><li>Push based Shuffle, Server端聚合后再Reduce(解决4-5)</li></ul><p>相对于阿里云, 我们不需要做的:</p><ul><li>不需要全部替换原有Shuffle Service, 能用Adaptive的方式, 决策使用哪种Shuffle模式</li><li>最好是CloudNative的方案, 但是不强制</li></ul><p>而相对于他们两个的方案, 我需要:</p><ul><li>IO敏感型调度, 解决热点问题</li></ul><h2 id="Shuffle-Service顶层设计"><a href="#Shuffle-Service顶层设计" class="headerlink" title="Shuffle Service顶层设计"></a>Shuffle Service顶层设计</h2><h3 id="服务架构图"><a href="#服务架构图" class="headerlink" title="服务架构图"></a>服务架构图</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172007077.png" alt="image-20220228172007077"></p><p>整体架构会采用典型的Master-Slave模式, 其中Client为Spark App, master为Remote Shuffle Service Manager(RssManager), Slave为Remote Shuffle Service Server(RssServer), 还有DFS作为数据备份, Zookeeper作为元数据存储.</p><h4 id="Remote-Shuffle-Service-Manager"><a href="#Remote-Shuffle-Service-Manager" class="headerlink" title="Remote Shuffle Service Manager"></a>Remote Shuffle Service Manager</h4><p>作为整个集群的控制节点, 负责整个集群与客户端的任务调度, 元数据管理等主要工作. </p><ul><li>Manager会采用主备方式完成服务高可用, 主备实例会通过Zookeeper进行选主工作</li><li>Manager会采取无状态应用模式, 整体状态都会同步到Zookeeper之中, Slave可以在重启时重建全部的状态</li><li>为了加速备实例的启动速度, 主备实例会保持心跳, 并定时同步全量或增量状态给备节点</li><li>Manager与Server会保持心跳状态, Server通过心跳上报资源容量信息. </li><li>Manager会主动想Server下发RPC通信</li><li>客户端会与Manager建立连接发送RPC请求, 但是Manager不会主动给客户端发送RPC请求</li><li>Manager与资源调取器(Yarn/K8s)保持连接, 查询任务状态, 用以退出清理</li><li>Manager内部设有调度器组件, 可以插件式的设置调度策略</li><li>Manager内部有元数据管理, 主要是App级别的信息, Task级别的信息由Driver持有, 通过RPC请求送达Manager</li><li>Manager服务有WebUI接口, 统计App级别监控信息</li><li>Manager服务有jmx接口, 指标方便指标统计</li></ul><h4 id="Remote-Shuffle-Service-Server"><a href="#Remote-Shuffle-Service-Server" class="headerlink" title="Remote Shuffle Service Server"></a>Remote Shuffle Service Server</h4><p>作为整个集群的数据节点, 负责与客户端Spark App进行数据交互</p><ul><li>Server是数据节点, 只会跟客户端进行数据通讯, 不参与控制通信</li><li>Server与Manager会保持心跳, 同时会与Manager有控制流的RPC请求</li><li>Server会将数据首先写入到本地的SSD, 如果开启了备份的话, 也会以pipeline的方式, 同步给其他备份节点, 最后开启了慢写入的话, 才会备份给DFS. 由于DFS一般有HDD磁盘组成, 因此性能会受到巨大的影响</li><li>默认情况下, 一个Reduce任务会开一个文件句柄, 用于写入Map的数据, 同时还有一个index文件记录数据块位置与TaskId的对应关系</li><li>读取数据时候, 会根据TaskId过滤无效的数据块</li></ul><h4 id="Spark-App"><a href="#Spark-App" class="headerlink" title="Spark App"></a>Spark App</h4><p>作为整个集群的客户端, 负责实现Spark对外的接口, 并与RSS交互, 该模块除了实现SparkShuffleManager的接口之外, 还要实现Spark App的事件处理, 还需要在调度上处理任务失败的情况</p><ul><li>Shuffle接口: 实现Shuffle注册, 数据读取, 数据写入等工作</li><li>Event接口: 实现整体控制流的处理, 例如App启停, Stage启停</li><li>Schedule接口: 当任务失败时的, 需要重试, push Shuffle的重试方式不同, 目前Spark并没有有接口暴露, 可能会侵入式的修改源码</li></ul><h4 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h4><p>作为整个集群的元数据中心, ZK承担着元数据备份, 服务发现, 主备切换等功能</p><ul><li>Manager会将App信息写入到ZK, 方便备实例恢复</li><li>Manager通过ZK进行主备切换和监控</li><li>Client会通过ZK的地址获取到Manager的地址</li></ul><h4 id="DFS"><a href="#DFS" class="headerlink" title="DFS"></a>DFS</h4><p>作为整个集群的数据备份中心, 目前公司内部只有HDFS作为DFS实现, 但是性能堪忧.<br>但当RSS集群也是以HDD磁盘为主时, Reduce任务直接获取DFS上的数据, 是个好的选择.<br>另外DFS方式的稳定性会更加好</p><h3 id="交互图架构图"><a href="#交互图架构图" class="headerlink" title="交互图架构图"></a>交互图架构图</h3><p>下面考虑一下RSS是如何和Spark的各个实例交互的, 先一个Map和Reduce任务是如何进行控制流通信的, 然后再看数据是如何写入的</p><h4 id="控制流图"><a href="#控制流图" class="headerlink" title="控制流图"></a>控制流图</h4><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172021282.png" alt="image-20220228172021282"></p><ol><li>Spark Driver向Manager发送申请Shuffle资源的请求</li><li>Manager返回结果, 并指明ReduceTaskId对应的Server地址</li><li>Driver根据位置分配Spark任务</li><li>SparkTask计算各自的数据, 发送到对应Server</li><li>Task写完所有的Map数据之后, Executor向DriverCommit任务</li><li>完成所有任务的时候, Driver向Manager发送CommitStage请求, 目的是传递最后成功的TaskId给到ShuffleManager</li><li>Manager将TaskId等消息,下发到各个Server</li><li>Driver下发Reduce任务(如果也是ShuffleMapTask的话, 依然需要先申请资源)</li><li>ReduceTask向对应的Server拉去数据, Server需要根据TaskId过滤重复数据</li></ol><h4 id="数据流图"><a href="#数据流图" class="headerlink" title="数据流图"></a>数据流图</h4><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172040055.png" alt="image-20220228172040055"></p><ul><li>MapTask先会将数据完成预聚合, 按照Partition分区</li><li>然后两个MapTask都将将P0,P1的数据推送到第一个Server</li><li>Server会将两个MapTask的数据都写入到一个文件之中, 因此第一个Server有P0和P1两个文件, 第二个Server有P2,P3,P4三个文件</li><li>启动ReduceTask时候, 直接会去对应Server流式拉取数据</li></ul><h2 id="Remote-Shuffle-Service设计要点"><a href="#Remote-Shuffle-Service设计要点" class="headerlink" title="Remote Shuffle Service设计要点"></a>Remote Shuffle Service设计要点</h2><h3 id="可靠性"><a href="#可靠性" class="headerlink" title="可靠性"></a>可靠性</h3><p>可靠性从三个方面来阐述, 一个是任务可靠性, 指异常发生的整体重试机制, 这里的重试指流程的重试, 而不是消息的重试, 因为RPC消息大多数可以做到幂等, 做不到幂等的就会出现不一致场景, 就是数据一致性问题. 另外一点数据可靠性, 只Shuffle数据文件的备份问题.</p><h4 id="任务可靠性"><a href="#任务可靠性" class="headerlink" title="任务可靠性"></a>任务可靠性</h4><p>首先我们还是回到上面的数据流图, 并标识其中可能失败的场景<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172054924.png" alt="image-20220228172054924"><br>下面分别解释一下错误的具体含义:</p><ol><li>MapDataPush失败值, MapTask之中单个Partition数据无法推送到对应的Server, 造成的原因一般为网络问题等</li><li>慢节点/坏节点, 当出现这种场景的时候, 是多个MapTask推送到同一个Server时候的出现</li><li>MapTask失败, 这个场景为部分Partition数据写入成功, 部分写入失败场景, 造成错误的原因很多, 比如Executor OOM</li><li>Map推测执行, 指启动两个Task计算同一份数据, 会导致所有数据写了2份</li><li>Reduce拉取失败, 指单次失败, 可能网络问题导致</li><li>Reduce持续失败, 拉取多次失败, 可能是慢节点等</li><li>Server失败重启, 进程级别的重启, 服务多一会时间会恢复</li><li>Server丢失数据, 一般是磁盘问题, 导致数据丢失了</li></ol><h6 id="PushData失败-多次失败"><a href="#PushData失败-多次失败" class="headerlink" title="PushData失败/多次失败"></a>PushData失败/多次失败</h6><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172110777.png" alt="image-20220228172110777"></p><ol><li>启动MapTask, Task之中携带Reduce对应Server地址, 一个Reduce对应2个地址, 正常情况只会使用前面一个</li><li>MapTask计算Partition任务会将数据写入到临时</li><li>向Server写入数据</li><li>写入失败,返回错误</li><li>从临时文件之中获取Partition数据</li><li>重试写入(第一次失败的时候, 依然写入老地址, 第二次失败时, 写入备用地址)</li><li>重试, 直到达到达到最大次数</li><li>如果重试未成功, 将失败的节点加入黑名单, 然后重新运行整个Stage</li><li>如果重试成功, 启动Reduce任务, 此时需要指明备节点是否存有数据.</li><li>此时对应整个Reduce可能两个节点都存在数据, 因此需要向两个节点拉取数据</li></ol><blockquote><p>PushData的Block粒度为单个Partition, 如果再小, 那么数据一致性就无法保证了, 这里编程时候需要注意</p></blockquote><h6 id="关于Block重试的详细说明"><a href="#关于Block重试的详细说明" class="headerlink" title="关于Block重试的详细说明"></a>关于Block重试的详细说明</h6><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172158952.png" alt="image-20220228172158952"></p><ol><li>网络写失败, 无妨, 重试即可</li><li>写本地失败, 写了一些, 需要Stage失败</li><li>写commit文件失败, 需要Stage失败</li><li>网络返回失败, Server做到幂等</li></ol><p>2和3的问题, 大致是因为服务节点问题导致的, 这个目前比较难以处理, 先不管了[TODO]<br>如果做到幂等, 需要在元数据信息里面记载对应的taskAttemptId</p><h5 id="数据重复"><a href="#数据重复" class="headerlink" title="数据重复"></a>数据重复</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172216824.png" alt="image-20220228172216824"></p><ol><li>Driver启动Map任务</li><li>开始写Partition数据, 成功写入P0</li><li>但P1数据还没有完成写入</li><li>此时Task任务遇到异常退出</li><li>Driver启动Task任务重试</li><li>Task又重复写入一遍P0数据</li><li>同时也完成P1数据的写入, 此时系统之中有2份P0, 1份P1. 由于写入数据时, 含有Task信息, 因此Server知道2份P0数据, 分别由哪个Task写入</li><li>Executor向Driver 确认成功的Task</li><li>Driver向Manager发送Map任务也完成了, 并将正常完成任务的TaskId信息发给Manager</li><li>Manager直接将这些信息下发到Server</li><li>Driver启动Reduce任务</li><li>Reduce任务拉取数据的时候, 如果发现数据块对应的TaskId不在commitTask列表之中, 就会自动跳过这个数据块</li></ol><blockquote><p>commitTask下发给RSS的话, 容易造成元数据膨胀问题, 但如果将这些元数据放在Spark里面的话, 就会造成代码耦合程度太大了. 因此先看看编程时候能不能将代码耦合去除, 如果去除的话, 直接在ReduceTask测过滤</p><p>推测执行的难点在于, 两个独立进程写入同一个数据块, 那么在Server端就必须加锁来防止竞争, 但是一旦加锁, 性能就会收到影响</p></blockquote><h5 id="拉取失败"><a href="#拉取失败" class="headerlink" title="拉取失败"></a>拉取失败</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172232885.png" alt="image-20220228172232885"></p><ol><li>Driver启动Reduce任务</li><li>ReduceTask开始拉取数据, 但是返回异常</li><li>此时跟MapTask一样, 先开始Partition级别拉取重试, 如果第二次失败的时候, 开始拉取备份节点数据. 为了防止重复, 拉取成功的数据块, 需要记录对应的Task号, 拉取备份节点的时候, 需要重新过滤</li><li>如果Partition级别的重试未成功</li><li>上报给Driver, 准备重试Task</li><li>下发重试任务到新的Executor</li><li>此时如果一直失败的话, 有两个选择, 一个是直接宣布App失败了, 另外就是重选上一个Stage, 先选第一种</li></ol><h5 id="Server失败"><a href="#Server失败" class="headerlink" title="Server失败"></a>Server失败</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172249213.png" alt="image-20220228172249213"></p><ol><li>以MapTask为例, ReduceTask其实也类似</li><li>MapTask开始写入数据</li><li>此时对应Server进程shutdown了, 对应Manager来就是心跳消失</li><li>MapTask首先会失败, 失败后会再次重试, 一般partition级别重试会一直失败, 那么就会换一个节点写入.</li><li>此时如果Shuffle Server能马上重启, 重新接入心跳, 那么Manager就会当没事发送. 如果超过一定时间, 还没有重启, 那么Manager会告之Driver数据丢失(这里可以被动的)</li><li>Driver发现DataLost之后, 先查看是否开启数据备份功能, 如果有, 则继续. 如果没有开启数据备份, 那么就开始停止整个Stage, 等待重新下发全部的任务(如果有Reduce任务, 且数据没有开启备份功能, 则直接APP失败退出)</li></ol><blockquote><p>如果Server重启且不能恢复的话, 直接让APP失败, 由上层平台重试也许是个最好的方案</p></blockquote><p>另外一种方式, Service一旦失败, 重试这个Stage, 上一个Stage的数据必须在DFS中有备份, 这样才能重试Stage.</p><h4 id="数据一致性"><a href="#数据一致性" class="headerlink" title="数据一致性"></a>数据一致性</h4><p>数据的一致性问题, 其实在上面已经单独说过了, 但这里还是要单独拿出来看的, 因为数据不一致, 结果一般就不对了.<br>不一致情况的原有有以下几种:</p><ul><li>MapTask重试, 一些数据写了两遍</li><li>推测执行数据有两份</li><li>Server失败, 导致一份数据写到两个Server, 这时有些数据重复了, 有些数据没重复<br>解决上面的问题的思路, 就是读的时候, 需要过滤多余的数据.<br>对于同一个Server写了2遍, 这时通过CommittedTaskId过滤掉多余Task产生的数据<br>对于多个Server数据, 读取的时候, 要记录哪些Task任务已经被消费了, 如果被消费了就不需要重新读取了</li></ul><h4 id="数据可靠性"><a href="#数据可靠性" class="headerlink" title="数据可靠性"></a>数据可靠性</h4><p>数据备份功能, 一般有几个问题要选择: 谁来备份, 怎么备份, 备份到哪儿</p><h5 id="谁来备份"><a href="#谁来备份" class="headerlink" title="谁来备份"></a>谁来备份</h5><p>一般有客户端和服务端备份, 客户端备份,就是客户端直接写多份数据, 服务端备份的话, 客户端只写一份数据, 然后服务端自己写到备份地方去<br>客户端备份的优点是简单, 缺点点网络连接多些, 并且与后端耦合<br>服务端是反过来的<br>我们这儿选服务端备份, 因为备份到哪儿还不是很确定</p><h5 id="怎么备份"><a href="#怎么备份" class="headerlink" title="怎么备份"></a>怎么备份</h5><p>这儿有同步和异步的选项, 因为是临时数据, 所以Shuffle的备份最多应该就2个, 不会有半异步的选项<br>同步的特点是, 速度慢, 但是能保证数据一致<br>异步的话, 性能会好, 但是容易导致数据并没有完全ready<br>我们这儿选同步, 因为如果是异步的话, 上面的任务可靠性的处理会更加复杂一些, 这个性能等后续再优化吧</p><h5 id="备份到哪儿"><a href="#备份到哪儿" class="headerlink" title="备份到哪儿"></a>备份到哪儿</h5><p>在我们的系统里面有两个选项, 其他Server或者DFS.<br>如果放到其他实例里面的话, 因为机器也是SSD的, 所以性能会好一些, 但是DFS性能会比较差, 不过稳定性会比较好, 不需要考虑数据丢失的问题了<br>这里优先DFS, 因为DFS有人维护.</p><h3 id="可用性"><a href="#可用性" class="headerlink" title="可用性"></a>可用性</h3><h4 id="主备切换"><a href="#主备切换" class="headerlink" title="主备切换"></a>主备切换</h4><p>主备切换的能力依靠ZK来完成:</p><ul><li>客户端和Server会监控ZK的地址, 如果Manager地址变化之后, 会主动切换地址和端口</li><li>主Manager会在ZK上写入数据, 而备Manager会一直监控着, 如果发现节点丢失, 即主Manager失联, 备Manager会读取Zookeeper上的元数据, 然后变成主Manager写入数据.</li></ul><h4 id="主备切换-任务不中断"><a href="#主备切换-任务不中断" class="headerlink" title="主备切换, 任务不中断"></a>主备切换, 任务不中断</h4><p>做到任务不中断, 需要有以下条件</p><ul><li>切换时间短</li><li>无任何不一致状态, 或者状态都可以恢复</li><li>连接重试<br>要做到切换时间短, 需要备节点时刻监控主节点的状态, 不能落后太多, 不然就会启动延迟<br>尽量将必要的元数据信息都写入到ZK之中, 但ZK并非更新友好型存储系统, 因此也需要实现中间状态重构的能力,而这个重构不能出现状态丢失<br>客户端的所有RPC请求, 都要有重试功能, 一旦发现主备切换, RPC失败之后, 迅速切换到另外的节点.</li></ul><h3 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h3><h4 id="页面"><a href="#页面" class="headerlink" title="页面"></a>页面</h4><p>参考Livy的实现, 只实现到APP级别即可<br>APP: Id, 提交时间, 结束时间, 时长, 当前Shuffle文件个数, 当前shuffle存储总量</p><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><p>App总量: 个数, shuffle存储量, 文件总个数<br>资源容量: 磁盘,内存slot, cpu<br>JVM相关: 内存, 线程</p><h4 id="慢节点告警"><a href="#慢节点告警" class="headerlink" title="慢节点告警"></a>慢节点告警</h4><p>如何定义慢节点?</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><h4 id="兼容性方案"><a href="#兼容性方案" class="headerlink" title="兼容性方案"></a>兼容性方案</h4><p>兼容性包含客户端和服务端之间,以及服务端Manager和Server时间.<br>客户端目前只会支持Spark2.4.3版本以及Spark3.0版本.服务端需要同时支持这两个版本的接入, 因此整体差异只会在于Spark与Client接口层次, RssServer的Client和Server之间的通信兼容性必然要遵循. 可以适当的在控制流监控预留部分json字段来保持未来的兼容.<br>服务端的Manager和Server的RPC也同上诉方式.<br>如果后续改动实在无法, 通过灰度升级的方案处理</p><h4 id="多版本支持"><a href="#多版本支持" class="headerlink" title="多版本支持"></a>多版本支持</h4><p>由于兼容性和容错的考虑, Push方式的Shuffle Service很难实现滚动升级, 原因在于每个实例都有很多的网络连接, 一旦重启, SparkTask就会出现异常, 造成大规模的任务重试, 对集群会产生巨大的压力. 因此只能采用灰度升级方案, 发布也必须支持多版本特性.</p><p>每个版本发布包都有一个版本号, 进程启动是会将版本号写入到Zookeeper的元数据之中, 客户端会根据自身的版本号, 选择对应的路径, 然后获取到ShuffleManager的地址, 完成整个任务的启动环境.</p><p>所以, 整个客户端/RssManager/RssServer的版本都是配套的. 整套环境之中, 目标最多只能存在3个版本.<br>为了防止用户客户端死活不升级情况, Spark加载版本和jar包方式, 将通过Zookeeper获取元数据, 然后启动的时候, 远程加载配置项和jar文件, 这段代码需要注入到Spark之中, 而非RSS.</p><h4 id="部署方案"><a href="#部署方案" class="headerlink" title="部署方案"></a>部署方案</h4><div class="table-container"><table><thead><tr><th>实例角色</th><th>最少个数</th><th>推荐个数</th><th>部署位置</th></tr></thead><tbody><tr><td>Zookeeper</td><td>3</td><td>5</td><td>单独节点, 至少独立磁盘</td></tr><tr><td>RssManager</td><td>2</td><td>2</td><td>Master单独一个节点, Slave单独一个节点</td></tr><tr><td>RssServer</td><td>5</td><td>30</td><td>单独节点</td></tr></tbody></table></div><ol><li>每个节点, 对于单个版本, 只部署单个实例</li><li>机器尽量在同一机房, 网络带宽尽量大, 因为需要内部传输备份数据</li><li>RssManager的资源尽量在8核16G以上, 防止RPC和元数据处理成为集群瓶颈</li><li>RssServer机器尽量是SSD盘, 每个节点过挂一些数据盘, 尽量满足1核2G1TB的配置方式</li></ol><h4 id="升级方案"><a href="#升级方案" class="headerlink" title="升级方案"></a>升级方案</h4><p>整体采用灰度升级方案, 即每个节点会部署多个实例, <code>RssManager</code>和<code>RssServer</code>各自部署一套, 两者元数据分开存储, 但调度器会感知多个版本的任务情况.<br>整体升级步骤如下:</p><ol><li>部署新版本的<code>RssManager</code>, 在standby的机器上部署另外一个备<code>RssManager</code></li><li>逐个安装新的版本的<code>RssServer</code>, 完成后并添加新版本的监控告警</li><li>更新新版本客户端到HDFS</li><li>更新Zookeeper上最新RSS客户端版本</li><li>过两天, 查看老版本的<code>RssManager</code>的负载, 如果任务数已经降低为0, 则准备下线版本</li><li>关闭监控告警, 逐个关闭Server, 最后关闭两个Manager.</li></ol><h5 id="Zookeeper节点切换"><a href="#Zookeeper节点切换" class="headerlink" title="Zookeeper节点切换"></a>Zookeeper节点切换</h5><p>整个集群之中, Zookeeper虽然不是单点的, 但是ZK集群确实单点的, 一旦节点老旧必须替换升级或者IP切换的时候, ZK地址已经变换, RSS和Spark配置都要相对的变化, 这时就必须重启完成, 此时必须<strong>容忍任务大规模重试</strong>.</p><h3 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h3><h4 id="鉴权"><a href="#鉴权" class="headerlink" title="鉴权"></a>鉴权</h4><p>内部集群可以先不考虑</p><h4 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h4><p>Shuffle数据作为临时数据, 用户即可清理, 可以先不做完整性校验(目前Spark也没有做)</p><h4 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h4><p>Shuffle数据根据App粒度清理, Yarn Shuffle Service之中, Driver在退出的时候, 会清理对应的数据, 但不删除目录. 目录有Yarn感知到App状态为完成之后, 下发NodeManager完成清理工作.<br>但在Shuffle Service之中, 由于无法感知App状态, 因此需要Driver来主动清理. </p><ul><li>在Driver退出的过程之中, 会向Manager发送AppEnd的消息, 接收到请求之后, 开始清理App对应的Shuffle, 想各个Service发送完毕异步请求之后, 将App的状态标记为Deleted, 过一段时间后删除该状态</li><li>如果Driver异常退出, 度过静默期(无RPC往来的时长)之后ShuffleManager主动查询Yarn上App的状态, 如果为退出状态, 则主动删除数据.</li><li>ShuffleManager如果主备切换, 由于App的信息保存在Zookeeper之中, 因此备节点依然会完成清理工作</li><li>ShuffleService实例如果发重启, 那么它会主动询问是否有资源未清理, 如果发现本地APP的在状态为Deleted, 或者查询不到.</li></ul><h3 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h3><h4 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h4><p>资源的类型有以下几类:</p><ol><li>磁盘容量</li><li>内存Buffer量</li><li>IO负载压力</li><li>CPU负载压力</li></ol><h4 id="DataLocation"><a href="#DataLocation" class="headerlink" title="DataLocation"></a>DataLocation</h4><p>分配Server节点的时候, 需要返回机架信息, 方便Reduce任务决策启动在哪些几点之上.</p><h3 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h3><h4 id="支持多磁盘写入"><a href="#支持多磁盘写入" class="headerlink" title="支持多磁盘写入"></a>支持多磁盘写入</h4><p>文件磁盘目录, 也需要根据Spark一样规划为多级目录, 同时支持多个SSD目录<br>文件也会根据AppId + ShuffleId + PartitionId的hash值计算出对应的目录路径</p><h4 id="小数据块合并发送"><a href="#小数据块合并发送" class="headerlink" title="小数据块合并发送"></a>小数据块合并发送</h4><p>Map写入, 如果数据块比较小, 且写入同一个Server, 则可以合并发送.</p><h4 id="文件流式读取"><a href="#文件流式读取" class="headerlink" title="文件流式读取"></a>文件流式读取</h4><p>对于Reduce任务, 如果Partition实在太大, 可以根据流式方式读取</p><h4 id="小数据块合并读取"><a href="#小数据块合并读取" class="headerlink" title="小数据块合并读取"></a>小数据块合并读取</h4><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228172327830.png" alt="image-20220228172327830"><br>例如上图, 在shuffleRead的时候,需要花费近6个小时, 主要原因在于网络小io太多了</p><h4 id="2GB限制"><a href="#2GB限制" class="headerlink" title="2GB限制"></a>2GB限制</h4><p>Spark在Shuffle的时候, 已经通过<code>DownloadManager</code>, 通过文件的方式解决了2GB的限制, 但目前这儿依然由这个限制.<br>如何解决问题?  仿照<code>Spark</code>的处理方式, 如果发现数据量大于2GB, 则启动文件发送, 服务端需要重新定义<code>handler</code>, 整个数据也写入到临时文件之中, 不要写在原有RSS的数据文件之中.<br><code>ShuffleDataWrapper</code>需要加入一个新的字段, 或者将<code>data_length</code>设置为负值</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Clickhouse研究]: Clickhouse的同步锁</title>
      <link href="/2022/02/28/Clickhouse%E7%A0%94%E7%A9%B6-Clickhouse%E7%9A%84%E5%90%8C%E6%AD%A5%E9%94%81/"/>
      <url>/2022/02/28/Clickhouse%E7%A0%94%E7%A9%B6-Clickhouse%E7%9A%84%E5%90%8C%E6%AD%A5%E9%94%81/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Clickhouse的存储锁"><a href="#Clickhouse的存储锁" class="headerlink" title="Clickhouse的存储锁"></a>Clickhouse的存储锁</h1><h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><p>代码主要在<code>IStorage</code>之中<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">IStorage</span> </span><br><span class="line">&#123;</span><br><span class="line">TableLockHolder lockForShare;</span><br><span class="line">TableLockHolder lockForAlter;</span><br><span class="line">TableExclusiveLockHolder lockExclusively;</span><br><span class="line"><span class="keyword">mutable</span> RWLock alter_lock;</span><br><span class="line"><span class="keyword">mutable</span> RWLock drop_lock;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>从代码的定义上来,  只有drop和alter相关的才会加锁</p><p>排它锁<code>lockExclusively</code>的锁同时锁住<code>alter_lock</code>和<code>drop_lock</code>, 锁类型为写锁<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TableExclusiveLockHolder <span class="title">IStorage::lockExclusively</span><span class="params">(<span class="type">const</span> String &amp; query_id, <span class="type">const</span> std::chrono::milliseconds &amp; acquire_timeout)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    TableExclusiveLockHolder result;</span><br><span class="line">    result.alter_lock = <span class="built_in">tryLockTimed</span>(alter_lock, RWLockImpl::Write, query_id, acquire_timeout);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (is_dropped)</span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">Exception</span>(<span class="string">&quot;Table is dropped&quot;</span>, ErrorCodes::TABLE_IS_DROPPED);</span><br><span class="line"></span><br><span class="line">    result.drop_lock = <span class="built_in">tryLockTimed</span>(drop_lock, RWLockImpl::Write, query_id, acquire_timeout);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>排它锁只有很少的地方用到<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228152910967.png" alt="image-20220228152910967"></p><ol><li>在renameTable时候用到</li><li>在DropTable的时候用到</li><li>在restartReplica时候用到</li></ol><p>修改锁<code>lockForAlter</code>只锁了<code>alter</code>, 锁类型为写锁<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TableLockHolder <span class="title">IStorage::lockForAlter</span><span class="params">(<span class="type">const</span> String &amp; query_id, <span class="type">const</span> std::chrono::milliseconds &amp; acquire_timeout)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    TableLockHolder result = <span class="built_in">tryLockTimed</span>(alter_lock, RWLockImpl::Write, query_id, acquire_timeout);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (is_dropped)</span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">Exception</span>(<span class="string">&quot;Table is dropped&quot;</span>, ErrorCodes::TABLE_IS_DROPPED);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>都只在修改DDL时候用到: DDL语句执行和复制表的同步DDL时<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228152931062.png" alt="image-20220228152931062"></p><p>共享锁<code>lockForShare</code>的锁加入在<code>drop_lock</code>之中, 锁类型为读锁<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">TableLockHolder <span class="title">IStorage::lockForShare</span><span class="params">(<span class="type">const</span> String &amp; query_id, <span class="type">const</span> std::chrono::milliseconds &amp; acquire_timeout)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    TableLockHolder result = <span class="built_in">tryLockTimed</span>(drop_lock, RWLockImpl::Read, query_id, acquire_timeout);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (is_dropped)</span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">Exception</span>(<span class="string">&quot;Table is dropped&quot;</span>, ErrorCodes::TABLE_IS_DROPPED);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>共享锁调用的地方非常多, 常见的Insert语句也是排它锁.</p><p>根据代码推论:</p><ol><li>插入过程之中删除插入表, 删除动作会等待插入完成再执行</li><li>插入过程之中修改插入表, 表结构能够修改成功</li><li>Insert语句锁住整个query执行时间</li></ol><h2 id="测试验证"><a href="#测试验证" class="headerlink" title="测试验证"></a>测试验证</h2><h3 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `lineorder_local`</span><br><span class="line">(</span><br><span class="line">    LO_ORDERKEY             UInt32,</span><br><span class="line">    LO_LINENUMBER           UInt8,</span><br><span class="line">    LO_CUSTKEY              UInt32,</span><br><span class="line">    LO_PARTKEY              UInt32,</span><br><span class="line">    LO_SUPPKEY              UInt32,</span><br><span class="line">    LO_ORDERDATE            <span class="type">Date</span>,</span><br><span class="line">    LO_ORDERPRIORITY        LowCardinality(String),</span><br><span class="line">    LO_SHIPPRIORITY         UInt8,</span><br><span class="line">    LO_QUANTITY             UInt8,</span><br><span class="line">    LO_EXTENDEDPRICE        UInt32,</span><br><span class="line">    LO_ORDTOTALPRICE        UInt32,</span><br><span class="line">    LO_DISCOUNT             UInt8,</span><br><span class="line">    LO_REVENUE              UInt32,</span><br><span class="line">    LO_SUPPLYCOST           UInt32,</span><br><span class="line">    LO_TAX                  UInt8,</span><br><span class="line">    LO_COMMITDATE           <span class="type">Date</span>,</span><br><span class="line">    LO_SHIPMODE             LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYear(LO_ORDERDATE) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (LO_ORDERDATE, LO_ORDERKEY);</span><br></pre></td></tr></table></figure><h3 id="插入语句"><a href="#插入语句" class="headerlink" title="插入语句"></a>插入语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.<span class="operator">/</span>clickhouse<span class="operator">-</span>client  <span class="comment">--query &quot;INSERT INTO hzw.lineorder_local FORMAT CSV&quot; &lt; lineorder.tbl</span></span><br></pre></td></tr></table></figure><h3 id="插入过程之中删除插入表"><a href="#插入过程之中删除插入表" class="headerlink" title="插入过程之中删除插入表"></a>插入过程之中删除插入表</h3><p>删除表语句<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> lineorder_local;</span><br></pre></td></tr></table></figure><br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228152954941.png" alt="image-20220228152954941"></p><p>出现无法获取锁的问题</p><h3 id="插入过程之中修改插入表"><a href="#插入过程之中修改插入表" class="headerlink" title="插入过程之中修改插入表"></a>插入过程之中修改插入表</h3><p> 修改表语句<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> lineorder_local <span class="keyword">drop</span> <span class="keyword">column</span> LO_QUANTITY;</span><br></pre></td></tr></table></figure><br> 执行后, 会立马成功<br> <img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228153018786.png" alt="image-20220228153018786"></p><p>由于乱序执行, 测试一个更加夸张的例子<br> <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> lineorder_local <span class="keyword">delete</span> <span class="keyword">where</span> <span class="number">1</span><span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><br> <img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220228153033588.png" alt="image-20220228153033588"><br> 这里alter立马会成功, 数据会删除, 但是insert还在继续.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Clickhouse研究]: Clickhouse的WAL功能</title>
      <link href="/2022/02/28/Clickhouse%E7%A0%94%E7%A9%B6-Clickhouse%E7%9A%84WAL%E5%8A%9F%E8%83%BD/"/>
      <url>/2022/02/28/Clickhouse%E7%A0%94%E7%A9%B6-Clickhouse%E7%9A%84WAL%E5%8A%9F%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Clickhouse的WAL功能"><a href="#Clickhouse的WAL功能" class="headerlink" title="Clickhouse的WAL功能"></a>Clickhouse的WAL功能</h1><h2 id="设计目的"><a href="#设计目的" class="headerlink" title="设计目的"></a>设计目的</h2><p>解决小批量数据写入时, 频繁写入dataPart, 导致磁盘繁忙的问题或者出现<code>DB::Exception: Too many parts</code></p><p>具体可以参考这篇<a href="https://bohutang.me/2020/08/18/clickhouse-and-friends-merge-tree-wal/">文档</a></p><blockquote><p>并非binlog的模式, 数据flush后, 会清理wal文件</p></blockquote><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>首先看写逻辑的入口, 在<code>MergeTreeDataWriter::writeTempPart</code>创建了一个<code>MergeTreeData::MutableDataPartPtr</code>的类型<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">MergeTreeData::MutableDataPartPtr <span class="title">MergeTreeDataWriter::writeTempPart</span><span class="params">(BlockWithPartition &amp; block_with_partition, <span class="type">const</span> StorageMetadataPtr &amp; metadata_snapshot)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">auto</span> new_data_part = data.<span class="built_in">createPart</span>(</span><br><span class="line">        part_name,</span><br><span class="line">        data.<span class="built_in">choosePartType</span>(expected_size, block.<span class="built_in">rows</span>()),</span><br><span class="line">        new_part_info,</span><br><span class="line">        <span class="built_in">createVolumeFromReservation</span>(reservation, volume),</span><br><span class="line">        TMP_PREFIX + part_name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br><code>MergeTreeData::MutableDataPartPtr</code>有三种实现, <code>IN_MEMORY</code>是写入到内存中, 由于内存是易失的, 所以需要WAL功能的辅助.<code>COMPACT</code>和<code>WIDE</code>都是文件存储, 但文件编码和压缩方式不同.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">MergeTreeData::MutableDataPartPtr <span class="title">MergeTreeData::createPart</span><span class="params">(<span class="type">const</span> String &amp; name,</span></span></span><br><span class="line"><span class="params"><span class="function">    MergeTreeDataPartType type, <span class="type">const</span> MergeTreePartInfo &amp; part_info,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> VolumePtr &amp; volume, <span class="type">const</span> String &amp; relative_path)</span> <span class="type">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (type == MergeTreeDataPartType::COMPACT)</span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">make_shared</span>&lt;MergeTreeDataPartCompact&gt;(*<span class="keyword">this</span>, name, part_info, volume, relative_path);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (type == MergeTreeDataPartType::WIDE)</span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">make_shared</span>&lt;MergeTreeDataPartWide&gt;(*<span class="keyword">this</span>, name, part_info, volume, relative_path);</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (type == MergeTreeDataPartType::IN_MEMORY)</span><br><span class="line">        <span class="keyword">return</span> std::<span class="built_in">make_shared</span>&lt;MergeTreeDataPartInMemory&gt;(*<span class="keyword">this</span>, name, part_info, volume, relative_path);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">Exception</span>(<span class="string">&quot;Unknown type of part &quot;</span> + relative_path, ErrorCodes::UNKNOWN_PART_TYPE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>上面的逻辑, 主要是根据<code>MergeTreeDataPartType</code>的类型选择, 那么我们看一下, 上面什么时候会选择该类型: 当part的文件小于<code>min_bytes_for_compact_part</code>或者行数小于<code>min_rows_for_compact_part</code>, 只要满足一种一项即可.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">MergeTreeDataPartType <span class="title">MergeTreeData::choosePartType</span><span class="params">(<span class="type">size_t</span> bytes_uncompressed, <span class="type">size_t</span> rows_count)</span> <span class="type">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> settings = <span class="built_in">getSettings</span>();</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">canUsePolymorphicParts</span>(*settings))</span><br><span class="line">        <span class="keyword">return</span> MergeTreeDataPartType::WIDE;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (bytes_uncompressed &lt; settings-&gt;min_bytes_for_compact_part || rows_count &lt; settings-&gt;min_rows_for_compact_part)</span><br><span class="line">        <span class="keyword">return</span> MergeTreeDataPartType::IN_MEMORY;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (bytes_uncompressed &lt; settings-&gt;min_bytes_for_wide_part || rows_count &lt; settings-&gt;min_rows_for_wide_part)</span><br><span class="line">        <span class="keyword">return</span> MergeTreeDataPartType::COMPACT;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> MergeTreeDataPartType::WIDE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>那么回过头来看<code>MergeTreeDataPartInMemory</code>的实现</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MergeTreeDataPartInMemory</span> : <span class="keyword">public</span> IMergeTreeDataPart</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">// 忽略部分函数</span></span><br><span class="line">  <span class="keyword">mutable</span> Block block;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从源码上看, <code>MergeTreeDataPartInMemory</code>就是原本Block写入本地磁盘, 而它则放入到内存中. </p><p>那么如果进程重启, 内存中数据丢失后, 该如何处理呢?</p><p>答案在于<code>MergeTreeWriteAheadLog</code>的功能中, 将用户数据写入到WAL.</p><p>当进程启动的时候, 会调用<code>restore</code>从WAL的文件中, 将数据恢复到内存中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">MergeTreeData::MutableDataPartsVector <span class="title">MergeTreeWriteAheadLog::restore</span><span class="params">(<span class="type">const</span> StorageMetadataPtr &amp; metadata_snapshot)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> (action_type == ActionType::ADD_PART)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">auto</span> part_disk = storage.<span class="built_in">reserveSpace</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDisk</span>();</span><br><span class="line">                <span class="keyword">auto</span> single_disk_volume = std::<span class="built_in">make_shared</span>&lt;SingleDiskVolume&gt;(<span class="string">&quot;volume_&quot;</span> + part_name, disk);</span><br><span class="line"></span><br><span class="line">                part = storage.<span class="built_in">createPart</span>(</span><br><span class="line">                    part_name,</span><br><span class="line">                    MergeTreeDataPartType::IN_MEMORY,</span><br><span class="line">                    MergeTreePartInfo::<span class="built_in">fromPartName</span>(part_name, storage.format_version),</span><br><span class="line">                    single_disk_volume,</span><br><span class="line">                    part_name);</span><br><span class="line"></span><br><span class="line">                block = block_in.<span class="built_in">read</span>();</span><br><span class="line">            &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么wal文件会越写越多, 什么时候会开始清理部分数据呢?</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">MergeTreeWriteAheadLog::addPart</span><span class="params">(<span class="type">const</span> Block &amp; block, <span class="type">const</span> String &amp; part_name)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 写数据到WAL</span></span><br><span class="line">    block_out-&gt;<span class="built_in">write</span>(block);</span><br><span class="line">    block_out-&gt;<span class="built_in">flush</span>();</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">auto</span> max_wal_bytes = storage.<span class="built_in">getSettings</span>()-&gt;write_ahead_log_max_bytes;</span><br><span class="line">    <span class="keyword">if</span> (out-&gt;<span class="built_in">count</span>() &gt; max_wal_bytes)</span><br><span class="line">        <span class="built_in">rotate</span>(lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在<code>addPart</code>的逻辑之中, 会检查WAL文件的大小, 当文件大于<code>write_ahead_log_max_bytes</code>(默认为1GB)时, 开始清理WAL文件</p><p>另外一个问题, <strong>WAL的内存部分数据存放在哪儿</strong>, 在insert的时候(<code>renameTempPartAndReplace</code>), 数据会放到<code>data_parts_indexes.insert</code>之中, read时候从这里读取数据</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">MergeTreeData::renameTempPartAndReplace</span><br><span class="line">&#123;</span><br><span class="line">    part-&gt;name = part_name;</span><br><span class="line">    part-&gt;info = part_info;</span><br><span class="line">    part-&gt;is_temp = <span class="literal">false</span>;</span><br><span class="line">    part-&gt;state = DataPartState::PreCommitted;</span><br><span class="line">    part-&gt;<span class="built_in">renameTo</span>(part_name, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> part_it = data_parts_indexes.<span class="built_in">insert</span>(part).first;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>数据恢复的时候, <code>loadDataParts</code>的时候, 数据被读取出来, 然后插入到<code>data_parts_indexes</code>之中, 通过<code>getActiveContainingPart</code>过滤重复的数据<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">MergeTreeData::loadDataParts</span><br><span class="line">&#123;</span><br><span class="line">    for (auto &amp; part : parts_from_wal)</span><br><span class="line">   &#123;</span><br><span class="line">       if (getActiveContainingPart(part-&gt;info, DataPartState::Committed, part_lock))</span><br><span class="line">           continue;</span><br><span class="line"></span><br><span class="line">       part-&gt;modification_time = time(nullptr);</span><br><span class="line">       /// Assume that all parts are Committed, covered parts will be detected and marked as Outdated later</span><br><span class="line">       part-&gt;state = DataPartState::Committed;</span><br><span class="line"></span><br><span class="line">       if (!data_parts_indexes.insert(part).second)</span><br><span class="line">           throw Exception(&quot;Part &quot; + part-&gt;name + &quot; already exists&quot;, ErrorCodes::DUPLICATE_DATA_PART);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到Clickhouse的WAL机制, 并没有RocksDB的那种MemTable, 因此两个批次的数据, 并不会在内存中合并. </p><p>所有的合并操作, 依然由后台线程来处理,  支持在合并的流程中, 抽象为2个DataPart的合并, 但实际上可以是一个InMem的DP和一个OnDisk的DP做合并.</p><p>Clickhouse的合并后的数据都写入到磁盘中.</p><p>另外一个点, 在复制表中, InMem的DP依然会做同步.</p><p>在<code>DataPartsExchange</code>有两个函数<code>sendPartFromMemory</code>和<code>downloadPartToMemory</code>, 前者用于发送数据, 后者用户下载数据, 同步后, 数据依然是InMem格式.</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>创建一张表, 指定<code>min_rows_for_compact_part</code>为200,write_ahead_log_max_bytes为8192(8K)<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.lineorder_local1</span><br><span class="line">(</span><br><span class="line">    `LO_ORDERKEY` UInt32,</span><br><span class="line">    `LO_LINENUMBER` UInt8,</span><br><span class="line">    `LO_CUSTKEY` UInt32,</span><br><span class="line">    `LO_PARTKEY` UInt32,</span><br><span class="line">    `LO_SUPPKEY` UInt32,</span><br><span class="line">    `LO_ORDERDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_ORDERPRIORITY` LowCardinality(String),</span><br><span class="line">    `LO_SHIPPRIORITY` UInt8,</span><br><span class="line">    `LO_QUANTITY` UInt8,</span><br><span class="line">    `LO_EXTENDEDPRICE` UInt32,</span><br><span class="line">    `LO_ORDTOTALPRICE` UInt32,</span><br><span class="line">    `LO_DISCOUNT` UInt8,</span><br><span class="line">    `LO_REVENUE` UInt32,</span><br><span class="line">    `LO_SUPPLYCOST` UInt32,</span><br><span class="line">    `LO_TAX` UInt8,</span><br><span class="line">    `LO_COMMITDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_SHIPMODE` LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYear(LO_ORDERDATE)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (LO_ORDERDATE, LO_ORDERKEY)</span><br><span class="line">SETTINGS index_granularity <span class="operator">=</span> <span class="number">8192</span>, min_rows_for_compact_part <span class="operator">=</span> <span class="number">200</span>,write_ahead_log_max_bytes<span class="operator">=</span><span class="number">8192</span>;</span><br></pre></td></tr></table></figure></p><p>插入100条数据<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> lineorder_local1 <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> lineorder_local limit <span class="number">100</span>;</span><br></pre></td></tr></table></figure></p><p>查看数据目录<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata-nmg-client01.nmg01 /var/lib/clickhouse/data/default/lineorder_local1]$ ll</span><br><span class="line">total 12</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse    6 Apr 12 14:53 detached</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse    1 Apr 12 14:53 format_version.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 4766 Apr 12 14:54 wal.bin</span><br></pre></td></tr></table></figure><br>出现了一个<code>wal.bin</code>的文件<br>再插入一次, 发现又出现了一个bin文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata-nmg-client01.nmg01 /var/lib/clickhouse/data/default/lineorder_local1]$ ll</span><br><span class="line">total 16</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse   10 Apr 12 15:18 detached</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse    1 Apr 12 15:18 format_version.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 9532 Apr 12 15:18 wal_1_2.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse    0 Apr 12 15:18 wal.bin</span><br></pre></td></tr></table></figure></p><p>多了一个<code>wal_1_2.bin</code>的文件, 我们在多插入几次, 到第5次插入的时候, 会生成一个datapart<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata-nmg-client01.nmg01 /var/lib/clickhouse/data/default/lineorder_local1]$ ll</span><br><span class="line">total 40</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse 4096 Apr 12 15:39 1992_1_5_1</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse   10 Apr 12 15:18 detached</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse    1 Apr 12 15:18 format_version.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 9532 Apr 12 15:18 wal_1_2.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 9532 Apr 12 15:26 wal_3_4.bin</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 4766 Apr 12 15:39 wal.bin</span><br></pre></td></tr></table></figure><br>再过一段时间观察, 发现<code>wal_*_*.bin</code>文件已经被删除了, 原因在于data_part已经commit的了<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata-nmg-client01.nmg01 /var/lib/clickhouse/data/default/lineorder_local1]$ ll</span><br><span class="line">total 16</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse 4096 Apr 12 15:39 1992_1_5_1</span><br><span class="line">drwxr-x--- 2 clickhouse clickhouse   10 Apr 12 15:18 detached</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse    1 Apr 12 15:18 format_version.txt</span><br><span class="line">-rw-r----- 1 clickhouse clickhouse 4766 Apr 12 15:39 wal.bin</span><br><span class="line">[root@bigdata-nmg-client01.nmg01 /var/lib/clickhouse/data/default/lineorder_local1]$</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Clickhouse问题]: 物化视图初始化数据丢失与重复问题</title>
      <link href="/2022/02/27/Clickhouse%E9%97%AE%E9%A2%98-%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE%E5%88%9D%E5%A7%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/"/>
      <url>/2022/02/27/Clickhouse%E9%97%AE%E9%A2%98-%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE%E5%88%9D%E5%A7%8B%E5%8C%96%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h2><p>现在CK创建视图有2种初始化数据的方式</p><ol><li><p>在 CREATE 语句中带上 polulate 关键字，会将执行时间点之前的底表的历史数据全部初始化到视图中。执行完成后的底表新数据也会进视图，但是执行过程中的<strong>数据会丢失</strong>；</p></li><li><p>在 CREATE 语句中不带 polulate 关键字，会将执行完成后的底表新数据会进视图，但是执行完成之前的数据都不会进视图；如果而后手工执行insert命令, 导入历史数据, 那么<strong>重复数据</strong></p></li></ol><h2 id="Polulate数据丢失"><a href="#Polulate数据丢失" class="headerlink" title="Polulate数据丢失"></a>Polulate数据丢失</h2><h3 id="根因分析"><a href="#根因分析" class="headerlink" title="根因分析"></a>根因分析</h3><p>先明晰一下MV更新的时候的时序图</p><p>下图中有4个角色: <strong>插入语句执行者, 底表插入数据执行者, MV1插入数据执行者, MV2创建语句执行者</strong> </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220227113952566.png" alt="image-20220227113952566"></p><p>首先, 对于底表来说, 第一批写开始时, 因为MV2并没有被创建, 因此对于它来说, 只会给MV1主动推送数据, 相当于这批次的数据全部丢失了.</p><p>其次, 对于MV2创建来说, 由于带有populate字段, 他会主动去拉取历史数据, 但是由于底表没有全部写完, 只写了DP1, 那么DP2和DP3的数据就丢失了.</p><p>因此, 总结来说, <strong>在插入过程之中创建底表会丢失的数据是, 在底表里DataPart没有写完的那些数据, 写完的数据还是会被读取的.</strong></p><p>最后, 看第二批写的情况, 由于检查MV的时候, MV1和MV2已经存在(虽然这个时候, 创建MV2的语句被没有结束, 但是由于CK没有一致性保证, 所以元数据的信息可以被外面捕获到), 此时第二批次的数据, 在正常情况下能够推送给MV2.</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p>利用存储的锁机制, 在插入时禁止populate操作</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220227114132890.png" alt="image-20220227114132890"></p><ol><li>写入时, 加入写入的共享锁, 可以支持同时插入到一个数据表中</li><li>执行create table的元数据创建时, 获取底表的排它锁, 如图中所示, 此时正好在插入, 则会等待插入执行完毕后, 再进场元数据操作</li><li>元数据执行完毕后立即释放锁, 耗时的populate阶段, 是无锁状态. 因此第二次插入, 只需要等待极小的一个时间即可执行数据插入动作.</li></ol><blockquote><p><strong>使用限制</strong></p><p>目前如果有大型insert的操作的话, 此时会无法完成基于底表的创建视图操作</p></blockquote><h3 id="复制表问题"><a href="#复制表问题" class="headerlink" title="复制表问题"></a>复制表问题</h3><p>目前视图使用create view to table的方式建立的, 如果视图里面带有populate, 就会出现MV多数据的情况, 原因如下图所示</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220227114313082.png" alt="image-20220227114313082"></p><p><strong>解决方案</strong></p><p>只允许一个节点写入, 跟DLAP-Manager交流, 目前复制表只有一个节点可写, 符合预期</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20220227114345189.png" alt="image-20220227114345189"></p><p>从上面的分析来看, 整体物化视图的初始化, 比较合适构建在<strong>外部的Manager</strong>中. 但此时就需要处理场景2中数据重复的问题</p><h2 id="Insert数据重复"><a href="#Insert数据重复" class="headerlink" title="Insert数据重复"></a>Insert数据重复</h2><p>Insert数据重复比较好了解, 因为是人工操作, 因此创建完毕物化视图, 和执行insert命令期间, 有可能已经有一些批次的数据写入到物化视图中, 如果此时导入全部的历史数据, 那么数据就会出现重复的情况.</p><blockquote><p>去重视图一般不影响, 但如果是聚合是视图, 那么结果将不正确.</p></blockquote><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>数据快照</p><h2 id="大底表的处理方案"><a href="#大底表的处理方案" class="headerlink" title="大底表的处理方案"></a>大底表的处理方案</h2><ol><li><p>创建视图时, 获取底表所有的分区</p></li><li><p>通过参数设置并发, 创建并发个数的临时表</p></li><li><p>按照分区修改插入语句, 此时引擎测需要保证: 该表达式能够完成分区裁剪, 并在实际执行时忽略掉判断(这是一个难点)</p><p>where splitByChar(‘<em>‘,_part)[1]=’1993’ and toInt32(splitByChar(‘</em>‘,_part)[3]) &lt; 500</p></li><li><p>临时表的插入, 只能有一个并发. 插入完毕后, 用attach parition的方式, 将分区加入到物化视图的底表中, 然后删除临时表, 再重新创建一个临时, 开启下一轮操作.</p></li><li><p>如果插入临时表时候, 系统需要自动清空(或者删除重建)临时表, 并重试. 如果attach时失败, 则返回异常, 由用户删除物化视图, 并开始重建工作.</p></li><li><p>期间Clickhouse Server节点如果出现异常, 则整个任务失败. 如果DLAP-Manager出现重启, 则可以继续redo整个任务.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkSQL: 小文件的处理方式</title>
      <link href="/2022/01/28/SparkSQL-%E5%B0%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/"/>
      <url>/2022/01/28/SparkSQL-%E5%B0%8F%E6%96%87%E4%BB%B6%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="自适应执行"><a href="#自适应执行" class="headerlink" title="自适应执行"></a>自适应执行</h2><p>社区在Spark2.3版本之后的AdaptiveExecute特性之中就能很好的解决Partition个数过多导致小文件过多的问题.<br>通过动态的评估Shuffle输入的个数(通过设置<code>spark.sql.adaptive.shuffle.targetPostShuffleInputSize</code>实现), 可以聚合多个Task任务, 减少Reduce的个数<br>使用方式:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.adaptive.enabled<span class="operator">=</span><span class="literal">true</span></span><br><span class="line"><span class="keyword">set</span> spark.sql.adaptive.shuffle.targetPostShuffleInputSize<span class="operator">=</span><span class="number">128</span>MB</span><br></pre></td></tr></table></figure><br>优点:</p><ul><li>自动根据任务的数据量进行聚合</li></ul><p>缺点:</p><ul><li>必须存在Shuffle过程, 否则不生效</li><li>任务的Shuffle输出比不能太低</li></ul><blockquote><p>Shuffle输出比, 为一个Shuffle任务中最后Output的数据量除以ShuffleRead的数据量的数值. 如果ShuffleRead为100GB, 而输出为1GB, 那么Shuffle输出比为1%. 如果这值比较低, 说明Task之中有很高强度的Filter功能. 这个数值太低会对系统产生比较大影响, 例如每个Shuffle块为<code>128MB</code>, 如果输出比为10%, 那么最后在HDFS之中只有<code>12.8MB</code>, 就如会出现小文件问题. 因此动态执行功能并不会对此产生太大的效果. 现实中, 由于SparkSQL已经有比较高效的<code>FilterPushDown</code>功能, 因此这个比例不太太高, 在在20%以上.</p></blockquote><h2 id="HINT方式"><a href="#HINT方式" class="headerlink" title="HINT方式"></a>HINT方式</h2><p>社区在Spark2.4版本之后引入HINT模式<a href="https://issues.apache.org/jira/browse/SPARK-24940">SPARK-24940</a>, 可以由用户来指定最后分区的个数, 只要在SQL语句之中加入注释文件<br>使用方式:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> ... <span class="keyword">SELECT</span> <span class="comment">/*+ COALESCE(numPartitions) */</span> ...</span><br><span class="line"><span class="keyword">INSERT</span> ... <span class="keyword">SELECT</span> <span class="comment">/*+ REPARTITION(numPartitions) */</span> ...</span><br></pre></td></tr></table></figure><br>优点:</p><ul><li>支持简单无Shuffle模式的Reparation</li></ul><p>缺点:</p><ul><li>需要人工干预, 设计Partition的个数, 而对于变化的场景来说, 难有一个固定的Partition个数</li><li>无法处理Shuffle输出比过低的场景</li></ul><h2 id="独立的小文件合并"><a href="#独立的小文件合并" class="headerlink" title="独立的小文件合并"></a>独立的小文件合并</h2><p>综上所诉两个方案都无法处理Shuffle输出比过低的场景, 因此我们需要一种兜底方案: 直接读取HDFS上的数据, 进行合并操作.<br>当插入任务完成之后, 新启动一个Job读取所有的数据, 然后根据设置的文件大小, 进行合并并会写到HDFS之中.<br>由于是直读直写的方式, 因此对于数据大小的评估是非常精确的, 因此可以很好的避免Shuffle输出比的问题.</p><p>优点:</p><ul><li>基本解决了小文件问题</li></ul><p>缺点:</p><ul><li>引入新的一次Job过程, 性能会受影响, 特别对中型任务会有一定的影响(10秒左右)</li></ul><p>使用方式:<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.merge.enabled<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> spark.sql.merge.size.per.task<span class="operator">=</span><span class="number">134217728</span>; <span class="comment">--128 * 1024 * 1024 bytes</span></span><br></pre></td></tr></table></figure><br>性能优化:<br>ORC和Parquet格式支持按行读取和按Stripe读取, Stripe读取可以认为是GroupRead, 由于不需要解析文件里面具体的数值, 因此可以按照Stripe粒度读取文件, 再写入文件之中, 以Stripe粒度合并文件.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.merge.mode<span class="operator">=</span>fast; <span class="comment">-- 默认是pretty, 是逐行读写文件的, 性能较慢</span></span><br></pre></td></tr></table></figure></p><blockquote><p>实际上说, 这种方式与启动独立合并的任务, 后台不停的合并是一样的, 只不过将这种插入到每个SQL任务中, 并自动完成了</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OLAP学习: 数学算法汇总</title>
      <link href="/2021/12/25/OLAP%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/"/>
      <url>/2021/12/25/OLAP%E5%AD%A6%E4%B9%A0-%E6%95%B0%E5%AD%A6%E7%AE%97%E6%B3%95%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="基数估计"><a href="#基数估计" class="headerlink" title="基数估计"></a>基数估计</h2><p>通俗的话来讲, 就是求<code>count distinct</code>,  在公司内部有大量UV场景, 因此一款数据库对<code>基数估计</code>的支持是一个非常重要的功能.</p><p>一般计算基数, 通过分布式构造Hash表, 进行group by求解, 但该方式非常消耗内存和CPU, 无法满足大型互联网的要求, 因此有以下两个优化方向. </p><h3 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h3><p>对于Long/Int型数据, 可以通过Bitmap方式来求解. Bitmap的存储和计算效率会明显优于Hash表.</p><p>普通的Bitmap结构比较清晰,  这里简单讲一下Roaring Bitmaps.</p><p>普通的bitmap为一个巨型数组, 而Roaring Bitmap会分为2级结构, 一级分桶, 总共有65535个(short最大值), 每个桶内则是short类型的bitmap, 可以存储65535个字节.</p><p>对于一个4字节的Int类型, 会分别取高16位和低16位, 也就是一个Int分裂为2个short类型.</p><p>前一个short表示index, 索引到具体的桶内, 后一个short在桶内的bitmap中操作.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/20211225095438.png" alt=""></p><p>short类型的bitmap有三种类型:</p><ol><li>array[short]类型</li><li>普通bitmap类型</li><li>RunLength类型</li></ol><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/4a6c83760984e3aa562432eeae79b399.png" alt="img"></p><p>对于String类型数据, 可以通过一个KV字典, 将String转化为Int后再行求解.</p><h3 id="HLL"><a href="#HLL" class="headerlink" title="HLL"></a>HLL</h3><p>以上面方法不同, HLL是一种<strong>近似计算</strong>基数的方式, 它是基于以下概率论的假设</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/82d51483cf2d377d799903748adf5f54.png" alt="img"></p><p>在真实求解的时候, HLL构造一个桶列表(byte类型, 图中是64个), 取hash值后6位(64=2^6), 索引对应的桶位置, 然后取第一个除首位1之外1的值,  最后将所有桶的数值, 求取调和平均数. (图中Loglog算法为算数平均数, 误差较大)</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/v2-db5df7bf6a2222ecbdf4f1c6ba357595_1440w.jpg" alt="img"></p><h2 id="百分数"><a href="#百分数" class="headerlink" title="百分数"></a>百分数</h2><h3 id="T-Digest"><a href="#T-Digest" class="headerlink" title="T-Digest"></a>T-Digest</h3><p>准确说tdigest并非是百分数计算方法, 而是一种<strong>抽样方式</strong>, 通过引入质心的概念, 完成类似于KNN聚类效果. </p><p>聚类之后, 数据量比原来会小很多,  然后再调用精确计算百分位的函数<code>quantile</code>进行计算.</p><p>T-Digest有两种方式, 一种称为<code>buffer and merge</code>, 另一种称为<code>cluster</code>, 整个算法过程主要在平衡误差和计算效率的结果.</p><p>具体算法就不学习了, 有需要时, 再来看这个<a href="https://blog.bcmeng.com/pdf/TDigest.pdf">pdf</a> </p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="BloomFilter"><a href="#BloomFilter" class="headerlink" title="BloomFilter"></a>BloomFilter</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/3.7.1.png" alt="bloomfilter"></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OLAP学习: 列存系统汇总</title>
      <link href="/2021/12/12/OLAP%E5%AD%A6%E4%B9%A0-%E5%88%97%E5%AD%98%E7%B3%BB%E7%BB%9F%E6%B1%87%E6%80%BB/"/>
      <url>/2021/12/12/OLAP%E5%AD%A6%E4%B9%A0-%E5%88%97%E5%AD%98%E7%B3%BB%E7%BB%9F%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="OLAP系统"><a href="#OLAP系统" class="headerlink" title="OLAP系统"></a>OLAP系统</h2><h3 id="开源系统"><a href="#开源系统" class="headerlink" title="开源系统"></a>开源系统</h3><h4 id="Clickhouse"><a href="#Clickhouse" class="headerlink" title="Clickhouse"></a>Clickhouse</h4><h4 id="StarRocks-Doris"><a href="#StarRocks-Doris" class="headerlink" title="StarRocks/Doris"></a>StarRocks/Doris</h4><h4 id="Impala-Kudu"><a href="#Impala-Kudu" class="headerlink" title="Impala/Kudu"></a>Impala/Kudu</h4><h4 id="Greenplum"><a href="#Greenplum" class="headerlink" title="Greenplum"></a>Greenplum</h4><h4 id="Druid"><a href="#Druid" class="headerlink" title="Druid"></a>Druid</h4><h4 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h4><h3 id="商业软件"><a href="#商业软件" class="headerlink" title="商业软件"></a>商业软件</h3><h4 id="AnalysisDatabase"><a href="#AnalysisDatabase" class="headerlink" title="AnalysisDatabase"></a>AnalysisDatabase</h4><h4 id="SQL-Server"><a href="#SQL-Server" class="headerlink" title="SQL Server"></a>SQL Server</h4><p><a href="https://dl.acm.org/doi/10.1145/1989323.1989448">2011</a></p><p><a href="https://dl.acm.org/doi/10.1145/2463676.2463708">2013</a></p><p><a href="https://dl.acm.org/doi/10.14778/2824032.2824071">2015</a></p><h4 id="TiFlash"><a href="#TiFlash" class="headerlink" title="TiFlash"></a>TiFlash</h4><p><a href="https://www.vldb.org/pvldb/vol13/p3072-huang.pdf">论文</a></p><h4 id="Oracle"><a href="#Oracle" class="headerlink" title="Oracle"></a>Oracle</h4><p><a href="https://ieeexplore.ieee.org/document/7113373">论文</a></p><p><a href="https://www.oracle.com/technetwork/database/in-memory/overview/twp-oracle-database-in-memory-2245633.pdf">Oracle19c</a></p><h4 id="SAP-HANA"><a href="#SAP-HANA" class="headerlink" title="SAP HANA"></a>SAP HANA</h4><p><a href="https://15799.courses.cs.cmu.edu/fall2013/static/papers/p731-sikka.pdf">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>StarRocks学习: 读写流程</title>
      <link href="/2021/12/10/StarRocks%E5%AD%A6%E4%B9%A0-%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/"/>
      <url>/2021/12/10/StarRocks%E5%AD%A6%E4%B9%A0-%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ReadListener(handleEvent) -&gt;ConnectProcessor(processOnce -&gt;dispatch-&gt;handleQuery) -&gt; StmtExecutor(execute)</span><br><span class="line">StmtExecutor(execute)</span><br><span class="line">    |-&gt; StatementPlanner(plan): Analyzer(analyze) -&gt; createQueryPlan: Optimizer -&gt; PlanFragmentBuilder</span><br><span class="line">    |-&gt; handleQueryStmt : Coordinator(<span class="built_in">exec</span>) -&gt; Coordinator(getNext) -&gt; MysqlChannel(sendOnePacket)</span><br><span class="line">Coordinator(<span class="built_in">exec</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">olap_scan_node -&gt; tabel_scanner -&gt; tablet_reader(_init_collector) -&gt; BetaRowset(get_segment_iterators) -&gt; SegmentIterator</span><br></pre></td></tr></table></figure><h3 id="Rowset-package"><a href="#Rowset-package" class="headerlink" title="Rowset package"></a>Rowset package</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SegmentIterator(do_get_next,_init,_get_row_ranges_by_keys,_init_column_iterators) -&gt; Segment(new_column_iterator) -&gt;ColumnReader-&gt;ColumnIterator</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ColumnIterator &gt;&gt; (DefaultValueColumnIterator|ArrayColumnIterator|DictCodeColumnIterator|ScalarColumnIterator)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Segment::new_iterator &gt;&gt; (SegmentChunkIteratorAdapter | SegmentIterator)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ColumnReader &gt;&gt; (ZoneMapIndexReader|OrdinalIndexReader|BitmapIndexReader|BloomFilterIndexReader) &gt;&gt; IndexedColumnReader &gt;&gt; IndexedColumnIterator &gt;&gt; IndexPageIterator</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PageBuilder &gt;&gt; (BinaryDictPageBuilder|BinaryPlainPageBuilder|BinaryPrefixPageBuilder|BitshufflePageBuilder|FrameOfReferencePageBuilder|PlainPageBuilder|RlePageBuilder) &gt;&gt; EncodingInfo &gt;&gt; ColumnReader</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StoragePageDecoder &gt;&gt; PageIO::read_and_decompress_page &gt;&gt; ColumnReader::read_page</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SegmentIterator &gt;&gt; ColumnDecoder.(check_global_dict|encode_to_global_id) </span><br></pre></td></tr></table></figure><h3 id="Vectorized-package"><a href="#Vectorized-package" class="headerlink" title="Vectorized package"></a>Vectorized package</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ChunkIterator &gt;&gt; (SegmentIterator|SegmentChunkIteratorAdapter|TimedChunkIterator|AggregateIterator|EmptyIterator|MergeIterator|ProjectionIterator|TabletReader|UnionIterator) &gt;&gt; TabletReader</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StorageEngine(_perform_base_compaction) &gt;&gt; (CumulativeCompaction|BaseCompaction)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ColumnPredicate &gt;&gt; (NullPredicate|ColumnEqPredicate|BinaryColumnEqPredicate|ColumnExprPredicate|ColumnTruePredicate|ColumnGePredicate|BinaryColumnGePredicate|ColumnGtPredicate|BinaryColumnGtPredicate....) &gt;&gt; </span><br><span class="line">ColumnPredicateRewriter &gt;&gt; SegmentIterator::(_rewrite_predicates()&gt;&gt; init())</span><br><span class="line">PredicateParser &gt;&gt; TabletScanner</span><br></pre></td></tr></table></figure><h3 id="Task-package"><a href="#Task-package" class="headerlink" title="Task package"></a>Task package</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EngineTask &gt;&gt; (EngineStorageMigrationTask|EnginePublishVersionTask|EngineCloneTask|EngineChecksumTask|EngineBatchLoadTask|EngineAlterTabletTask)</span><br></pre></td></tr></table></figure><h2 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_sink -&gt; tablet_sink -&gt;  LoadChannel-&gt;TabletsChannel-&gt;DeltaWriter-&gt;rowset_writer</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse分享: 读已提交</title>
      <link href="/2021/11/08/Clickhouse%E5%88%86%E4%BA%AB-%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4/"/>
      <url>/2021/11/08/Clickhouse%E5%88%86%E4%BA%AB-%E8%AF%BB%E5%B7%B2%E6%8F%90%E4%BA%A4/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前情回顾"><a href="#前情回顾" class="headerlink" title="前情回顾"></a>前情回顾</h2><p>总结之前的<code>原子写入文章</code>, 其实<code>原子写入解决</code>的是<code>数据脏写</code>的问题, 但是存在<code>数据脏读</code>的问题, 即当写入失败回退时, 部分成功的节点的数据, 已经能够被访问了.</p><p>那么从数据库经典的事务级别来说, 实现原子写入就实现了ACID的<code>原子性</code>, 但由于<code>脏读</code>状态的存在, 目前的事务级是<code>读未提交</code>.</p><p><code>读未提交</code>实际上在OLAP应用中, 对大多数业务的影响并没有那么大, 相对于总数为亿行级别的数据, 单次写入个数在10万以内, 因此数据的误差只有万分之一到千分之一, 在大多数业务系统中并不会有那么大的影响.</p><p>但对于某些数据量比较小的用户, <code>读未提交</code>还是会对他们产生比较大的疑问, 因此实现<code>读已提交</code>的隔离级别, 虽非必要, 但是最好如此.</p><blockquote><p>OLAP系统究竟需要怎么样的隔离级别, 这是一个非常好的问题. 隔离级别需要应用层来确定, 但从目前观察到的业务状态, <code>重复读</code>其实在OLAP中没有必要, 因为OLAP系统中, 用户的SQL很多有依赖关系, 不会出现SQL2的某些数值, 需要从SQL1的结果中获取的场景, 因此<code>可重复读</code>基本上没有实现的意义.</p></blockquote><h2 id="实现读已提交"><a href="#实现读已提交" class="headerlink" title="实现读已提交"></a>实现读已提交</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20211108150144214.png" alt="image-20211108150144214"></p><p>在这个架构中需要引入全局的Zookeeper作为协调者. 在ZK上会维护一个<code>processing_insert</code>的文件夹, 其中每个文件表示正在写入的批次信息, 大多数情况下该目录下只有一个文件, 因为一般只有一个写入的Flink的任务在工作.</p><p>和<code>原子写入</code>一样, 一次Flink的Snapshot批次开始的时候, 会产生一个Label, 并写入到ZK中,  之后所有的写入都复用该Label, 每次攒批写入也都有自己的Label, 每个写入的DP都有一个<code>insert_id</code>和一个<code>batch_id</code></p><p>和<code>原子写入</code>不一样的是, 如果在snapshot过程读取数据的话, 读引擎会去ZK中拿到<code>processing_insert</code>路径, 并获取到其中的<code>batch_id</code>, 读取数据的时候, 会自动过滤该id, 也就是这个snapshot批次写入的数据, 读不可见.</p><blockquote><p>这里写入local表的时候,  不需要设计2个状态的DP,  查询的可见性由计算层来维护了</p></blockquote><p>出现回滚的时候,  会将历史所有的数据删除, 这里不需要删除ZK上的记录, 因为下一次Flink重启的时候<code>batch_id</code>依然是上次的, 数据写入将会重试, 然后就没必要删除数据了.</p><blockquote><p>删除数据必须要设置为幂等的,  如果删除数据一致无法成功, 此时只能保障让人工来处理. </p></blockquote><p>对于重试的问题, 和<code>原子写入</code>一样, <code>batch_id</code>相同并且<code>insert_id</code>不同, 因此在<code>prepare</code>的时候需要执行<code>commit</code>local table的操作.</p><blockquote><p>为什么使用Label而不是使用时间戳? 一, 分布式时间戳生成比较麻烦; 二, Flink写入一半只有一个, 只需要判断两种状态, 不需要判断前后时序关系</p></blockquote><h2 id="Flink集成"><a href="#Flink集成" class="headerlink" title="Flink集成"></a>Flink集成</h2><p>跟<code>原子写入</code>一样通过两阶段写入的方式处理, 但过程有点不一致:</p><ul><li>prepare阶段: 执行local的表的commit操作, 写入DP, 并删除无用的DP, 此时local查询时, 该DP已经可见</li><li>commit阶段: 只删除ZK上的路径, 查询时会自动查询底层的数据.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse分享:原子写入</title>
      <link href="/2021/08/24/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E5%8E%9F%E5%AD%90%E5%86%99%E5%85%A5/"/>
      <url>/2021/08/24/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E5%8E%9F%E5%AD%90%E5%86%99%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>众所周知, Clickhouse是没有事务能力的, 对写入数据也不承诺原子性保护,  但在业务使用过程中, 用户往往需要数据量尽量一致, 现在在使用CK时, 用户经常在校对离线和实时数据时, 发现数据行数都不一致.</p><p>因此, 我们需要给力CK加上一个原子性写入的保护, 但基于OLAP系统的吞吐量, 给CK加强一致事务保护是不现实的, 所以我们的目标并不是100%的原子性, 而是一个折中.</p><p>有个要注意的点, 这篇文章讨论的是写入的<code>原子性</code>, 它只能保证写入要么全部成功, 那么全部失败; 它能保证<code>最终一致性</code>, 但无法保证强一致性; 也<code>无法承诺任何隔离性</code>; 但可以保证持久性.</p><blockquote><p>实际上用ACID来评价这个原子性, 并不是很恰当, 例如<code>最终一致性</code>是分布式复制协议里面的概念; 这里只是用数据库的ACID做为评价的指标.</p></blockquote><h2 id="实时数据写入流程"><a href="#实时数据写入流程" class="headerlink" title="实时数据写入流程"></a>实时数据写入流程</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210824150229371.png" alt="image-20210824150229371"></p><p>目前实时数据导入的流程大致入上图所示:</p><ol><li><p>数据存储在Kafka中,  数据从Kafka读取, 并流向Flink程序. 由于Flink有比较完善的Checkpoint机制, 只要Source能够<strong>回放</strong>, 就能保证数据是一致的. 因此这个步骤不会丢失数据</p></li><li><p>数据在Flink中做完转化后, 会写入到CHProxy中. CHProxy是一个Clickhouse的代理, 负责鉴权或者流程控制等功能. 这个步骤比较容易丢数据,  因为Flink要求Sink算子能够<strong>幂等写入</strong>, 但CHProxy和Clickhouse现有功能都无法保证.</p></li><li><p>CHProxy接受到写入请求后, 会随机选择一个Clickhouse后端执行写入SQL. 由于随机选择, Flink重试后有可能导致数据写入到不同shard节点, 导致<strong>重复</strong></p><blockquote><ol><li>Flink选CHProxy也是随机的</li><li>写入时, 直接写入本地表, 数据没有做hash</li></ol></blockquote></li><li><p>Clickhouse节点接收到SQL请求后, 会将数据按照分区切分数据, 写入临时文件, 再分批提交.  提交过程失败, 会导致任务重试后<strong>数据重复</strong>(部分提交成功的情况)</p></li><li><p>多replica集群,  数据写入到主节点后, 会<strong>异步的同步</strong>到从节点, 此时主节点下线会导致<strong>数据丢失</strong> (节点重启后, 还能恢复)</p></li></ol><p>从上面的分析可知, 2,3,4,5都存在着数据重复或者丢失的风险,  其中2和3是机制问题, 一旦触发Flink重试(重启应用必然触发)/CHProxy随机路由(重试写入时就触发), 数据就会出现大概率重复. 而4和5是小概率实践, 4是一个磁盘写入操作, 即使顺序执行多个DP操作时, 出现异常的概率都是相对较小的;5的情况, 只要节点能够重启, 就能够保证最终一致性.</p><p>综上所示, 原子写入的目标是修改2和3的逻辑, 保证数据丢失重复的概率不高于4和5.</p><h2 id="两阶段提交"><a href="#两阶段提交" class="headerlink" title="两阶段提交"></a>两阶段提交</h2><p> 二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时<strong>保持一致性</strong>而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。</p><p> 在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。</p><p> <strong>当一个事务跨越多个节点时</strong>，<strong>为了保持事务的ACID特性</strong>，需要<strong>引入一个作为协调者的组件</strong>来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。</p><p> 因此，<strong>二阶段提交的算法思路可以概括为：</strong> <strong>参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。</strong></p><p>所谓的两个阶段是指：</p><ul><li><p><strong>第一阶段：投票</strong></p><p>该阶段的主要目的在于打探数据库集群中的各个参与者是否能够正常的执行事务，具体步骤如下：</p><ol><li>协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行结果；</li><li>事务参与者收到请求之后，执行事务但不提交，并记录事务日志；</li><li>参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。</li></ol></li><li><p><strong>第二阶段：事务提交</strong></p><p>在经过第一阶段协调者的询盘之后，各个参与者会回复自己事务的执行情况，这时候存在 3 种可能性：</p><ol><li>所有的参与者都回复能够正常执行事务。</li><li>一个或多个参与者回复事务执行失败。</li><li>协调者等待超时。</li></ol></li></ul><p>对于第 1 种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如下：</p><ol><li>协调者向各个参与者发送 commit 通知，请求提交事务；</li><li>参与者收到事务提交通知之后执行 commit 操作，然后释放占有的资源；</li><li>参与者向协调者返回事务 commit 结果信息。</li></ol><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/bV0KNR.png" alt="2pc-success"></p><p>对于第 2 和第 3 种情况，协调者均认为参与者无法成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，具体步骤如下：</p><ol><li>协调者向各个参与者发送事务 rollback 通知，请求回滚事务；</li><li>参与者收到事务回滚通知之后执行 rollback 操作，然后释放占有的资源；</li><li>参与者向协调者返回事务 rollback 结果信息。</li></ol><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/bV0KOc.png" alt="2pc-failed"></p><p>两阶段提交协议原理简单、易于实现，但是缺点也是显而易见的，包含如下：</p><ul><li><strong>单点问题</strong>: 协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，就会影响整个数据库集群的正常运行。比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或回滚通知，那么参与者们将一直处于阻塞状态，整个数据库集群将无法提供服务。 <strong>极端情况下, 协调者发出Commmit消息之后宕机</strong>,  整体系统将存储不确定的状态.</li><li><strong>同步阻塞</strong>: 两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率极其低下。</li><li><p><strong>数据不一致性</strong>: 两阶段提交协议虽然是分布式数据强一致性所设计，但仍然存在数据不一致性的可能性。比如在第二阶段中，假设协调者发出了事务 commit 通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。</p><p>针对上述问题可以引入 <strong>超时机制</strong> 和 <strong>互询机制</strong> 在很大程度上予以解决。</p></li></ul><p>对于协调者来说如果在指定时间内没有收到所有参与者的应答，则可以自动退出 WAIT 状态，并向所有参与者发送 rollback 通知。对于参与者来说如果位于 READY 状态，但是在指定时间内没有收到协调者的第二阶段通知，则不能武断地执行 rollback 操作，因为协调者可能发送的是 commit 通知，这个时候执行 rollback 就会导致数据不一致。</p><p>此时，我们可以介入互询机制，让参与者 A 去询问其他参与者 B 的执行情况。如果 B 执行了 rollback 或 commit 操作，则 A 可以大胆的与 B 执行相同的操作；如果 B 此时还没有到达 READY 状态，则可以推断出协调者发出的肯定是 rollback 通知；如果 B 同样位于 READY 状态，则 A 可以继续询问另外的参与者。只有当所有的参与者都位于 READY 状态时，此时两阶段提交协议无法处理，将陷入长时间的阻塞状态。</p><blockquote><p>上文的介绍摘抄自<a href="https://segmentfault.com/a/1190000012534071">参考文档</a></p></blockquote><h2 id="Flink中的两阶段提交"><a href="#Flink中的两阶段提交" class="headerlink" title="Flink中的两阶段提交"></a>Flink中的两阶段提交</h2><p>Flink提供了基于2PC的SinkFunction，名为TwoPhaseCommitSinkFunction，帮助我们做了一些基础的工作。它的第一层类继承关系如下：</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/v2-16e2dc211fae60c8df956546ae465ce6_720w.jpg" alt="img"></p><p>但是TwoPhaseCommitSinkFunction仍然留了以下四个抽象方法待子类来实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> TXN <span class="title function_">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"> <span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">preCommit</span><span class="params">(TXN transaction)</span> <span class="keyword">throws</span> Exception;</span><br><span class="line"> <span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">commit</span><span class="params">(TXN transaction)</span>;</span><br><span class="line"> <span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title function_">abort</span><span class="params">(TXN transaction)</span>;</span><br></pre></td></tr></table></figure><ul><li><p>beginTransaction()：开始一个事务，返回事务信息的句柄。</p></li><li><p>preCommit()：预提交（即提交请求）阶段的逻辑, 在snapshotState()方法被调用</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/v2-3d7059f46f5b1557742cf866e10a7e20_720w.jpg" alt="img"></p><p>每当需要做checkpoint时，JobManager就在数据流中打入一个屏障（barrier），作为检查点的界限。屏障随着算子链向下游传递，每到达一个算子都会触发将状态快照写入状态后端(state BackEnd)的动作。当屏障到达Kafka sink后，触发preCommit(实际上是KafkaProducer.flush())方法刷写消息数据，但还未真正提交。接下来还是需要通过检查点来触发提交阶段。</p></li><li><p>commit()：正式提交阶段的逻辑, 该方法的调用点位于TwoPhaseCommitSinkFunction.notifyCheckpointComplete()方法中</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/v2-e5933278ad5c34eb94e7a712fd61ae90_720w.jpg" alt="img"></p><p>可见，只有在所有检查点都成功完成这个前提下，写入才会成功。这符合前文所述2PC的流程，其中JobManager为协调者，各个算子为参与者（不过只有sink一个参与者会执行提交）。一旦有检查点失败，notifyCheckpointComplete()方法就不会执行。如果重试也不成功的话，最终会调用abort()方法回滚事务。</p></li><li><p>abort()：取消事务, 在这里执行Rollback操作</p></li></ul><p>从Flink的实现, 再回看两阶段提交的问题:</p><ol><li><p>单点问题, 依然存在, Flink只会重启再重试, 但当代码未触发Rollback而异常退出时, Flink实际上对此没有能力</p></li><li><p>同步阻塞, Clickhouse并没有写入锁, 因此本身就没有这类问题. <strong>事务超时处理可以由应用层完成</strong></p></li><li><p>数据不一致, Flink有Rollback的接口, 再正常情况下能够保证数据最终一致, 但无法保证强一致性. 在单点问题发生时, Flink连最终一致性也无法保证. 而且数据不一致时, 并没有<strong>互询机制</strong>对账.</p></li></ol><h2 id="Clickhouse两阶段写入"><a href="#Clickhouse两阶段写入" class="headerlink" title="Clickhouse两阶段写入"></a>Clickhouse两阶段写入</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210824191907998.png" alt="image-20210824191907998" style="zoom:50%;" /></p><h3 id="Prepare"><a href="#Prepare" class="headerlink" title="Prepare"></a>Prepare</h3><p>跟现在写tmp文件夹一样, 让DP写入的地方写入到一个叫做<code>draft</code>的文件夹. 类似<code>detach</code>目录, 查询时这些数据不可见, 同时也不会触发<code>Merge</code>以及<code>Mutation</code>操作.</p><p>Prepare过程会设置两个Label标志, 第一个标志Flink的CP编号, 第二个标志插入数据的标志. Label可以由客户端设置, 也可以让系统自动生成.</p><p>Insert语句会包含返回值, 返回的内容为: Host节点名 + Lable_CP名称 + Lable_name名称 </p><h3 id="Commit"><a href="#Commit" class="headerlink" title="Commit"></a>Commit</h3><p>Commit过程类似于Attach,  Commit需要提供对应的Lable_name列表, 只会Commit对应Label的DataPart.</p><p>另外由于Commit可能会处于Unknown状态,  那么需要能够实现多次commit, 用于第一次失败后的重试.</p><h3 id="Rollback"><a href="#Rollback" class="headerlink" title="Rollback"></a>Rollback</h3><p>触发回滚时, DataPart可能会处于Draft或者Visible状态,  对于Draft状态的DataPart只需要删除文件即可.</p><p>但是对于Visible状态的DataPart, 需要防止数据被Merge, 导致无法回滚, 此时要求</p><ol><li>只能Merge相同Label_CP的DataPart, 不同Label_CP的DP不会merge, Mutation后依然保留Label_CP标志</li><li>超过Flink Checkpoint间隔后, 可以解除Merge限制, 该时间限制由commit命令的配置项传递 </li></ol><p>这样既能保证Rollup正确执行, 也保证DP个数不会太多, 导致节点负载高.</p><blockquote><p>Draft或者Visible状态只是一种描述, 并非CK内部的DP状态类型.</p></blockquote><h3 id="数据清理"><a href="#数据清理" class="headerlink" title="数据清理"></a>数据清理</h3><p>Flink重启后, 不一定能够下发Rollback命令, 因此需要有一个定时的线程来删除Draft的数据.</p><p>默认清理1天前的数据, Clickhouse的插入语句, 不允许执行一天.</p><h3 id="物化视图"><a href="#物化视图" class="headerlink" title="物化视图"></a>物化视图</h3><p>CK的物化视图的更新发生在插入过程中, 因此在插入的时候, 需要对物化视图产生的DP, 打上相同的Label标签. </p><p>由于物化视图是分布式写入, 因此每个节点都可能有对应的数据块, 因此插入物化视图时, 返回的数据行数会很多.</p><h3 id="DDL变更"><a href="#DDL变更" class="headerlink" title="DDL变更"></a>DDL变更</h3><p>Prepare过程中, 按照老的元数据写入数据, 但是commit阶段, 元数据已经改变.</p><p>此时Commit DP时, 需要执行Mutation, 这可能导致Commit时间过长, 导致超时, 但如果是异步处理的话, 可能无法完成.</p><h3 id="集群扩缩容"><a href="#集群扩缩容" class="headerlink" title="集群扩缩容"></a>集群扩缩容</h3><p>集群扩容的时候, 比较简单, 主要按照原有的流程处理即可.</p><p>但缩容的过程, 由于节点下线, 导致对于该shard的commit,永远都不会成功, 这样就必然会触发Flink的重试, 而整个集群上所有Flink任务的重试, 对业务来说是无法接受的. </p><p>因此需要实现下线时,  流量转发的能力, 将commit的请求, 随着数据的迁移而转发, 服务进程依然会提供服务, 知道flink上依赖的任务全部解除依赖(新的一轮snapshot过程即可解除, 删除对应链接通路即可.)</p><h2 id="对比Doris"><a href="#对比Doris" class="headerlink" title="对比Doris"></a>对比Doris</h2><p>对于原子插入, Doris处理方式基本一致, 但是Doris的方案能够保证一致性和隔离性, 原因在于<strong>Doris有统一的元数据</strong></p><p>Doris在查询时,  FE会指明那些数据是可见的,  因此Doris的可见性(实际上就是隔离性)可以在FE上通过原子的更新改变.</p><p>而CK没有统一的元数据, 因此需要向每个节点发送commit, 单个节点commit后无法了解全局的Commit,  因此一旦出现全局回滚, 就会出现不一致的问题.</p><blockquote><p><a href="https://www.bilibili.com/video/BV1Dq4y1S7Ws">Doris分享视频</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse运维增强:统一元数据</title>
      <link href="/2021/08/20/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-%E7%BB%9F%E4%B8%80%E5%85%83%E6%95%B0%E6%8D%AE/"/>
      <url>/2021/08/20/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-%E7%BB%9F%E4%B8%80%E5%85%83%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Clickhouse元数据"><a href="#Clickhouse元数据" class="headerlink" title="Clickhouse元数据"></a>Clickhouse元数据</h2><h3 id="库表相关"><a href="#库表相关" class="headerlink" title="库表相关"></a>库表相关</h3><h3 id="列相关"><a href="#列相关" class="headerlink" title="列相关"></a>列相关</h3><h3 id="复制相关"><a href="#复制相关" class="headerlink" title="复制相关"></a>复制相关</h3><h2 id="Paxos-vs-MySQL"><a href="#Paxos-vs-MySQL" class="headerlink" title="Paxos vs MySQL"></a>Paxos vs MySQL</h2><h2 id="统一元数据设计"><a href="#统一元数据设计" class="headerlink" title="统一元数据设计"></a>统一元数据设计</h2><h3 id="库表管理"><a href="#库表管理" class="headerlink" title="库表管理"></a>库表管理</h3><h3 id="数据变更"><a href="#数据变更" class="headerlink" title="数据变更"></a>数据变更</h3>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse技术分享: SQL查询流程</title>
      <link href="/2021/08/16/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-SQL%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B/"/>
      <url>/2021/08/16/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-SQL%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="SQL流程概览"><a href="#SQL流程概览" class="headerlink" title="SQL流程概览"></a>SQL流程概览</h2><p>数据库发展至今, SQL处理流程已经非常完善,  以Presto的分析流程为例</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/v2-88847346d6a1d998a2c933f7df9ca0f2_720w.jpg" alt="img"></p><ol><li>SQL文本由客户端提交到服务端时, 首先会进入Parser模块, 进行<code>词法和语法解析</code></li><li>经过解析后, 会生成AST(abstract syntax code)树, 然后会进入<code>语义分析</code>阶段, 结合元数据信息, 将AST中无意义的文本, 转化为有含义的对象.</li><li>然后根据AST树, 转化为<code>逻辑计划</code></li><li>逻辑计划经过<code>优化器</code>(optimizer)后, 生成最终执行的执行计划(在Presto是分布式执行计划)</li><li>最后放入<code>执行器</code>执行具体任务</li></ol><p>Clickhouse的流程也基本相似:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210816161057679.png" alt="image-20210816161057679"></p><ol><li>客户单通过TCP端口提交任务到<code>Clickhouse Server</code></li><li><code>TCP Handler</code>会响应请求, 并调用<code>executeQuery</code>函数处理请求</li><li><code>executeQuery</code>会处理上面SQL处理的5个步骤, 最后生成执行计划<ol><li>Parser为词法解析</li><li>Interpreter为语义解析</li><li>QueryPlan为逻辑计划</li><li>QueryPipeline为经过优化的执行计划</li></ol></li><li>将<code>QueryPipeline</code>放入到<code>PipelineExecutor</code>中执行任务</li></ol><blockquote><p>注: 由于Clickhouse并非一个MPP的数据库, 因此并没有分布式执行计划一说, 分布式方式被拆散到QueryPlan之中.</p></blockquote><h2 id="TCPHandler"><a href="#TCPHandler" class="headerlink" title="TCPHandler"></a>TCPHandler</h2><p>通过TCP端口(CK默认的客户端)连接的请求会在<code>TCPHandler::runImpl()</code>函数中被处理, 在做完一些准备工作后, 他会执行以下代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// Processing Query</span></span><br><span class="line">state.io = <span class="built_in">executeQuery</span>(state.query, query_context, <span class="literal">false</span>, state.stage, may_have_embedded_data);</span><br><span class="line"><span class="keyword">if</span> (state.need_receive_data_for_input) <span class="comment">// It implies pipeline execution</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">/// It is special case for input(), all works for reading data from client will be done in callbacks.</span></span><br><span class="line">    <span class="keyword">auto</span> executor = state.io.pipeline.<span class="built_in">execute</span>();</span><br><span class="line">    executor-&gt;<span class="built_in">execute</span>(state.io.pipeline.<span class="built_in">getNumThreads</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>executeQuery</code>的输入<code>state.query</code>是String类型, 实际上就是客户端的SQL语句,  输出<code>state.io</code>输出对象为<code>BlockIO</code>, 这是一个<code>QueryPipeline</code>的封装</p><p>最终<code>executeQuery</code>将SQL字符串转化为执行计划.</p><p>然后调用<code>pipeline.execute()</code>获得计划执行器, 然后调用<code>execute</code>开始任务的执行.</p><blockquote><p><code>TCPHandler</code>类的实现挺不错的,  整体的细节都封装在<code>executeQuery</code>和<code>PipelineExecutor</code>之中</p></blockquote><h2 id="executeQuery"><a href="#executeQuery" class="headerlink" title="executeQuery"></a>executeQuery</h2><p><code>executeQuery</code>的实现函数为<code>executeQueryImpl</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ASTPtr ast;</span><br><span class="line"><span class="function">ParserQuery <span class="title">parser</span><span class="params">(end)</span></span>;</span><br><span class="line">ast = <span class="built_in">parseQuery</span>(parser, begin, end, <span class="string">&quot;&quot;</span>, max_query_size, settings.max_parser_depth);</span><br><span class="line"><span class="keyword">auto</span> interpreter = InterpreterFactory::<span class="built_in">get</span>(ast, context, <span class="built_in">SelectQueryOptions</span>(stage).<span class="built_in">setInternal</span>(internal));</span><br><span class="line">BlockIO res = interpreter-&gt;<span class="built_in">execute</span>();</span><br><span class="line">QueryPipeline &amp; pipeline = res.pipeline;</span><br><span class="line"><span class="comment">// 设置一些回调函数, 还有一些querylog</span></span><br></pre></td></tr></table></figure><p>首先, 使用<code>ParserQuery</code>将SQL语句转化为AST树(代码在<code>parseQuery.cpp</code>中)</p><p>其次, 使用工厂方法<code>InterpreterFactory::get</code>根据AST树, 创建出对应的<code>Interpreter</code>,  我们关注查询, 对应的是<code>InterpreterSelectQuery</code></p><p>然后, 调用<code>Interpreter</code>的<code>execute</code>方法生成了最终的执行计划</p><p>最后, 设置一些回调函数和queryLog等执行后的处理内容.</p><blockquote><p>这里将语法解析步骤和后面的语义分析步骤分开, 导致代码可读性降低了, 一下子很难找到例如优化器部分的代码了</p></blockquote><h3 id="parseQuery"><a href="#parseQuery" class="headerlink" title="parseQuery"></a>parseQuery</h3><p>Clickhouse的Parser模块是自己通过代码实现的, 而非通过现有的库实现, 这样语法解析的性能会好很多.</p><p>拆解整个<code>parseQuery</code>的调用链: <code>parseQuery -&gt; parseQueryAndMovePosition -&gt; tryParseQuery -&gt; parser.parse</code></p><p>最后还是调用了<code>IParser::parse</code>方法, <code>IParser</code>有一个子类<code>IParserBase</code>实现了<code>parse</code>函数, 但又派生了自己的一个<code>parseImpl</code>方法, 所有的Parser都是<code>IParserBase</code>的子类, 并且实现了<code>parseImpl</code>方法. </p><p>在<code>executeQuery</code>定义的<code>ParserQuery</code>可以看做所有Parser的代理类</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ParserQuery::parseImpl</span><span class="params">(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">ParserQueryWithOutput <span class="title">query_with_output_p</span><span class="params">(end)</span></span>;</span><br><span class="line">    <span class="function">ParserInsertQuery <span class="title">insert_p</span><span class="params">(end)</span></span>;</span><br><span class="line">    ParserUseQuery use_p;</span><br><span class="line">    ParserSetQuery set_p;</span><br><span class="line">    <span class="comment">// 省略其他的一些ParserQuery</span></span><br><span class="line"></span><br><span class="line">    <span class="type">bool</span> res = query_with_output_p.<span class="built_in">parse</span>(pos, node, expected)</span><br><span class="line">        || insert_p.<span class="built_in">parse</span>(pos, node, expected)</span><br><span class="line">        || use_p.<span class="built_in">parse</span>(pos, node, expected)</span><br><span class="line">        || set_role_p.<span class="built_in">parse</span>(pos, node, expected);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到这个类, 将所有的Parser放到自己内, 我们跟着的<code>Select</code>语句的Parser封装在第一个<code>ParserQueryWithOutput</code></p><p>整个路径为<code>ParserQueryWithOutput -&gt; ParserSelectWithUnionQuery -&gt;ParserUnionQueryElement -&gt;ParserSelectQuery</code></p><p>最后追踪到<code>ParserSelectQuery::parseImpl</code>开始解析<code>Select</code>语句</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ParserSelectQuery::parseImpl</span><span class="params">(Pos &amp; pos, ASTPtr &amp; node, Expected &amp; expected)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> select_query = std::<span class="built_in">make_shared</span>&lt;ASTSelectQuery&gt;();</span><br><span class="line">    node = select_query;</span><br><span class="line"></span><br><span class="line">    <span class="function">ParserKeyword <span class="title">s_select</span><span class="params">(<span class="string">&quot;SELECT&quot;</span>)</span></span>;</span><br><span class="line">    <span class="function">ParserKeyword <span class="title">s_all</span><span class="params">(<span class="string">&quot;ALL&quot;</span>)</span></span>;</span><br><span class="line">    <span class="comment">// 省略很多代码</span></span><br><span class="line">    ASTPtr with_expression_list;</span><br><span class="line">    ASTPtr select_expression_list;</span><br><span class="line">    <span class="comment">// 省略很多代码</span></span><br><span class="line">  </span><br><span class="line">    <span class="comment">// 开始解析字符</span></span><br><span class="line">    <span class="comment">/// FROM database.table or FROM table or FROM (subquery) or FROM tableFunction(...)</span></span><br><span class="line">    <span class="keyword">if</span> (s_from.<span class="built_in">ignore</span>(pos, expected))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">ParserTablesInSelectQuery</span>().<span class="built_in">parse</span>(pos, tables, expected))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">      <span class="comment">/// PREWHERE expr</span></span><br><span class="line">    <span class="keyword">if</span> (s_prewhere.<span class="built_in">ignore</span>(pos, expected))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!exp_elem.<span class="built_in">parse</span>(pos, prewhere_expression, expected))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以注意到解析开始时, 先定义了一个<code>ASTSelectQuery</code>,  <code>IAST</code>是抽象语法树的实现, 每个语句几乎都对应着一个语法树.</p><p>Parser目的就是通过SQL文本解析, 将AST树构建出来, 赋值里面的具体数值, 例如<code>ASTSelectQuery</code>就需要将这些参数都给赋值了</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ASTSelectQuery</span> : <span class="keyword">public</span> IAST</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">bool</span> distinct = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">bool</span> group_by_with_totals = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">bool</span> group_by_with_rollup = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">bool</span> group_by_with_cube = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">bool</span> group_by_with_constant_keys = <span class="literal">false</span>;</span><br><span class="line">    <span class="type">bool</span> limit_with_ties = <span class="literal">false</span>;</span><br><span class="line">    ASTs children;</span><br><span class="line">    std::unordered_map&lt;Expression, <span class="type">size_t</span>&gt; positions;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外, 在<code>ASTSelectQuery</code>也有很多其他分支AST树, 例如在<code>parseImpl</code>提到的<code>with_expression_list</code>和<code>select_expression_list</code></p><p>至此整个抽象语法树已经构建完毕了</p><h3 id="InterpreterSelectQuery"><a href="#InterpreterSelectQuery" class="headerlink" title="InterpreterSelectQuery"></a>InterpreterSelectQuery</h3><p>这个类中最重要的是<code>构造函数</code>和<code>execute</code>方法, 构造函数做的是Analyzer的工作, 就是<code>语义分析</code>的过程, 而<code>execute</code>函数做的构建逻辑计划的步骤.</p><h4 id="InterpreterSelectQuery-InterpreterSelectQuery"><a href="#InterpreterSelectQuery-InterpreterSelectQuery" class="headerlink" title="InterpreterSelectQuery::InterpreterSelectQuery()"></a>InterpreterSelectQuery::InterpreterSelectQuery()</h4><p>构造函数中有一个非常长的Lambda函数: <code>analyze</code>, 截取其中重要的方法: </p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">syntax_analyzer_result = <span class="built_in">TreeRewriter</span>(context).<span class="built_in">analyzeSelect</span>();</span><br><span class="line">query_analyzer = std::<span class="built_in">make_unique</span>&lt;SelectQueryExpressionAnalyzer&gt;();</span><br><span class="line">source_header = metadata_snapshot-&gt;<span class="built_in">getSampleBlockForColumns</span>(required_columns, storage-&gt;<span class="built_in">getVirtuals</span>(), storage-&gt;<span class="built_in">getStorageID</span>());</span><br><span class="line">result_header = <span class="built_in">getSampleBlockImpl</span>();</span><br><span class="line"><span class="comment">// getSampleBlockImpl函数中</span></span><br><span class="line">query_analyzer = std::<span class="built_in">make_unique</span>&lt;SelectQueryExpressionAnalyzer&gt;()</span><br></pre></td></tr></table></figure><p>首先, 通过<code>TreeRewriter</code>将做一些语法树的转化动作</p><p>其次, 生成<code>query_analyzer</code></p><p>最后, 明确一下整个SQL的<code>source_header</code>和<code>result_header</code>, 也就是需要读取表的字段和输出的字段</p><p>这里有三个对象非常关键, 看一下这三个的描述</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TreeRewriterResultPtr syntax_analyzer_result;</span><br><span class="line">std::unique_ptr&lt;SelectQueryExpressionAnalyzer&gt; query_analyzer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/// Is calculated in getSampleBlock. Is used later in readImpl.</span></span><br><span class="line">ExpressionAnalysisResult analysis_result;</span><br></pre></td></tr></table></figure><p><code>TreeRewriter</code>是经过语义分析的语法树, 在这里除了完成元数据转义之外,  还有不少的优化器, 这个未来再说.</p><p><code>SelectQueryExpressionAnalyzer</code>和<code>ExpressionAnalysisResult</code>是一对, 前者负责把AST树上的表达式拆解掉, 表示为<code>ActionDag</code>, 后者为分析的结果.</p><p>至于为啥一个执行动作和执行结果都要暴露出来给<code>InterpreterSelectQuery</code>使用呢,  我感觉是代码写的不够好, 导致这两个对象在使用的时候, 调用比较混乱.</p><blockquote><p>这部分代码逻辑太复杂了, 后续得专门分析一下</p></blockquote><p>在<code>语义分析</code>阶段实际上就是需要根据元数据信息, 对文本做语义替换, 并完成校验工作, 查询列存不存在的校验工作.</p><h4 id="InterpreterSelectQuery-execute"><a href="#InterpreterSelectQuery-execute" class="headerlink" title="InterpreterSelectQuery::execute()"></a>InterpreterSelectQuery::execute()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">BlockIO <span class="title">InterpreterSelectQuery::execute</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    BlockIO res;</span><br><span class="line">    QueryPlan query_plan;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">buildQueryPlan</span>(query_plan);</span><br><span class="line"></span><br><span class="line">    res.pipeline = std::<span class="built_in">move</span>(*query_plan.<span class="built_in">buildQueryPipeline</span>(</span><br><span class="line">        QueryPlanOptimizationSettings::<span class="built_in">fromContext</span>(context), BuildQueryPipelineSettings::<span class="built_in">fromContext</span>(context)));</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>首先, 构造<code>QueryPlan</code>, 具体的步骤就是根据AST树, 然后往<code>QueryPlan</code>一直<code>addStep</code></p><p><code>IQueryPlanStep</code>算逻辑计划的封装, <code>QueryPlan</code>维系的就是<code>Step</code>的列表</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QueryPlan</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">Node</span></span><br><span class="line">    &#123;</span><br><span class="line">        QueryPlanStepPtr step;</span><br><span class="line">        std::vector&lt;Node *&gt; children = &#123;&#125;;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">using</span> Nodes = std::list&lt;Node&gt;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Nodes nodes;</span><br><span class="line">    Node * root = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>IQueryPlanStep</code>有5个基础子类:</p><ul><li>CreatingSetsStep:  Creates sets for subqueries and JOIN.</li><li>ISourceStep: Returns single logical DataStream</li><li>ITransformingStep:  数据做转化的算子, 大多数的算子都在这里</li><li>JoinStep: 处理Jion算子</li><li>UnionStep: 处理Union算子</li></ul><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210818111008868.png" alt="image-20210818111008868" style="zoom:50%;" /></p><p>然后, 有了<code>QueryPlan</code>之后构造执行计划<code>QueryPipeline</code>, 具体代码在<code>QueryPlan::buildQueryPipeline</code>中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">QueryPipelinePtr <span class="title">QueryPlan::buildQueryPipeline</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">checkInitialized</span>();</span><br><span class="line">    <span class="built_in">optimize</span>(optimization_settings);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">Frame</span></span><br><span class="line">    &#123;</span><br><span class="line">        Node * node = &#123;&#125;;</span><br><span class="line">        QueryPipelines pipelines = &#123;&#125;;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    QueryPipelinePtr last_pipeline;</span><br><span class="line"></span><br><span class="line">    std::stack&lt;Frame&gt; stack;</span><br><span class="line">    stack.<span class="built_in">push</span>(Frame&#123;.node = root&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (!stack.<span class="built_in">empty</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">auto</span> &amp; frame = stack.<span class="built_in">top</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (last_pipeline)</span><br><span class="line">        &#123;</span><br><span class="line">            frame.pipelines.<span class="built_in">emplace_back</span>(std::<span class="built_in">move</span>(last_pipeline));</span><br><span class="line">            last_pipeline = <span class="literal">nullptr</span>; <span class="comment">//-V1048</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">size_t</span> next_child = frame.pipelines.<span class="built_in">size</span>();</span><br><span class="line">        <span class="keyword">if</span> (next_child == frame.node-&gt;children.<span class="built_in">size</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">bool</span> limit_max_threads = frame.pipelines.<span class="built_in">empty</span>();</span><br><span class="line">            <span class="comment">// 逻辑计划, 转化为执行计划</span></span><br><span class="line">            last_pipeline = frame.node-&gt;step-&gt;<span class="built_in">updatePipeline</span>(std::<span class="built_in">move</span>(frame.pipelines), build_pipeline_settings);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (limit_max_threads &amp;&amp; max_threads)</span><br><span class="line">                last_pipeline-&gt;<span class="built_in">limitMaxThreads</span>(max_threads);</span><br><span class="line"></span><br><span class="line">            stack.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            stack.<span class="built_in">push</span>(Frame&#123;.node = frame.node-&gt;children[next_child]&#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp; context : interpreter_context)</span><br><span class="line">        last_pipeline-&gt;<span class="built_in">addInterpreterContext</span>(std::<span class="built_in">move</span>(context));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> last_pipeline;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>整段代码有两个核心</p><p>第一, 调用了<code>optimize(optimization_settings)</code>方法, 对逻辑计划进行了优化</p><p>第二, 遍历整个逻辑树, 分别调用<code>step-&gt;updatePipeline</code>方法, 将逻辑计划转化为执行计划</p><p>优化器部分后续再展开, 这里优先讲一下执行计划.</p><p>就像逻辑计划, <code>QueryPlan</code>表示逻辑树, <code>IQueryPlanStep</code>表示其中的节点</p><p>执行计划的整课树为<code>QueryPipeline</code>,  其中的节点便是<code>IProcessor</code></p><p><code>IProcessor</code>有很多的实现, 主要分为3类:</p><ol><li><p>各种<code>Transform</code>结尾的执行算子, 这些算子与逻辑计划中的<code>ITransformingStep</code>对应</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210818112558755.png" alt="image-20210818112558755" style="zoom:50%;" /></p></li><li><p>输入相关的, 包含在<code>ISource</code>中, 跟MergeTree读取相关的Source藏匿于<code>SourceWithProcess</code>中</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210818112726000.png" alt="image-20210818112726000" style="zoom:50%;" /></p></li><li><p>输出相关的, 包含在<code>ISink</code>和<code>IOutputFormat</code>中</p></li></ol><p><code>IProcessor</code>除了定义了处理逻辑, 还需要定位输入输出, 在代码里输入输出, 用<code>Port</code>来表征,  <code>IProcessor</code>可以多个Input也可以有多个output, 因此两者都用列表表示<code>IProcessor</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">IProcessor</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">protected</span>:</span><br><span class="line">  InputPorts inputs;</span><br><span class="line">  OutputPorts outputs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Port</code>定义为如下, 关联<code>IProcessor</code>, 相当于<code>IProcessor</code>关于上下游数据的连接器</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Port</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="keyword">friend</span> <span class="type">void</span> <span class="title">connect</span><span class="params">(OutputPort &amp;, InputPort &amp;)</span></span>;</span><br><span class="line">    IProcessor * processor = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">connect</span><span class="params">(OutputPort &amp; output, InputPort &amp; input)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (input.state || output.state)</span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">Exception</span>(<span class="string">&quot;Port is already connected&quot;</span>, ErrorCodes::LOGICAL_ERROR);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> out_name = output.<span class="built_in">getProcessor</span>().<span class="built_in">getName</span>();</span><br><span class="line">    <span class="keyword">auto</span> in_name = input.<span class="built_in">getProcessor</span>().<span class="built_in">getName</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assertCompatibleHeader</span>(output.<span class="built_in">getHeader</span>(), input.<span class="built_in">getHeader</span>(), <span class="string">&quot; function connect between &quot;</span> + out_name + <span class="string">&quot; and &quot;</span> + in_name);</span><br><span class="line"></span><br><span class="line">    input.output_port = &amp;output;</span><br><span class="line">    output.input_port = &amp;input;</span><br><span class="line">    input.state = std::<span class="built_in">make_shared</span>&lt;Port::State&gt;();</span><br><span class="line">    output.state = input.state;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的<code>connect</code> 将上下游的<code>IProcessor</code>连接在一起</p><p><code>Port</code>有两个实现类:  <code>InputPort</code>和<code>OutputPort</code>表示<code>IProcessor</code>的输入和输出, 并相互关联.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">OutputPort</span> : <span class="keyword">public</span> Port</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    InputPort * input_port = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InputPort</span> : <span class="keyword">public</span> Port</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    OutputPort * output_port = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="PipelineExecutor"><a href="#PipelineExecutor" class="headerlink" title="PipelineExecutor"></a>PipelineExecutor</h2><h3 id="构造PipelineExecutor"><a href="#构造PipelineExecutor" class="headerlink" title="构造PipelineExecutor"></a>构造PipelineExecutor</h3><p><code>PipelineExecutor</code>的构造函数几乎只需要<code>IProcessor</code>列表, 参考<code>QueryPipeline::execute()</code>方法</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">PipelineExecutorPtr <span class="title">QueryPipeline::execute</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">isCompleted</span>())</span><br><span class="line">        <span class="keyword">throw</span> <span class="built_in">Exception</span>(<span class="string">&quot;Cannot execute pipeline because it is not completed.&quot;</span>, ErrorCodes::LOGICAL_ERROR);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> std::<span class="built_in">make_shared</span>&lt;PipelineExecutor&gt;(pipe.processors, process_list_element);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PipelineExecutor的构造函数</span></span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        graph = std::<span class="built_in">make_unique</span>&lt;ExecutingGraph&gt;(processors);</span><br><span class="line">        <span class="keyword">if</span> (process_list_element)</span><br><span class="line">            process_list_element-&gt;<span class="built_in">addPipelineExecutor</span>(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">catch</span> (Exception &amp; exception)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">throw</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>构造拿到列表, 根据执行计划构造<code>ExecutingGraph</code>, 对应还有<code>ExecutingGraph::Node</code>和<code>ExecutingGraph::Edge</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span></span><br><span class="line">&#123;</span><br><span class="line">    IProcessor * processor = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="type">uint64_t</span> processors_id = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    Edges direct_edges;</span><br><span class="line">    Edges back_edges;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">Node</span>(IProcessor * processor_, <span class="type">uint64_t</span> processor_id)</span><br><span class="line">        : <span class="built_in">processor</span>(processor_), <span class="built_in">processors_id</span>(processor_id)</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Edge</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">Edge</span>(<span class="type">uint64_t</span> to_, <span class="type">bool</span> backward_,</span><br><span class="line">         <span class="type">uint64_t</span> input_port_number_, <span class="type">uint64_t</span> output_port_number_,</span><br><span class="line">         std::vector&lt;<span class="type">void</span> *&gt; * update_list)</span><br><span class="line">        : <span class="built_in">to</span>(to_), <span class="built_in">backward</span>(backward_)</span><br><span class="line">        , <span class="built_in">input_port_number</span>(input_port_number_), <span class="built_in">output_port_number</span>(output_port_number_)</span><br><span class="line">    &#123;</span><br><span class="line">        update_info.update_list = update_list;</span><br><span class="line">        update_info.id = <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint64_t</span> to = std::numeric_limits&lt;<span class="type">uint64_t</span>&gt;::<span class="built_in">max</span>();</span><br><span class="line">    <span class="type">bool</span> backward;</span><br><span class="line">    <span class="type">uint64_t</span> input_port_number;</span><br><span class="line">    <span class="type">uint64_t</span> output_port_number;</span><br><span class="line">    Port::UpdateInfo update_info;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>可以看到<code>Node</code>定义着和<code>IProcessor</code>的关系, 而<code>Edge</code>定义着<code>Port</code>的编号</p><h3 id="PipelineExecutor执行"><a href="#PipelineExecutor执行" class="headerlink" title="PipelineExecutor执行"></a>PipelineExecutor执行</h3><p>调用链为<code>execute -&gt; executeImpl -&gt; initializeExecution -&gt; executeSingleThread -&gt; executeStepImpl</code> </p><p>其中<code>initializeExecution</code>主要是为了初始化ThreadLocal的线程池,  主要的触发逻辑都在<code>executeStepImpl</code>中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// executeStepImpl的核心逻辑</span></span><br><span class="line"><span class="keyword">while</span> (node &amp;&amp; !yield)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (finished)</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="built_in">addJob</span>(node);</span><br><span class="line">    &#123;</span><br><span class="line">        node-&gt;<span class="built_in">job</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (node-&gt;exception)</span><br><span class="line">        <span class="built_in">cancel</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// addJob逻辑</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">PipelineExecutor::addJob</span><span class="params">(ExecutingGraph::Node * execution_state)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> job = [execution_state]()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">try</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Stopwatch watch;</span></span><br><span class="line">            <span class="built_in">executeJob</span>(execution_state-&gt;processor);</span><br><span class="line">            <span class="comment">// execution_state-&gt;execution_time_ns += watch.elapsed();</span></span><br><span class="line"></span><br><span class="line">            ++execution_state-&gt;num_executed_jobs;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">catch</span> (...)</span><br><span class="line">        &#123;</span><br><span class="line">            execution_state-&gt;exception = std::<span class="built_in">current_exception</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    execution_state-&gt;job = std::<span class="built_in">move</span>(job);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// executeJob逻辑, 实际上就是调用processor-&gt;work()</span></span><br><span class="line"><span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">executeJob</span><span class="params">(IProcessor * processor)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        processor-&gt;<span class="built_in">work</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">catch</span> (Exception &amp; exception)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">checkCanAddAdditionalInfoToException</span>(exception))</span><br><span class="line">            exception.<span class="built_in">addMessage</span>(<span class="string">&quot;While executing &quot;</span> + processor-&gt;<span class="built_in">getName</span>());</span><br><span class="line">        <span class="keyword">throw</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整体Clickhouse处理SQL的逻辑就如上面所写的,  整体上实现和SparkSQL挺不一样的, 感觉上SparkSQL相对来说更好理解.</p><p>例如你想找一下类型校验的代码,  一般这个代码会写在<code>Analyzer</code>上, 但我粗粗的浏览代码的时候, 就一下子就难以找到. 但是在SparkSQL里面, 你只要搜索<code>TypeCoercion</code>就能找到所有相关的判断规则, 而Clickhouse中到现在依然没有找到.</p><blockquote><p>代码基于21年8月份的master分支</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入浅出Clickhouse: 分布式设计</title>
      <link href="/2021/08/15/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAClickhouse-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BE%E8%AE%A1/"/>
      <url>/2021/08/15/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAClickhouse-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">remote_servers</span> <span class="attr">incl</span>=<span class="string">&quot;clickhouse_remote_servers&quot;</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">test01</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">host</span>&gt;</span>host01<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">port</span>&gt;</span>9100<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">password</span>&gt;</span>xxxx<span class="tag">&lt;/<span class="name">password</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">host</span>&gt;</span>host02<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">port</span>&gt;</span>9100<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">password</span>&gt;</span>xxxx<span class="tag">&lt;/<span class="name">password</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">host</span>&gt;</span>host03<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">port</span>&gt;</span>9100<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">password</span>&gt;</span>xxxx<span class="tag">&lt;/<span class="name">password</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">host</span>&gt;</span>host04<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">port</span>&gt;</span>9100<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">password</span>&gt;</span>xxxx<span class="tag">&lt;/<span class="name">password</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">test01</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">remote_servers</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在Clickhouse的<code>config.xml</code>文件中有以上类似配置项内容, 在这个样例里面, 定义了一个叫做<code>test01</code>的集群, 它由4个节点组成,  其中分为2个shard,  2副本.</p><p>大致是如下一种形式, shard0和shard1的数据时是不一样的, 而同个shard中的数据时完全一样的.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210830142321251.png" alt="image-20210830142321251"></p><p><strong>Shard内的数据复制能力</strong>, Clickhouse发明了一种叫做<code>ReplicatedMergeTree</code>的引擎, 通过Zookeeper做元数据中心, 完成多个节点建的数据同步. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.lineorder_local</span><br><span class="line">(</span><br><span class="line">    `LO_ORDERKEY` UInt32,</span><br><span class="line">    `LO_LINENUMBER` UInt8,</span><br><span class="line">    `LO_CUSTKEY` UInt32,</span><br><span class="line">    `LO_PARTKEY` UInt32,</span><br><span class="line">    `LO_SUPPKEY` UInt32,</span><br><span class="line">    `LO_ORDERDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_ORDERPRIORITY` LowCardinality(String),</span><br><span class="line">    `LO_SHIPPRIORITY` UInt8,</span><br><span class="line">    `LO_QUANTITY` UInt8,</span><br><span class="line">    `LO_EXTENDEDPRICE` UInt32,</span><br><span class="line">    `LO_ORDTOTALPRICE` UInt32,</span><br><span class="line">    `LO_DISCOUNT` UInt8,</span><br><span class="line">    `LO_REVENUE` UInt32,</span><br><span class="line">    `LO_SUPPLYCOST` UInt32,</span><br><span class="line">    `LO_TAX` UInt8,</span><br><span class="line">    `LO_COMMITDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_SHIPMODE` LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> ReplicatedMergeTree(<span class="string">&#x27;/clickhouse/tables/&#123;shard&#125;/default/lineorder_local&#x27;</span>, <span class="string">&#x27;&#123;replica&#125;&#x27;</span>)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYear(LO_ORDERDATE)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (LO_ORDERDATE, LO_ORDERKEY)</span><br><span class="line">SETTINGS index_granularity <span class="operator">=</span> <span class="number">8192</span></span><br></pre></td></tr></table></figure><p><code>ReplicatedMergeTree</code>的建表语句如上所示, 和<code>MergeTree</code>引擎的区别就是<code>ReplicatedMergeTree(&#39;/clickhouse/tables/&#123;shard&#125;/default/lineorder_local&#39;, &#39;&#123;replica&#125;&#39;)</code>这段内容, <code>&#39;/clickhouse/tables/&#123;shard&#125;/default/lineorder_local&#39;</code>表示的是Zookeeper的路径, <code>&#39;&#123;replica&#125;&#39;</code>表示的是replica的序号.</p><p><code>&#123;value&#125;</code>这类数值在配置文件夹的<code>macros.xml</code>文件中定义</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">cluster</span>&gt;</span>test01<span class="tag">&lt;/<span class="name">cluster</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">shard</span>&gt;</span>test01-01<span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replica</span>&gt;</span>01<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Shard间的分区</strong>,  Clickhouse也创建了一个<code>Distributed</code>数据表引擎表示, 负责写入时数据的分发, 读取时数据的合并.</p><p>但是<code>Distributed</code>表不存数据, 因此需要有一个local表存储真实的数据, 上面提到的<code>ReplicatedMergeTree</code>就是数据表.</p><p>建表语句如下,  整体的结构与local表一致,  引擎<code>Distributed</code>中, <code>test01</code>表示是cluster, <code>default</code>表示的数据库, <code>lineorder_local</code>表示的是表名, 而<code>xxHash32(LO_ORDERKEY)</code>表示根据这个表达式来分区, 通过<code>xxHash</code>分区可以保证相同的key, 必然在一个Shard内, 这样才能完成后续的去重或者预聚合的功能,.  分布式引擎, 还支持<code>rand</code>方式的分区,  随机分区会减少插入过程中做hash的消耗, 但会存在数据重复的情况, 需要根据情况设置.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.lineorder</span><br><span class="line">(</span><br><span class="line">    `LO_ORDERKEY` UInt32,</span><br><span class="line">    `LO_LINENUMBER` UInt8,</span><br><span class="line">    `LO_CUSTKEY` UInt32,</span><br><span class="line">    `LO_PARTKEY` UInt32,</span><br><span class="line">    `LO_SUPPKEY` UInt32,</span><br><span class="line">    `LO_ORDERDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_ORDERPRIORITY` LowCardinality(String),</span><br><span class="line">    `LO_SHIPPRIORITY` UInt8,</span><br><span class="line">    `LO_QUANTITY` UInt8,</span><br><span class="line">    `LO_EXTENDEDPRICE` UInt32,</span><br><span class="line">    `LO_ORDTOTALPRICE` UInt32,</span><br><span class="line">    `LO_DISCOUNT` UInt8,</span><br><span class="line">    `LO_REVENUE` UInt32,</span><br><span class="line">    `LO_SUPPLYCOST` UInt32,</span><br><span class="line">    `LO_TAX` UInt8,</span><br><span class="line">    `LO_COMMITDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_SHIPMODE` LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> Distributed(<span class="string">&#x27;test01&#x27;</span>, <span class="string">&#x27;default&#x27;</span>, <span class="string">&#x27;lineorder_local&#x27;</span>, xxHash32(LO_ORDERKEY))</span><br></pre></td></tr></table></figure><h2 id="数据复制"><a href="#数据复制" class="headerlink" title="数据复制"></a>数据复制</h2><p>在一个shard内的数据流动叫做数据复制, 我们经常听到的<code>一致性协议</code>以及<code>paxos</code>或者<code>raft</code>都在这个范畴之内, 是一个比较复制事情.</p><p>Clickhouse实现的时候, 直接引入了Zookeeper作为元数据中心, Zookeeper通过它自己的<code>ZAB</code>实现了<code>一致性协议</code>, 但Zookeeper的QPS不高, 当元数据量太多时, 容易引发瓶颈. 目前社区基于<a href="https://github.com/eBay/NuRaft">NuRaft</a>实现<a href="https://github.com/ClickHouse/ClickHouse/pull/24059">ClickKeeper</a>, 希望替代Zookeeper. 这都是后话, 先不去管理, 基于Zookeeper的分布式复制, 需要完成的两件事情是, <strong>数据插入</strong>和<strong>数据变更</strong>, 我们分情况看一下这两个设计.</p><h3 id="数据插入"><a href="#数据插入" class="headerlink" title="数据插入"></a>数据插入</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210830145322611.png" alt="image-20210830145322611"></p><ol><li>数据写入shard的一个节点</li><li>在本地节点中, 生成DP文件</li><li>并将DP写入本地的事件写入到ZK上</li><li>其他副本监控着写入时间, 如果发现有数据写入的时间, 就会立刻读取事件. </li><li>事件写明了DP的编号以及DP的地址, 副本节点会主动去事件写入节点拿数据</li><li>Clickhouse由一个内部端口, 专门用于节点间数据传输, 通过该端口将DP文件传输到节点本地</li><li>然后再将DP数据加载到节点内部, 并更新ZK上的信息, 告诉ZK自己已经拿到DP, 并且其他副本也可以从自己这儿下载DP</li></ol><p>默认情况下, Clickhouse只会写入一个节点, 然后写入请求就接受了, 这种情况下, 一旦主节点挂机, 这份数据就无法被传输, 可以设置配置项<code>insert_quorum</code>强制副本写入其他节点后再返回.</p><h3 id="数据变更"><a href="#数据变更" class="headerlink" title="数据变更"></a>数据变更</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210830145330506.png" alt="image-20210830145330506"></p><ol><li>事件的触发有两种, 一种由内部的<code>merge</code>请求触发, 一种由外部客户端触发的<code>mutation</code>或者<code>alter</code>方法</li><li>主节点或者客户端触发变更后, 由响应节点将事件写入到Zookeeper, 写入后该任务就算完成了</li><li>其他所有副本都监听这个事件</li><li>一旦发现新的变更, 就会自己按照事件内容处理任务</li></ol><p>由于事件处理效率并不相同, 如果有一个节点, 处理事件晚于其他节点很多, 那么它可能直接去其他节点获取已经变更过的数据, 而非在自己节点完成.</p><h2 id="数据分区"><a href="#数据分区" class="headerlink" title="数据分区"></a>数据分区</h2><p>分布式DDL的逻辑相对来说比较简单,  也是在Zookeeper上写入一条事件记录, 然后每个节点监听执行, 这个步骤就不展开了, 这里只聚焦于分区写入的流程.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210830152305274.png" alt="image-20210830152305274"></p><ol><li>数据写入分布式表</li><li>接收到数据写入的节点, 将分布式表写入本地的临时文件夹中, 临时文件夹包含远程shard的地址</li><li>本地的数据直接attach数据, 然后分批发送远程的数据.</li></ol><p>分布式插入可以设置为同步插入模式, 需要设置配置项<code>insert_distributed_sync=1</code>.  由于分布式表和local表的分离, 默认的异步插入会产生积压, 此时如果将local表的字段类型修改后, 整个积压任务就会一直异常, 并卡住, 后续的数据都无法插入.</p><p>因此, 我们写入数据的时候, 会尽量直接写local表, 不是去重表类型的, 基本上不让用户使用分布式表写入.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入浅出Clickhouse: MergeTree引擎设计</title>
      <link href="/2021/08/15/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAClickhouse-MergeTree%E5%BC%95%E6%93%8E%E8%AE%BE%E8%AE%A1/"/>
      <url>/2021/08/15/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAClickhouse-MergeTree%E5%BC%95%E6%93%8E%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><code>LSM</code>(Log-Struct Merge Tree)是大数据领域一种非常常见的技术, 例如<code>LevelDB</code>, <code>RocksDB</code>亦或是<code>HBase</code>都采用了<code>LSM</code>结构来完成存储系统的构架.</p><p>但你有没有发现, 这些大数据系统, 大多数都是KV方式的存储系统,  对外的接口也是以点查方式提供的.  </p><p>虽然例如<code>HBase</code>的RowKey是全局有序的, 但是如果使用RowKey做大规模的范围查询, 它的整体效果可能还比不上在HDFS的文件上直接做过滤更有效.</p><p>基于以上的问题, 我们再回顾一下LSM的设计, 解答一下为什么LSM最适合点查, 然后再介绍Clickhouse基于LSM做了哪些取舍, 让MergeTree适合做范围的查询分析.</p><h2 id="LSM树回顾"><a href="#LSM树回顾" class="headerlink" title="LSM树回顾"></a>LSM树回顾</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/v2-cbf13812e03058e856dbc97d4a7a495d_1440w.jpg" alt="img"></p><p>LSM的架构如上图所示,  数据先存放在Memory中, 然后通过一次的合并, 数据会固化到磁盘上.</p><p>为了防止每次合并都处理全部的数据, 因此LSM会将数据分层,  上一层比下一层小很多, 并且上一层的合并频率也比下一层多很多次.</p><p>由于需要合并去重的要求, 因此LSM树必须要指定一个类似主键之类的不重复的key.</p><p>另外一个点, 在Level-1以上, Key是在不同的文件块中, 是绝对不重复的, 而由于一般多是采用Range分区的方式, 我们可以称LSM树是Key键是<strong>全局有序且不重复</strong>的 </p><p>当数据更新的时候, LSM只会将数据标记为删除, 或者更新一个数据的版本,  等到后续做数据合并的时候, 再做去重或者删除数据的动作</p><p>这就是所谓的<strong>写放大</strong>问题</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/rocksdb_read_process.png" alt="img"></p><p>当读取数据的时候, 会先读取<code>MemTable</code>, 再读取<code>Immutable MemTable</code>, 最后在磁盘上一层一层读取文件, 直到最后读取到数据.</p><p>造成<strong>读放大</strong>的原因有两个点: 第一, 数据一旦没有命中, 就要往更深的层查找; 第二, 每个层中, 虽然因为全局有序, 点查只需要扫描一个SST文件, 但必须一次扫描整个SST文件.</p><p>总结一下, LSM的特点有:</p><ol><li>必须有Key键, 且全局有序不重复</li><li>写入时, 一层一层写入, 更新删除只做标记</li><li>读取时, 一层一层读取, 读取效率在于命中率</li></ol><p>然后再结合点查询的场景,  回答一下为什么LSM最适合点查:</p><ol><li>Key键全局有序且不重复, 这样查询时一下就能定位到对应的数据地址,  符合点查对对查询时延的要求</li><li>插入修改删除只是在Mem中添加数据, 这里就要求一次插入的数据不能太多, 不然内存存储不下, 就要开始合并, 会导致系统不稳定, 而点查一般一次只修改一行数据, 完美符合</li><li>读取数据时候, 时延在于命中率, 而点查询的热点效应明显, 主要在前几层能够命中, 整体读取的效率可以接受.</li></ol><blockquote><p>LSM树相对于B+树来说, 强化了Update和Delete的能力, 相对弱化的读取的能力</p></blockquote><p>但是如果是范围查询, 就会出现:</p><ol><li>一次更新数据可能是全部数据的20%以上, 例如修改某个字段的标号, 或者删除某些字段的数据</li><li>读取时给一个宽裕的过滤范围, 大多数的数据都能匹配中, 例如一个宽泛的前缀匹配</li></ol><p>上面两个场景, 如果用LSM树的架构, 基本上是不可取的.</p><blockquote><p>LSM树的参考<a href="https://zhuanlan.zhihu.com/p/103968892">文章1</a>, <a href="http://alexstocks.github.io/html/rocksdb.html">文章2</a></p></blockquote><h2 id="Clickhouse-MergeTree设计"><a href="#Clickhouse-MergeTree设计" class="headerlink" title="Clickhouse MergeTree设计"></a>Clickhouse MergeTree设计</h2><p>Clickhouse作为一个OLAP系统放弃了对于点查场景的支持, 而是主要面向范围查询的支持, 因此对于MergeTree做了大刀阔斧的修改.</p><p>对于Clickhouse来说, 没有单条的插入, 插入修改都抽象为对DataPart的操作.</p><p>DataPart在逻辑可以理解为批量的一组数据, 在物理上是磁盘上的一个文件夹, </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210823132128627.png" alt="image-20210823132128627"></p><p>Clickhouse的数据组织, 如上图所示, 相比于MergeTree:</p><ol><li>放弃了全局有序的约束, 只保证DataPart级别是有序的,  此时面对OLAP场景的批量插入, 只需要针对当前插入的数据做排序, 并插入为DP即可</li><li>放弃了多层的架构,  没有Memory层,  磁盘也只有一层. 在做Mutation(LSM对应Delete和Update)的时候,  也是DataPart到DataPart的转化</li></ol><p>以上这两个设计的特点, 解决了写入和修改的场景, 但有一个明确的假设条件:  <strong>高吞吐的插入</strong>和 <strong>低频次大数据量的修改</strong></p><p>此外这种方式, 比较实现列存模式(一个DP包含很行的数据), 因此天然比较适合OLAP,  而LSM比较适合行存, 一般用于OLTP.</p><p>但任何设计都有好坏, 一旦应用场景无法满足Clickhouse假设, 那么整体效果就会比较差:</p><ol><li>插入批次小: Clickhouse由于不像LSM一样全局有序, 可以明确数据必然在一个数据块中, 因此查询时需要扫描分区裁剪后的所有DP, 因此CK会根据DataPart数量, 合并DataPart, 防止DataPart个数太多,  扫描性能太慢. 但如果插入批次小, 就会不停的触发Merge动作, 导致不必要的资源浪费, 因此都推荐在客户端攒批的方式写入 </li><li>修改频次高:  Mutation在Clickhouse被设计为一个非常重的操作, 因为需要处理大量数据, 但如果此时频次一高,  整个系统CPU马上就会上去</li><li>数据要求实时去重: 在Clickhouse设计中, DataPart数据间是没有关联的, 只有在Merge时才会产生关系, 但如果需要实时去重, 那么DP之间就会产生关联性(后写入的数据需要覆盖前面写入的数据), 不管后面如何优化, 这类场景的查询和写入都会比原生的差很多.</li></ol><h2 id="各种MergeTree"><a href="#各种MergeTree" class="headerlink" title="各种MergeTree"></a>各种MergeTree</h2><h3 id="普通MergeTree"><a href="#普通MergeTree" class="headerlink" title="普通MergeTree"></a>普通MergeTree</h3><p>MergeTree表引擎主要用于海量数据分析，支持数据分区、存储有序、主键索引、稀疏索引、数据TTL等。MergeTree支持所有ClickHouse SQL语法，但是有些功能与MySQL并不一致，比如在MergeTree中主键并不用于去重，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_tbl (</span><br><span class="line">  id UInt16,</span><br><span class="line">  create_time <span class="type">Date</span>,</span><br><span class="line">  comment Nullable(String)</span><br><span class="line">) ENGINE <span class="operator">=</span> MergeTree()</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> create_time</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span>  (id, create_time)</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (id, create_time)</span><br><span class="line">TTL create_time <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="number">1</span> <span class="keyword">MONTH</span></span><br><span class="line">SETTINGS index_granularity<span class="operator">=</span><span class="number">8192</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 虽然主键定义为(id, create_time), 但是最后还是5条数据, 并不会改变</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl <span class="keyword">values</span>(<span class="number">0</span>, <span class="string">&#x27;2019-12-12&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl <span class="keyword">values</span>(<span class="number">0</span>, <span class="string">&#x27;2019-12-12&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;2019-12-13&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;2019-12-13&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl <span class="keyword">values</span>(<span class="number">2</span>, <span class="string">&#x27;2019-12-14&#x27;</span>, <span class="keyword">null</span>);</span><br></pre></td></tr></table></figure><h3 id="ReplacingMergeTree"><a href="#ReplacingMergeTree" class="headerlink" title="ReplacingMergeTree"></a>ReplacingMergeTree</h3><p>为了解决MergeTree相同主键无法去重的问题，ClickHouse提供了ReplacingMergeTree引擎，用来做去重, 但这个引擎依然有很多限制:</p><ul><li>在没有彻底optimize之前，可能无法达到主键去重的效果，比如部分数据已经被去重，而另外一部分数据仍旧有主键重复；</li><li>在分布式场景下，相同primary key的数据可能被sharding到不同节点上，不同shard间可能无法去重；</li><li>optimize是后台动作，无法预测具体执行时间点；</li><li>手动执行optimize在海量数据场景下要消耗大量时间，无法满足业务即时查询的需求；</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_tbl_replacing (</span><br><span class="line">  id UInt16,</span><br><span class="line">  create_time <span class="type">Date</span>,</span><br><span class="line">  comment Nullable(String)</span><br><span class="line">) ENGINE <span class="operator">=</span> ReplacingMergeTree()</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> create_time</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span>  (id, create_time)</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY (id, create_time)</span><br><span class="line">TTL create_time <span class="operator">+</span> <span class="type">INTERVAL</span> <span class="number">1</span> <span class="keyword">MONTH</span></span><br><span class="line">SETTINGS index_granularity<span class="operator">=</span><span class="number">8192</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 写入主键重复的数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl_replacing <span class="keyword">values</span>(<span class="number">0</span>, <span class="string">&#x27;2019-12-12&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl_replacing <span class="keyword">values</span>(<span class="number">0</span>, <span class="string">&#x27;2019-12-12&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl_replacing <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;2019-12-13&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl_replacing <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;2019-12-13&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test_tbl_replacing <span class="keyword">values</span>(<span class="number">2</span>, <span class="string">&#x27;2019-12-14&#x27;</span>, <span class="keyword">null</span>);</span><br><span class="line"><span class="comment">-- 此时数据依然为5条</span></span><br><span class="line"><span class="comment">-- 强制后台compaction：</span></span><br><span class="line">optimize <span class="keyword">table</span> test_tbl_replacing <span class="keyword">final</span>;</span><br><span class="line"><span class="comment">-- 此时数据为3条</span></span><br></pre></td></tr></table></figure><h3 id="CollapsingMergeTree"><a href="#CollapsingMergeTree" class="headerlink" title="CollapsingMergeTree"></a>CollapsingMergeTree</h3><p>ClickHouse实现了CollapsingMergeTree来消除ReplacingMergeTree的限制。该引擎要求在建表语句中指定一个标记列Sign，后台Compaction时会将主键相同、Sign相反的行进行折叠，也即删除。</p><p>CollapsingMergeTree将行按照Sign的值分为两类：Sign=1的行称之为状态行，Sign=-1的行称之为取消行。</p><p>每次需要新增状态时，写入一行状态行；需要删除状态时，则写入一行取消行。</p><p>在后台Compaction时，状态行与取消行会自动做折叠（删除）处理。而尚未进行Compaction的数据，状态行与取消行同时存在。</p><p>因此为了能够达到主键折叠（删除）的目的，需要业务层进行适当改造</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> UAct</span><br><span class="line">(</span><br><span class="line">    UserID UInt64,</span><br><span class="line">    PageViews UInt8,</span><br><span class="line">    Duration UInt8,</span><br><span class="line">    Sign Int8</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> CollapsingMergeTree(Sign)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> UserID;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入状态行，注意sign一列的值为1</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> UAct <span class="keyword">VALUES</span> (<span class="number">4324182021466249494</span>, <span class="number">5</span>, <span class="number">146</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入一行取消行，用于抵消上述状态行。注意sign一列的值为-1，其余值与状态行一致；</span></span><br><span class="line"><span class="comment">-- 并且插入一行主键相同的新状态行，用来将PageViews从5更新至6，将Duration从146更新为185.</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> UAct <span class="keyword">VALUES</span> (<span class="number">4324182021466249494</span>, <span class="number">5</span>, <span class="number">146</span>, <span class="number">-1</span>), (<span class="number">4324182021466249494</span>, <span class="number">6</span>, <span class="number">185</span>, <span class="number">1</span>);</span><br><span class="line"><span class="comment">-- 查询数据：可以看到未Compaction之前，状态行与取消行共存。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 为了获取正确的sum值，需要改写SQL： </span></span><br><span class="line"><span class="comment">-- sum(PageViews) =&gt; sum(PageViews * Sign)、 </span></span><br><span class="line"><span class="comment">-- sum(Duration) =&gt; sum(Duration * Sign)</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    UserID,</span><br><span class="line">    <span class="built_in">sum</span>(PageViews <span class="operator">*</span> Sign) <span class="keyword">AS</span> PageViews,</span><br><span class="line">    <span class="built_in">sum</span>(Duration <span class="operator">*</span> Sign) <span class="keyword">AS</span> Duration</span><br><span class="line"><span class="keyword">FROM</span> UAct</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> UserID</span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">sum</span>(Sign) <span class="operator">&gt;</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 强制后台Compaction</span></span><br><span class="line">optimize <span class="keyword">table</span> UAct <span class="keyword">final</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 再次查询，可以看到状态行、取消行已经被折叠，只剩下最新的一行状态行。</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> UAct;</span><br></pre></td></tr></table></figure><p>CollapsingMergeTree虽然解决了主键相同的数据即时删除的问题，但是状态持续变化且多线程并行写入情况下，状态行与取消行位置可能乱序，导致无法正常折叠。</p><h3 id="VersionedCollapsingMergeTree"><a href="#VersionedCollapsingMergeTree" class="headerlink" title="VersionedCollapsingMergeTree"></a>VersionedCollapsingMergeTree</h3><p>为了解决CollapsingMergeTree乱序写入情况下无法正常折叠问题，VersionedCollapsingMergeTree表引擎在建表语句中新增了一列Version，用于在乱序情况下记录状态行与取消行的对应关系。主键相同，且Version相同、Sign相反的行，在Compaction时会被删除。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> UAct_version</span><br><span class="line">(</span><br><span class="line">    UserID UInt64,</span><br><span class="line">    PageViews UInt8,</span><br><span class="line">    Duration UInt8,</span><br><span class="line">    Sign Int8,</span><br><span class="line">    Version UInt8</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> VersionedCollapsingMergeTree(Sign, Version)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> UserID;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 先插入一行取消行，注意Signz=-1, Version=1</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> UAct_version <span class="keyword">VALUES</span> (<span class="number">4324182021466249494</span>, <span class="number">5</span>, <span class="number">146</span>, <span class="number">-1</span>, <span class="number">1</span>);</span><br><span class="line"><span class="comment">-- 后插入一行状态行，注意Sign=1, Version=1；及一行新的状态行注意Sign=1, Version=2，将PageViews从5更新至6，将Duration从146更新为185。</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> UAct_version <span class="keyword">VALUES</span> (<span class="number">4324182021466249494</span>, <span class="number">5</span>, <span class="number">146</span>, <span class="number">1</span>, <span class="number">1</span>),(<span class="number">4324182021466249494</span>, <span class="number">6</span>, <span class="number">185</span>, <span class="number">1</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 为了获取正确的sum值，需要改写SQL： </span></span><br><span class="line"><span class="comment">-- sum(PageViews) =&gt; sum(PageViews * Sign)、 </span></span><br><span class="line"><span class="comment">-- sum(Duration) =&gt; sum(Duration * Sign)</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    UserID,</span><br><span class="line">    <span class="built_in">sum</span>(PageViews <span class="operator">*</span> Sign) <span class="keyword">AS</span> PageViews,</span><br><span class="line">    <span class="built_in">sum</span>(Duration <span class="operator">*</span> Sign) <span class="keyword">AS</span> Duration</span><br><span class="line"><span class="keyword">FROM</span> UAct_version</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> UserID</span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">sum</span>(Sign) <span class="operator">&gt;</span> <span class="number">0</span>;</span><br><span class="line"><span class="comment">-- 强制后台Compaction</span></span><br><span class="line">optimize <span class="keyword">table</span> UAct_version <span class="keyword">final</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 再次查询，可以看到即便取消行与状态行位置乱序，仍旧可以被正确折叠。</span></span><br></pre></td></tr></table></figure><h3 id="SummingMergeTree"><a href="#SummingMergeTree" class="headerlink" title="SummingMergeTree"></a>SummingMergeTree</h3><p>ClickHouse通过SummingMergeTree来支持对主键列进行预先聚合。在后台Compaction时，会将主键相同的多行进行sum求和，然后使用一行数据取而代之，从而大幅度降低存储空间占用，提升聚合计算性能。</p><p>值得注意的是：</p><ul><li>ClickHouse只在后台Compaction时才会进行数据的预先聚合，而compaction的执行时机无法预测，所以可能存在部分数据已经被预先聚合、部分数据尚未被聚合的情况。因此，在执行聚合计算时，SQL中仍需要使用GROUP BY子句。</li><li>在预先聚合时，ClickHouse会对主键列之外的其他所有列进行预聚合。如果这些列是可聚合的（比如数值类型），则直接sum；如果不可聚合（比如String类型），则随机选择一个值。</li><li>通常建议将SummingMergeTree与MergeTree配合使用，使用MergeTree来存储具体明细，使用SummingMergeTree来存储预先聚合的结果加速查询。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> summtt</span><br><span class="line">(</span><br><span class="line">    key UInt32,</span><br><span class="line">    <span class="keyword">value</span> UInt32</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> SummingMergeTree()</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> key</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> summtt <span class="keyword">Values</span>(<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">-- compaction前查询，仍存在多行</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> summtt;</span><br><span class="line">┌─key─┬─<span class="keyword">value</span>─┐</span><br><span class="line">│   <span class="number">1</span> │     <span class="number">1</span> │</span><br><span class="line">│   <span class="number">1</span> │     <span class="number">2</span> │</span><br><span class="line">│   <span class="number">2</span> │     <span class="number">1</span> │</span><br><span class="line">└─────┴───────┘</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 通过GROUP BY进行聚合计算</span></span><br><span class="line"><span class="keyword">SELECT</span> key, <span class="built_in">sum</span>(<span class="keyword">value</span>) <span class="keyword">FROM</span> summtt <span class="keyword">GROUP</span> <span class="keyword">BY</span> key</span><br><span class="line">┌─key─┬─<span class="built_in">sum</span>(<span class="keyword">value</span>)─┐</span><br><span class="line">│   <span class="number">2</span> │          <span class="number">1</span> │</span><br><span class="line">│   <span class="number">1</span> │          <span class="number">3</span> │</span><br><span class="line">└─────┴────────────┘</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 强制compaction</span></span><br><span class="line">optimize <span class="keyword">table</span> summtt <span class="keyword">final</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- compaction后查询，可以看到数据已经被预先聚合</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> summtt;</span><br><span class="line">┌─key─┬─<span class="keyword">value</span>─┐</span><br><span class="line">│   <span class="number">1</span> │     <span class="number">3</span> │</span><br><span class="line">│   <span class="number">2</span> │     <span class="number">1</span> │</span><br><span class="line">└─────┴───────┘</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- compaction后，仍旧需要通过GROUP BY进行聚合计算</span></span><br><span class="line"><span class="keyword">SELECT</span> key, <span class="built_in">sum</span>(<span class="keyword">value</span>) <span class="keyword">FROM</span> summtt <span class="keyword">GROUP</span> <span class="keyword">BY</span> key</span><br><span class="line">┌─key─┬─<span class="built_in">sum</span>(<span class="keyword">value</span>)─┐</span><br><span class="line">│   <span class="number">2</span> │          <span class="number">1</span> │</span><br><span class="line">│   <span class="number">1</span> │          <span class="number">3</span> │</span><br><span class="line">└─────┴────────────┘</span><br></pre></td></tr></table></figure><h3 id="AggregatingMergeTree"><a href="#AggregatingMergeTree" class="headerlink" title="AggregatingMergeTree"></a>AggregatingMergeTree</h3><p>AggregatingMergeTree也是预先聚合引擎的一种，用于提升聚合计算的性能。与SummingMergeTree的区别在于：SummingMergeTree对非主键列进行sum聚合，而AggregatingMergeTree则可以指定各种聚合函数。</p><p>AggregatingMergeTree的语法比较复杂，需要结合物化视图或ClickHouse的特殊数据类型AggregateFunction一起使用。在insert和select时，也有独特的写法和要求：写入时需要使用-State语法，查询时使用-Merge语法。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建立明细表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> visits</span><br><span class="line">(</span><br><span class="line">    UserID UInt64,</span><br><span class="line">    CounterID UInt8,</span><br><span class="line">    StartDate <span class="type">Date</span>,</span><br><span class="line">    Sign Int8</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> CollapsingMergeTree(Sign)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> UserID;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 对明细表建立物化视图，该物化视图对明细表进行预先聚合</span></span><br><span class="line"><span class="comment">-- 注意：预先聚合使用的函数分别为： sumState, uniqState。对应于写入语法&lt;agg&gt;-State.</span></span><br><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span> visits_agg_view</span><br><span class="line">ENGINE <span class="operator">=</span> AggregatingMergeTree() <span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(StartDate) <span class="keyword">ORDER</span> <span class="keyword">BY</span> (CounterID, StartDate)</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span></span><br><span class="line">    CounterID,</span><br><span class="line">    StartDate,</span><br><span class="line">    sumState(Sign)    <span class="keyword">AS</span> Visits,</span><br><span class="line">    uniqState(UserID) <span class="keyword">AS</span> Users</span><br><span class="line"><span class="keyword">FROM</span> visits</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> CounterID, StartDate;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 插入明细数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> visits <span class="keyword">VALUES</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="string">&#x27;2019-11-11&#x27;</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> visits <span class="keyword">VALUES</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="string">&#x27;2019-11-12&#x27;</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 对物化视图进行最终的聚合操作</span></span><br><span class="line"><span class="comment">-- 注意：使用的聚合函数为 sumMerge， uniqMerge。对应于查询语法&lt;agg&gt;-Merge.</span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    StartDate,</span><br><span class="line">    sumMerge(Visits) <span class="keyword">AS</span> Visits,</span><br><span class="line">    uniqMerge(Users) <span class="keyword">AS</span> Users</span><br><span class="line"><span class="keyword">FROM</span> visits_agg_view</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> StartDate</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> StartDate;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 普通函数 sum, uniq不再可以使用</span></span><br><span class="line"><span class="comment">-- 如下SQL会报错： Illegal type AggregateFunction(sum, Int8) of argument </span></span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    StartDate,</span><br><span class="line">    <span class="built_in">sum</span>(Visits),</span><br><span class="line">    uniq(Users)</span><br><span class="line"><span class="keyword">FROM</span> visits_agg_view</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> StartDate</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> StartDate;</span><br></pre></td></tr></table></figure><blockquote><p>描述和示例皆摘抄自<a href="https://developer.aliyun.com/article/762461">阿里云</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入浅出Clickhouse: 索引结构设计</title>
      <link href="/2021/08/15/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAClickhouse-%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
      <url>/2021/08/15/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAClickhouse-%E7%B4%A2%E5%BC%95%E7%BB%93%E6%9E%84%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="数据目录"><a href="#数据目录" class="headerlink" title="数据目录"></a>数据目录</h2><p>当Clickhouse创建一个表,  会在配置文件<code>path</code>指定的路径下对应数据目录.</p><p><strong>数据目录</strong>的路径为:  <code>&#123;path&#125;</code>/<code>data</code>/<code>&#123;database&#125;</code>/<code>&#123;table&#125;</code></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> src (</span><br><span class="line">id Int32,</span><br><span class="line"><span class="keyword">year</span> String,</span><br><span class="line">num Int64,</span><br><span class="line">index a num TYPE minmax GRANULARITY <span class="number">2</span></span><br><span class="line">) Engine <span class="operator">=</span>MergeTree()</span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span> id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> (id, <span class="keyword">year</span>)</span><br><span class="line"><span class="keyword">primary</span> key (id, <span class="keyword">year</span>)</span><br><span class="line"></span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 插入数据</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> src <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;2021&#x27;</span>,<span class="number">12</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> src <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;2021&#x27;</span>,<span class="number">12</span>);</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> 插入数据</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> src <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;2020&#x27;</span>,<span class="number">13</span>);</span><br></pre></td></tr></table></figure><p>例如上面的例子, 在<code>default</code>数据库上创建一个<code>src</code>的表, 如果<code>path</code>是在<code>~/clickhouse/data</code>目录, 那么整个目录为</p><p><code>~/clickhouse/data/data/default/src</code> </p><p>当插入3次数据后, Clickhouse会再数据目录下,  每次都新建一个目录, 如下图所示, 这种目录在Clickhouse称之为<code>DataPart</code></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210828175325388.png" alt="image-20210828175325388"></p><p>目录的格式为: <code>partition</code>_<code>min_block</code>_<code>max_block</code>_<code>level</code></p><ul><li><code>partition</code>是分区的值, 根据建表的分区表示计算得来, 是一个规定的值; Clickhouse的目录并非按照分区规整的, 在数据目录下, 只有DataPart这层数据了,  也就是说, 通一个分区可能在多个DataPart上, 如上面的<code>1_1_1_0</code>和<code>1_2_2_0</code></li><li><code>min_block</code>和<code>max_block</code>中的<code>block</code>是一个单挑递增的计数器,  插入3次后, 计数器就变成了3. 新的<code>DataPart</code>的<code>min_block</code>和<code>max_block</code>是一样的, 但在后续merge之后, 会变成一个范围值.  </li><li><code>level</code>表示DataPart生命年龄, 当出现merge或者mutation操作时, level就会加1</li></ul><p>举上面例子, 如果执行<code>OPTIMIZE TABLE src</code> , 强制表进行合并, 此时<code>1_1_1_0</code>和<code>1_2_2_0</code>两个DP会强制合并, 合并为<code>1_1_2_1</code>, 新DP的<code>level</code>变成了2, 而<code>min_max_block</code>由合并的DP的值确定.  另外合并只发送在同一个Partition内</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210828180126364.png" alt="image-20210828180126364"></p><blockquote><p>合并操作, 并不会马上删除老的DP, 而是将其设置为<code>inactive</code>, 读操作只会读取新的DP, 因此上图总共有4个DP, 过一段只会剩下两个DP</p></blockquote><p>DataPart是存储数据的地方,  以上面的一个DataPart为例:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210828175418382.png" alt="image-20210828175418382"></p><ul><li><code>checksums.txt</code>是整个DP的检验和</li><li><code>columns.txt</code>表示DP的列结构,  Clickhouse将列字段放在DP级别的原因是,  DP在做mutation的时候, 列可能会出现不一样</li><li><code>data.bin</code>存储着所有列的值, 称之为<code>数据块</code></li><li><code>data.mrk3</code>是<code>数据块</code>的索引文件</li><li><code>default_compression_codec.txt</code>压缩方式</li><li><code>minmax_id.idx</code>分区文件的<code>min_max</code>索引</li><li><code>partition.dat</code>分区的值</li><li><code>primary.idx</code>主键索引, 或者成为一级索引</li><li><code>skp_idx_a.idx2</code>和<code>skp_idx_a.mrk3</code> 跳数索引, 或者叫做二级索引.</li></ul><p>当一个DataPart内数据量比较大的时候, <code>data.bin</code>会按照各自的列拆分为<code>id.bin</code> <code>num.bin</code> <code>year.bin</code>, 同理的还有<code>data.mrk3</code></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210828183605821.png" alt="image-20210828183605821"></p><p>Clickhouse有两层索引, 一层为基于分区的索引, 这个在另外的文章中已经讲过,  它的目的是选择到具体的DP; 另外一层是基于数据排序的索引,  描述的是在一个DP内, 数据时如何组织以及被索引的.</p><h2 id="数据索引"><a href="#数据索引" class="headerlink" title="数据索引"></a>数据索引</h2><p>废话不多说, 直接上图, 依然以上面的建表语句为例, 与其中相关的是<code>order by</code> 和 <code>partition by</code>的语法.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210828183928398.png" alt="image-20210828183928398"></p><p>在Clickhouse的一个DataPart中, 数据的逻辑分布可以想象成一个矩阵, 如图中的<code>表示例数据</code>方式陈列.</p><p><strong>首先</strong>, 行与行之间的排序并非是固定的, 而是根据<code>order by</code><strong>定义的方式排序</strong>, 例如案例中按照<code>id</code>和<code>year</code>排序, 那么<code>num</code>字段的位置也随着前面字段改变. 另外值得说的一点, 这个排序只是在DP内的, DP之间并没有顺序关系.</p><p><strong>其次</strong>, 在物理上Clickhouse将按照每8192行数据切分整个矩阵, 将大矩阵切分为一个一个子矩阵, 在图中用<code>编号</code>来表示. <code>8192</code>的数值由配置项<code>index_granularity</code>决定, 该配置为索引结构中最核心的配置.</p><p>Clickhouse的主键索引是一个<strong>稀疏索引</strong>, 它并不存储每一个行的数据, 而是存储每个子矩阵的第一个行数据, 因此8192行数据才会有一个索引值, 索引非常小, 对应的代价就是查找时, 需要用折半查找的方式来查询具体的编号, 复杂度为<code>log(n)</code></p><p>主键索引可以是组合索引, 类似于mysql的组合索引, CK在查询时也必须满足<code>最左匹配原则</code>, 即查询时必须从最左的字段匹配起, 一旦有跳过字段方式, 索引将无法命中.</p><p><strong>再次</strong>, Clickhouse对于非排序字段的查询, 设计了一种叫做<code>跳数索引</code>的二级索引方式, 名为跳数, 意思是并非记录每个编号内的索引, 而是选择一批编号进行计算, 例如图中是按照2个编号算一个<code>跳数索引</code>的方式. </p><p>跳数索引有三种: <code>min_max</code>,<code>set</code>和<code>bloomfilter</code>, 以<code>min_max</code>为例, 它存储的是两个数据块中的最大最小值, 此外跳数索引支持表达式, 但不是所有函数都支持, 支持的函数列表, 可以参考<a href="https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes">官网</a></p><p>跳数索引主要的目的为判断查询的数值是否存在, 如果不存在则跳过, 由于跳数索引是随机的, 因此的查询复杂度为<code>n</code></p><blockquote><p>其实不应该叫做<code>二级索引</code>, 因为Clickhouse没有回表的动作, <code>跳数索引</code>选中数据块之后, 就直接通过暴力扫描的方式开始计算了.</p></blockquote><p><strong>最后</strong>, Clickhouse的数据存放在bin文件中, 这是真正的存储的地方. Clickhouse<strong>并非<code>innodb</code>类似的聚族索引将数据文件和索引放在一起</strong>,  而是数据文件和索引文件分开存储. 图中只列举了<code>num.bin</code>文件, 实际上还有<code>id.bin</code>和<code>year.bin</code>.</p><p>数据存储也并非按照8192行方式存储, 而是通过一个个数据块方式存储. 一个数据块大小为64K ~ 1Mb, 如果一个编号的数据太小, 就会将合并多个编号内的数据; 如果一个编号数据又太大, 就会拆分一个编号的数据. 而<code>num.mrk</code>文件实际上就是管理这层一对多,多对一关系以及维护存储上offset索引的数据结构. (具体的对应关系不展开了, 看图理解)</p><p>Clickhouse为什么不直接按照8192行的方式存储数据呢, 我个人的理解是为了最终数据块过大过小影响读取的稳定性. 当数据过小时, 多次查询索引和读数据, 会引起过多的IO. 当数据过大时, 会挤占过多的内存空间影响系统的稳定性.</p><p><strong>总结一下</strong></p><p>Clickhouse索引的特点为: <code>排序索引</code>+<code>稀疏索引</code> + <code>列式存储</code>,  因此相应的Clickhouse最合适的场景就是<strong>基于排序字段的范围过滤后的聚合查询</strong>.</p><ul><li>因为<strong>排序索引</strong>,  所有基于<strong>排序字段的查询</strong>会明显由于MR类型计算, 否则Hive/Spark这类动态资源的更优</li><li>由于<strong>稀疏索引</strong>, 点查询的效率可能没有KV型数据库高, 因此适合相对大范围的过滤条件</li><li>因为<strong>列式存储</strong>,  数据压缩率高, 对应做聚合查询效率也会更高.</li></ul><p><strong>与Hadoop列存结构的相比</strong></p><p>以Parquet的为例, Parquet也会按照行列方式切分整个矩阵, 用<code>Page</code>或者<code>RowGroup</code>的概念实现, 这点跟Clickhouse一样</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/FileLayout.gif" alt="File Layout"></p><p>Parquet也有部分索引元数据, 能够实现<code>谓词下推(Predicate PushDown)</code>的能力, 因为元数据中包含整个块中的最大最小值, 因此能够方便的过滤数据块. 但是这个索引更像是CK中的<code>跳数索引</code></p><p>但是由于Parquet是一个通用的存储格式, 因此它不能像Clickhouse一样定义排序字段, 因此也无法享受折半查找带来的查询优势. </p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++用法记录</title>
      <link href="/2021/08/12/C-%E7%94%A8%E6%B3%95%E8%AE%B0%E5%BD%95/"/>
      <url>/2021/08/12/C-%E7%94%A8%E6%B3%95%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="无名的命名空间"><a href="#无名的命名空间" class="headerlink" title="无名的命名空间"></a>无名的命名空间</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">fun</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">//....</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于命名空间没有名字，在其他文件中显然无法引用，它只在本文件的作用域有效。</p><p>若无名命名空间的成员fun函数的作用域为文件A，在文件A中使用无名命名空间的成员，不用也无法用命名空间名限定。</p><h2 id="模板里面使用具体的类型"><a href="#模板里面使用具体的类型" class="headerlink" title="模板里面使用具体的类型"></a>模板里面使用具体的类型</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">NeedChild</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> Condition = <span class="built_in">bool</span> (*)(<span class="type">const</span> ASTPtr &amp; node, <span class="type">const</span> ASTPtr &amp; child);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">bool</span> <span class="title">all</span><span class="params">(<span class="type">const</span> ASTPtr &amp;, <span class="type">const</span> ASTPtr &amp;)</span> </span>&#123; <span class="keyword">return</span> <span class="literal">true</span>; &#125;</span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">bool</span> <span class="title">none</span><span class="params">(<span class="type">const</span> ASTPtr &amp;, <span class="type">const</span> ASTPtr &amp;)</span> </span>&#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/// Simple matcher for one node type. Use need_child function for complex traversal logic.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Data_, NeedChild::Condition need_child = NeedChild::all, <span class="keyword">typename</span> T = ASTPtr&gt;</span><br><span class="line"><span class="keyword">class</span> OneTypeMatcher</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> Data = Data_;</span><br><span class="line">    <span class="keyword">using</span> TypeToVisit = <span class="keyword">typename</span> Data::TypeToVisit;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">bool</span> <span class="title">needChildVisit</span><span class="params">(<span class="type">const</span> ASTPtr &amp; node, <span class="type">const</span> ASTPtr &amp; child)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">need_child</span>(node, child); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">visit</span><span class="params">(T &amp; ast, Data &amp; data)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">auto</span> * t = <span class="built_in">typeid_cast</span>&lt;TypeToVisit *&gt;(ast.<span class="built_in">get</span>()))</span><br><span class="line">            data.<span class="built_in">visit</span>(*t, ast);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>NeedChild</code>这种用法, 目前不知道具体的称谓.</p><h2 id="Lambda表达式"><a href="#Lambda表达式" class="headerlink" title="Lambda表达式"></a>Lambda表达式</h2><p>基本格式</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> fun = [捕获参数](函数参数)&#123;函数体&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> fun = []()&#123; std::cout &lt;&lt; <span class="string">&quot;Hello Lambda&quot;</span> &lt;&lt; std::endl; &#125;;</span><br></pre></td></tr></table></figure><p>捕获列表:</p><ul><li><code>[]</code>: 没有使用任何函数对象参数</li><li><code>[=]</code>: 函数体内可以使用Lambda所在作用范围内所有可见的局部变量（包括Lambda所在类的this），并且是值传递方式（相当于编译器自动为我们按值传递了所有局部变量）</li><li><code>[&amp;]</code>:  函数体内可以使用Lambda所在作用范围内所有可见的局部变量（包括Lambda所在类的this），并且是引用传递方式（相当于编译器自动为我们按引用传递了所有局部变量）</li><li><code>[this]</code>: 函数体内可以使用Lambda所在类中的成员变量</li><li><code>[a]</code>: 将a按值进行传递。按值进行传递时，函数体内不能修改传递进来的a的拷贝，因为默认情况下函数是const的。要修改传递进来的a的拷贝，可以添加mutable修饰符</li><li><code>[&amp;a]</code>: 将a按引用进行传递</li><li><code>[a, &amp;b]</code>: 将a按值进行传递，b按引用进行传递</li><li><code>[=，&amp;a, &amp;b]</code>: 除a和b按引用进行传递外，其他参数都按值进行传递</li><li><code>[&amp;, a, b]</code>: 除a和b按值进行传递外，其他参数都按引用进行传递</li></ul><blockquote><p> Lambda是const函数, 内联展开, 没有实际地址</p></blockquote><h2 id="的用法"><a href="#的用法" class="headerlink" title="::*的用法"></a>::*的用法</h2><p>Clickhouse有这么的一个结构体</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Optimization</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">using</span> Function = <span class="built_in">size_t</span> (*)(QueryPlan::Node *, QueryPlan::Nodes &amp;);</span><br><span class="line">    <span class="type">const</span> Function apply = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> * name = <span class="string">&quot;&quot;</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">bool</span> QueryPlanOptimizationSettings::* <span class="type">const</span> is_enabled&#123;&#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><code>is_enabled</code> is a pointer to a member. It means that it points to an <code>bool</code> member variable that is declared in the class <code>QueryPlanOptimizationSettings</code>.</p><blockquote><p><a href="https://stackoverflow.com/questions/9939305/what-is-in-c">参考</a></p></blockquote><h2 id="类型关键字"><a href="#类型关键字" class="headerlink" title="类型关键字"></a>类型关键字</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">auto：自动类型推导,声明变量时必须赋初值。类型由右值的决定</span><br><span class="line">decltype :声明表达式类型，声明变量时时不必赋初值。类型由编译器根据表达式自动推导</span><br><span class="line">typeid：运行时类型信息（RTTi），不能用来声明变量</span><br></pre></td></tr></table></figure><p>用法</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">auto</span> b = a;</span><br><span class="line"><span class="keyword">decltype</span>(b) c;</span><br><span class="line"><span class="keyword">if</span> ((<span class="built_in">typeid</span>(a) == <span class="built_in">typeid</span>(b))</span><br><span class="line">&amp;&amp; (<span class="built_in">typeid</span>(a) == <span class="built_in">typeid</span>(c))</span><br><span class="line">&amp;&amp; (<span class="built_in">typeid</span>(b) == <span class="built_in">typeid</span>(c))) &#123; <span class="comment">//true</span></span><br><span class="line"></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;true&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="强制类型转化"><a href="#强制类型转化" class="headerlink" title="强制类型转化"></a>强制类型转化</h2><p>四种类型转化关键字</p><ol><li><p>static_cast:</p><p>任何编写程序时能够明确的类型转换都可以使用static_cast,由于不提供运行时的检查,需要在编写程序时确认转换的安全性。</p></li><li><p>dynamic_cast</p><p>dynamic_cast会在运行时检查类型转换是否合法，具有一定的安全性;上行转换和static_cast没有区别，都是安全的；下行转换时，dynamic_cast会检查转换的类型，相比static_cast更安全</p></li><li><p>const_cast</p><p>常量指针被转换成非常量指针，并且仍然指向原来的对象；常量引用被转换成非常量引，并且仍然引用原来的对象。</p></li><li><p>reinterpret_cast<br> 非常激进的指针类型转换，在编译期完成，可以转换任何类型的指针，所以极不安全。非极端情况不要使用。</p></li></ol><p>智能指针无法使用上面的关键字, 有专门的用法:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">static_pointer_cast</span><br><span class="line">dynamic_pointer_cast</span><br><span class="line">const_pointer_cast </span><br></pre></td></tr></table></figure><h2 id="模板的显示实例化"><a href="#模板的显示实例化" class="headerlink" title="模板的显示实例化"></a>模板的显示实例化</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">swap</span><span class="params">(T &amp;a, T &amp;b )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  T temp;</span><br><span class="line">  temp = a;</span><br><span class="line">  a = b;</span><br><span class="line">  b = temp;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> <span class="type">void</span> <span class="built_in">swap</span>&lt;<span class="type">int</span>&gt;(<span class="type">int</span> &amp;a, <span class="type">int</span> &amp; b);  </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse技术分享: Projection调研</title>
      <link href="/2021/07/21/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-Projection%E8%B0%83%E7%A0%94/"/>
      <url>/2021/07/21/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-Projection%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>Clickhouse的查询性能是有目共睹的优秀, 但与之对应单个查询对于机器资源的消耗也是非常巨大的, 导致Clickhouse整体的QPS会比较低.</p><p>当用户需要提高QPS时, 往往会通过建立物化视图, 进预计算, 查询时直接走物化视图来进行加速.</p><p>但这种方案有两个缺点:</p><ol><li>实际上有多张表, 明细查询可能需要走底表, 聚合查询需要查物化视图, 用户管理起来会有一定麻烦程度</li><li>如果出现慢查询, 需要用户新建一张物化视图, 然后导入数据, 再通过上线变更的方式, 来规避, 整体流程过长</li></ol><p>针对以上问题, 业界的预聚合引擎, 类似麒麟都实现了SQL rewrite的功能, 来自动替换用户的查询SQL, 这样上面的问题就直接解决了.</p><p>Clickhouse并没有打算基于物化视图的SQL rewrite, 而是实现一个Projection的功能, 号称是DataPart-Level的物化视图</p><blockquote><p>Originated from Vertica</p><ul><li>Projections are collections of table columns,</li><li>Projections store data in a format that optimizes query execution</li></ul></blockquote><p>看一下这两个的对比, 这里的<code>Query Routing</code>就是我们需要的能力</p><div class="table-container"><table><thead><tr><th style="text-align:center"><strong>Feature</strong></th><th style="text-align:center"><strong>Materialized View</strong></th><th style="text-align:center"><strong>Projection</strong></th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center">Data Consistency</td><td style="text-align:center">NO</td><td style="text-align:center">YES</td><td style="text-align:center">物化视图需要Merge后保持一致</td></tr><tr><td style="text-align:center">Schema Consistency</td><td style="text-align:center">NO</td><td style="text-align:center">YES</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Query Routing</td><td style="text-align:center">NO</td><td style="text-align:center">YES</td><td style="text-align:center">就是SQL Rewrite</td></tr><tr><td style="text-align:center">Query Index Optimization</td><td style="text-align:center">NO</td><td style="text-align:center">YES</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Partial Materialization</td><td style="text-align:center">NO</td><td style="text-align:center">Yes (but not recommended)</td><td style="text-align:center">这个没必要</td></tr><tr><td style="text-align:center">Complex Queries</td><td style="text-align:center">YES</td><td style="text-align:center">No (May support ARRAY JOIN)</td><td style="text-align:center">Joins或者子查询</td></tr><tr><td style="text-align:center">Special Engines</td><td style="text-align:center">YES</td><td style="text-align:center">NO</td><td style="text-align:center">Projection不依赖引擎</td></tr></tbody></table></div><blockquote><p>物化视图在复杂查询上还是有比较大的优势的, 而Clickhouse对于复杂查询的支持很差, 因此Projection的能力已经满足Clickhouse的要求.</p></blockquote><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="建表时指定projection"><a href="#建表时指定projection" class="headerlink" title="建表时指定projection"></a>建表时指定projection</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">       name1 [type1] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr1] [compression_codec] [TTL expr1],</span><br><span class="line">       ...</span><br><span class="line">       PROJECTION projection_name_1 (<span class="keyword">SELECT</span> <span class="operator">&lt;</span><span class="keyword">COLUMN</span> LIST EXPR<span class="operator">&gt;</span> [<span class="keyword">GROUP</span> <span class="keyword">BY</span>] [<span class="keyword">ORDER</span> <span class="keyword">BY</span>]),</span><br><span class="line">       ...</span><br><span class="line">) ENGINE <span class="operator">=</span> MergeTree()</span><br></pre></td></tr></table></figure><p>其中<code>projection_name_1</code>就是Projection的名字, 目前语法只支持<code>GROUP BY</code>和<code>ORDER BY</code>, 且两者不能同时出现.</p><blockquote><p>不支持Join等其他更加复杂的表达式</p></blockquote><p>如果是<code>GROUP BY</code>的话, 底层存储会使用<code>AggregatedMergeTree</code></p><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210727112308287.png" alt="image-20210727112308287"></p><p>当创建一个物化视图的时候, 会在DataPart的目录下, 生成以ProjectionName命名的文件夹, 文件夹里面存储着Projection的数据</p><p>如上图的<code>tp1.proj</code>目录</p><h3 id="DDL语法"><a href="#DDL语法" class="headerlink" title="DDL语法"></a>DDL语法</h3><h4 id="添加Projection"><a href="#添加Projection" class="headerlink" title="添加Projection"></a>添加Projection</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [db.]<span class="keyword">table</span> <span class="keyword">ADD</span> PROJECTION name <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="operator">&lt;</span><span class="keyword">COLUMN</span> LIST EXPR<span class="operator">&gt;</span>  [<span class="keyword">GROUP</span> <span class="keyword">BY</span>] [<span class="keyword">ORDER</span> <span class="keyword">BY</span>];</span><br></pre></td></tr></table></figure><h4 id="删除Projection"><a href="#删除Projection" class="headerlink" title="删除Projection"></a>删除Projection</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [db.]<span class="keyword">table</span> <span class="keyword">DROP</span> PROJECTION name;</span><br></pre></td></tr></table></figure><h4 id="物化Projection"><a href="#物化Projection" class="headerlink" title="物化Projection"></a>物化Projection</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [db.]<span class="keyword">table</span> MATERIALIZE PROJECTION name [<span class="keyword">IN</span> <span class="keyword">PARTITION</span> partition_name];</span><br></pre></td></tr></table></figure><h4 id="删除Projection数据"><a href="#删除Projection数据" class="headerlink" title="删除Projection数据"></a>删除Projection数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> [db.]<span class="keyword">table</span> CLEAR PROJECTION name [<span class="keyword">IN</span> <span class="keyword">PARTITION</span> partition_name];</span><br></pre></td></tr></table></figure><h3 id="语句查询"><a href="#语句查询" class="headerlink" title="语句查询"></a>语句查询</h3><p>目前Projection依然是实验特性, 默认是关闭的, 需要通过配置项开启</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> allow_experimental_projection_optimization<span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>如果需要判断是否命中Projection, 那么可以设置以下配置, 如果没有使用Projection, 那么程序会直接抛出异常</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> force_optimize_projection<span class="operator">=</span><span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>后续就能正常使用Select语句进行查询</p><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><h4 id="预聚合"><a href="#预聚合" class="headerlink" title="预聚合"></a>预聚合</h4><p>创建一张基表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.part_local</span><br><span class="line">(</span><br><span class="line">    `P_PARTKEY` UInt32,</span><br><span class="line">    `P_NAME` String,</span><br><span class="line">    `P_MFGR` LowCardinality(String),</span><br><span class="line">    `P_CATEGORY` LowCardinality(String),</span><br><span class="line">    `P_BRAND` LowCardinality(String),</span><br><span class="line">    `P_COLOR` LowCardinality(String),</span><br><span class="line">    `P_TYPE` LowCardinality(String),</span><br><span class="line">    `P_SIZE` UInt8,</span><br><span class="line">    `P_CONTAINER` LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> P_PARTKEY;</span><br></pre></td></tr></table></figure><p>创建Projection</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 删除历史Projection</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> default.part_local <span class="keyword">drop</span> projection tp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建Projection</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> default.part_local</span><br><span class="line">    <span class="keyword">ADD</span> PROJECTION tp</span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">            P_BRAND,</span><br><span class="line">            P_CATEGORY,</span><br><span class="line">            <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">AS</span> c,</span><br><span class="line">            xxHash32(<span class="built_in">sum</span>(P_SIZE)) <span class="keyword">AS</span> s</span><br><span class="line">        <span class="keyword">GROUP</span> <span class="keyword">BY</span></span><br><span class="line">            P_BRAND,</span><br><span class="line">            P_CATEGORY</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 物化历史数据</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">TABLE</span> default.part_local MATERIALIZE PROJECTION tp;</span><br></pre></td></tr></table></figure><p>查询使用Projection</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    P_CATEGORY,</span><br><span class="line">    <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">AS</span> c,</span><br><span class="line">    <span class="built_in">sqrt</span>(<span class="built_in">sum</span>(P_SIZE)) <span class="keyword">AS</span> s</span><br><span class="line"><span class="keyword">FROM</span> default.part_local</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> P_CATEGORY</span><br></pre></td></tr></table></figure><p>当查询的依赖的列都在Projection时, 就会触发SQL Rewrite.</p><blockquote><p>另外一个值得注意的是, 对于这个列<code>xxHash32(sum(P_SIZE)) AS s</code>, Projection实际储的是<code>sum(P_SIZE)</code>, 而非<code>xxHash32(sum(P_SIZE))</code></p><p>因此在这个查询的Case中, 能够命中<code>sqrt(sum(P_SIZE))</code>查询语法. 如果查询<code>max(P_SIZE)</code>, 则无法命中Projection</p></blockquote><h4 id="排序键替换"><a href="#排序键替换" class="headerlink" title="排序键替换"></a>排序键替换</h4><p>另外一个常见的场景, 就是用户需要查询两类的索引, 而CK的主键索引类似组合索引, 遵循最左匹配原则.</p><p>举个例子, 有个表如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.part_local</span><br><span class="line">(</span><br><span class="line">    `P_PARTKEY` UInt32,</span><br><span class="line">    `P_NAME` String,</span><br><span class="line">    `P_MFGR` LowCardinality(String),</span><br><span class="line">    `P_CATEGORY` LowCardinality(String),</span><br><span class="line">    `P_BRAND` LowCardinality(String),</span><br><span class="line">    `P_COLOR` LowCardinality(String),</span><br><span class="line">    `P_TYPE` LowCardinality(String),</span><br><span class="line">    `P_SIZE` UInt8,</span><br><span class="line">    `P_CONTAINER` LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> P_PARTKEY;</span><br></pre></td></tr></table></figure><p>有两类常用的查询SQL</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> default.part_local <span class="keyword">where</span> P_PARTKEY <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> default.part_local <span class="keyword">where</span> P_BRAND <span class="operator">=</span> <span class="string">&#x27;XX&#x27;</span>;</span><br></pre></td></tr></table></figure><p>这时排序键只能满足<code>P_PARTKEY</code>的查询, 即使将排序键设置为<code>(P_PARTKEY,P_BRAND )</code> , 查询<code>where P_BRAND = &#39;XX&#39;</code>也无法命中索引.</p><p>先前的处理方式, 就只能建一个额外的物化视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span>  part_mv1 </span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> P_BRAND</span><br><span class="line"><span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span> default.part_local;</span><br></pre></td></tr></table></figure><p>查询<code>P_BRAND</code>需要指定物化视图</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> default.part_mv1 <span class="keyword">where</span> P_BRAND <span class="operator">=</span> <span class="string">&#x27;XX&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>Clickhouse实际上有二级索引的能力, 但是由于本身主键索引已经稀疏索引了, 二级索引实现为跳数索引, 匹配率就更加差了, 因此在高QPS情况下, 效果非常差</p><p>线上只有在低QPS场景, 才允许业务使用</p></blockquote><p>有了Projection后, 使用建一个Projection来解决</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> default.part_local <span class="keyword">drop</span> projection tp1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> default.part_local</span><br><span class="line">    <span class="keyword">ADD</span> PROJECTION tp1</span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">SELECT</span></span><br><span class="line">            P_PARTKEY,</span><br><span class="line">            P_NAME,</span><br><span class="line">            P_MFGR,</span><br><span class="line">            P_CATEGORY,</span><br><span class="line">            P_BRAND,</span><br><span class="line">            P_COLOR,</span><br><span class="line">            P_TYPE,</span><br><span class="line">            P_SIZE,</span><br><span class="line">            P_CONTAINER</span><br><span class="line">        <span class="keyword">ORDER</span> <span class="keyword">BY</span> P_BRAND</span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">TABLE</span> default.part_local MATERIALIZE PROJECTION tp1;</span><br></pre></td></tr></table></figure><p>然后查询<code>P_BRAND</code>就能命中主键索引</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> default.part_mv1 <span class="keyword">where</span> P_BRAND <span class="operator">=</span> <span class="string">&#x27;XX&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>这个版本的一个比较大的问题, 一旦开启Projection后,  按照<code>P_PARTKEY</code>查询依然会走Projection, 然后实际上查询原始表的效果最好</p></blockquote><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h2 id="附录-资料地址"><a href="#附录-资料地址" class="headerlink" title="附录: 资料地址"></a>附录: 资料地址</h2><div class="table-container"><table><thead><tr><th>名称</th><th>链接地址</th></tr></thead><tbody><tr><td>设计文档</td><td><a href="https://github.com/ClickHouse/ClickHouse/issues/14730">ISSUE地址</a></td></tr><tr><td>PullRequest</td><td><a href="https://github.com/ClickHouse/ClickHouse/pull/20202">PR地址</a></td></tr><tr><td>Meetup分享PPT</td><td><a href="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/doc/3.%20clickhouse%20meetup%202021-02-06%20%E9%83%91%E5%A4%A9%E7%A5%BA.pdf">链接地址</a></td></tr><tr><td>Meetup分享视频</td><td><a href="https://www.bilibili.com/video/BV1eo4y197wm">观看地址</a></td></tr><tr><td>物化视图Rewrite</td><td><a href="https://calcite.apache.org/docs/materialized_views.html">Calcite官网文档</a></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse技术分享: 分区裁剪</title>
      <link href="/2021/07/15/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E5%88%86%E5%8C%BA%E8%A3%81%E5%89%AA/"/>
      <url>/2021/07/15/Clickhouse%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB-%E5%88%86%E5%8C%BA%E8%A3%81%E5%89%AA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>本文代码截图, 基于2021-07-19的master版本</p></blockquote><h2 id="DataPart存储"><a href="#DataPart存储" class="headerlink" title="DataPart存储"></a>DataPart存储</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> default.lineorder_local</span><br><span class="line">(</span><br><span class="line">    `LO_ORDERKEY` UInt32,</span><br><span class="line">    `LO_LINENUMBER` UInt8,</span><br><span class="line">    `LO_CUSTKEY` UInt32,</span><br><span class="line">    `LO_PARTKEY` UInt32,</span><br><span class="line">    `LO_SUPPKEY` UInt32,</span><br><span class="line">    `LO_ORDERDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_ORDERPRIORITY` LowCardinality(String),</span><br><span class="line">    `LO_SHIPPRIORITY` UInt8,</span><br><span class="line">    `LO_QUANTITY` UInt8,</span><br><span class="line">    `LO_EXTENDEDPRICE` UInt32,</span><br><span class="line">    `LO_ORDTOTALPRICE` UInt32,</span><br><span class="line">    `LO_DISCOUNT` UInt8,</span><br><span class="line">    `LO_REVENUE` UInt32,</span><br><span class="line">    `LO_SUPPLYCOST` UInt32,</span><br><span class="line">    `LO_TAX` UInt8,</span><br><span class="line">    `LO_COMMITDATE` <span class="type">Date</span>,</span><br><span class="line">    `LO_SHIPMODE` LowCardinality(String)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYear(LO_ORDERDATE)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (LO_ORDERDATE, LO_ORDERKEY)</span><br><span class="line">SETTINGS index_granularity <span class="operator">=</span> <span class="number">8192</span>;</span><br></pre></td></tr></table></figure><p>选了Clickhouse社区官方的<a href="https://clickhouse.tech/docs/en/getting-started/example-datasets/star-schema/">SSB测试套</a>的<code>lineorder</code>表为例, 他以<code>toYear(LO_ORDERDATE)</code>为分区键, 此时插入一些数据, 在Clickhouse DataPart级别的目录下, 会出现以下文件</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719104902873.png" alt="image-20210719104902873"></p><p>其中跟分区相关的有两个:  <code>partition.dat</code>和<code>minmax_LO_ORDERDATE.idx</code></p><p><code>partition.dat</code>存储的是Partition的具体数值,  对应Clickhouse的源码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">MergeTreePartition</span></span><br><span class="line">&#123;</span><br><span class="line">    Row value;</span><br><span class="line">  <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">load</span><span class="params">(<span class="type">const</span> MergeTreeData &amp; storage, <span class="type">const</span> DiskPtr &amp; disk, <span class="type">const</span> String &amp; part_path)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">store</span><span class="params">(<span class="type">const</span> MergeTreeData &amp; storage, <span class="type">const</span> DiskPtr &amp; disk, <span class="type">const</span> String &amp; part_path, MergeTreeDataPartChecksums &amp; checksums)</span> <span class="type">const</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>就是我们在<code>system.parts</code>表中看到的<code>partition</code>数值, 其中<code>value</code>的类型是<code>Row</code>, 对应表达式计算后的结果.</p><p>其中<code>load</code>函数是读取<code>dat</code>文件的数值到<code>row</code>中, <code>store</code>是存储<code>row</code>数据写入到文件里.</p><p><code>minmax_LO_ORDERDATE.idx</code>存储的是对应字段<code>LO_ORDERDATE</code>的最大值和最小值范围</p><blockquote><p>注意是<code>LO_ORDERDATE</code>的值, 而非<code>toYear(LO_ORDERDATE)</code>的值</p></blockquote><p>对的代码在<code>IMergeTreeDataPart</code>的内部<code>struct</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">MinMaxIndex</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">/// A direct product of ranges for each key column. See Storages/MergeTree/KeyCondition.cpp for details.</span></span><br><span class="line">    std::vector&lt;Range&gt; hyperrectangle;</span><br><span class="line">    <span class="type">bool</span> initialized = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">MinMaxIndex</span>() = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/// For month-based partitioning.</span></span><br><span class="line">    <span class="built_in">MinMaxIndex</span>(DayNum min_date, DayNum max_date)</span><br><span class="line">        : <span class="built_in">hyperrectangle</span>(<span class="number">1</span>, <span class="built_in">Range</span>(min_date, <span class="literal">true</span>, max_date, <span class="literal">true</span>))</span><br><span class="line">        , <span class="built_in">initialized</span>(<span class="literal">true</span>)</span><br><span class="line">    &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">load</span><span class="params">(<span class="type">const</span> MergeTreeData &amp; data, <span class="type">const</span> DiskPtr &amp; disk_, <span class="type">const</span> String &amp; part_path)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">store</span><span class="params">(<span class="type">const</span> MergeTreeData &amp; data, <span class="type">const</span> DiskPtr &amp; disk_, <span class="type">const</span> String &amp; part_path, Checksums &amp; checksums)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">store</span><span class="params">(<span class="type">const</span> Names &amp; column_names, <span class="type">const</span> DataTypes &amp; data_types, <span class="type">const</span> DiskPtr &amp; disk_, <span class="type">const</span> String &amp; part_path, Checksums &amp; checksums)</span> <span class="type">const</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">update</span><span class="params">(<span class="type">const</span> Block &amp; block, <span class="type">const</span> Names &amp; column_names)</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">merge</span><span class="params">(<span class="type">const</span> MinMaxIndex &amp; other)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>与<code>partition.dat</code>类似, 他们都有<code>load</code>和<code>store</code>方法,  但是<code>MinMaxIndex</code>额外有<code>update</code>和<code>merge</code>, 因为DDL的SQL并不会改变<code>partition</code>, 但一个<code>Delete where</code>的语句有可能改变了上下界.</p><h3 id="举例总结"><a href="#举例总结" class="headerlink" title="举例总结"></a>举例总结</h3><p>举个例子来说, 如果一组数据, 只有3个值, 其中 <code>LO_ORDERDATE</code>列为: <code>1992-01-01</code>,<code>1992-06-02</code>和 <code>1992-12-01</code>, 那么<code>partition.dat</code>存储的值就是<code>toYear(LO_ORDERDATE)</code>的数值, 也就是<code>1992</code>; 而<code>minmax_LO_ORDERDATE.idx</code>存储的是<code>1992-01-01</code>和<code>1992-12-01</code>, 表示这个区间的上下确界.</p><h2 id="数据查询"><a href="#数据查询" class="headerlink" title="数据查询"></a>数据查询</h2><h3 id="单调函数"><a href="#单调函数" class="headerlink" title="单调函数"></a>单调函数</h3><p>下图是一个比较典型的单调递增函数:  </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210715164527310.png" alt="image-20210715164527310"></p><script type="math/tex; mode=display">y=2/x^2-2</script><p>下图是一个典型的非单调递增函数</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210715164955108.png" alt="image-20210715164955108"></p><script type="math/tex; mode=display">y=0.2x^3-1.5x^2+2x+4</script><p>单调函数的特点, x的最大最小值, 必然是y的最大最小值(单调递增).</p><p>所以很容易想到, 如果Clickhouse某个函数是单调的, 那么能通过DataPart上的minmax索引, 来过滤整个DataPart. </p><p><strong>CK的整个分区裁剪实际上是在分区column上的minmax索引, 而非字面理解的分区裁剪</strong></p><h3 id="Clickhouse函数"><a href="#Clickhouse函数" class="headerlink" title="Clickhouse函数"></a>Clickhouse函数</h3><p> 在Clickhouse的<code>IFunction.h</code>文件里面, 有两个关于单调性的CK函数接口: <code>hasInformationAboutMonotonicity</code>和<code>getMonotonicityForRange</code></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719141057728.png" alt="image-20210719141057728"></p><p>其中实现这些函数的, <strong>基本上</strong>都可以实现分区裁剪功能, 没实现的不能实现分区裁剪.</p><blockquote><p>Clickhouse Function接口在21年经过重构, 有一部分函数实现新的接口, 一部分实现老接口</p></blockquote><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719141707904.png" alt="image-20210719141707904"></p><p>实现IFunctionBase的函数</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719141638424.png" alt="image-20210719141638424"></p><p>实现IFunction的函数</p><h3 id="DataPart筛选"><a href="#DataPart筛选" class="headerlink" title="DataPart筛选"></a>DataPart筛选</h3><p><code>MergeTree</code>引擎的数据筛选的代码封装在<code>MergeTreeDataSelectExecutor</code>类中</p><p>首先, 根据分区键和查询语句, 构造<code>KeyCondition</code></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719143828417.png" alt="image-20210719143828417"></p><p>然后, 遍历每个DataPart过滤不符合条件的, 保留能够命中<code>minmax</code>索引的DP</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719144211941.png" alt="image-20210719144211941"></p><p>在方法<code>checkInHyperrectangle</code>判断函数链是不是都是单调, 例如<code>toYear(toDate(&#39;2010-10-10&#39;))</code>这类两个函数复合的表达式, 两个都是单调函数, 因此单调链是有效的.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719145332621.png" alt="image-20210719145332621"></p><h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h3><p>上文提到实现<code>hasInformationAboutMonotonicity</code>基本上能够分区裁剪, 之所以说<strong>基本上</strong>, 原因在于还有一些例外.</p><p>回到上面的<code>KeyCondition</code>, 在整个构造过程中, 有一个非常重要的函数<code>isKeyPossiblyWrappedByMonotonicFunctionsImpl</code>来判断是否单调.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210719150454648.png" alt="image-20210719150454648"></p><p>从代码上看到, 只有<strong>参数个数是1或者2个</strong>才能命中; 有<strong>2个参数的, 其中一个需要是常量表达式.</strong></p><p>因此类似<code>date_trunc(unit, value[, timezone])</code>这函数也是单调的, 但是因为有3个参数, 因此也是无法命中索引.</p><blockquote><p>这个是代码实现问题, 只是3个参数的函数非常少, 没啥动力去实现.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse运维增强: 存算分离</title>
      <link href="/2021/07/14/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB/"/>
      <url>/2021/07/14/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-%E5%AD%98%E7%AE%97%E5%88%86%E7%A6%BB/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="什么是存算分离"><a href="#什么是存算分离" class="headerlink" title="什么是存算分离"></a>什么是存算分离</h2><p>从字面意思理解就是, 存储计算是解耦的,  当需要增加存储资源的时候, 不需要对应的增加计算部分的资源.</p><p>Clickhouse的架构是典型的存储计算在一起的方式, 现在我们公司CK的算力紧缺, 存储却相对较低, 可是要扩容的时候, 存储也对应的增加起来, 形成了一定浪费.</p><p>存算分离架构的流行主要得益于云服务的兴起, 按需和弹性的资源推向了企业用户, 目前大多数的云上数据库都开始支持存算分离架构.</p><blockquote><p>传统公司的服务器一般采用预算制, 部门一般能申请就会尽量去申请, 高峰时期能有充足的资源预备是他们首要的目标, 低峰期间的资源浪费是可以忍受的.</p></blockquote><p>云上存算分离架构的设计目标有如下几个:</p><ol><li><p>数据一般存储在统一的数据服务中, 且数据服务不绑定单个服务; 典型的做法就是放到S3型存储服务离</p></li><li><p>计算能够弹性, 扩缩容时间尽量短(某些服务是不支持缩容的);机型大多数要求同构, 少部分支持异构</p></li><li><p>网络带宽不是首要考虑的点,  云上内网带宽能够足够的大</p></li><li><p>性能衰减不能太严重, 某些场景需要优于单机水平, 方便对外宣传</p></li></ol><p>上面4个设计点, 1和2比较好解决, 例如<strong>Spark + HDFS构成的OLAP服务</strong>是一个典型的存算分离架构,  Spark和HDFS可以分布在不同的集群上, 计算和存储的扩缩容都可以分开处理.</p><p>但第3点在传统公司是难以解决的,  因为机房的带宽非常有限, 一旦跨机房, 性能将直线下降. 所以会将这两个服务部署在一起, 导致<strong>部署上依然是存算不分离</strong>的. </p><p>因此存算分离的一个最大的难点实际上被<strong>硬件解决</strong>了</p><p>所以云服务大多数的设计点在解决第4个问题, 而采用的方案就是<strong>缓存</strong>.</p><p>学过计算机的都知道<strong>介质的访问速度</strong>:</p><p>内存 &gt; SSD &gt; HHD &gt;  远程HDD</p><p>在云服务世界里就是:</p><p>服务器内存 &gt; 本地高速盘 &gt; 本地普通盘 &gt; S3服务</p><p>S3服务作为云服务里面, 最按需, 最弹性, 最Serverless接口的存储服务, 毫无疑问的被大多数服务选中作为最终的数据存储地方.</p><p>因此针对S3的访问加速, 也分为前面三种介质模式, 我分别举一个例子来介绍一下.</p><blockquote><p>基于之前的经验, 最新版不一定正确</p></blockquote><h2 id="缓存加速"><a href="#缓存加速" class="headerlink" title="缓存加速"></a>缓存加速</h2><h3 id="HDD类型"><a href="#HDD类型" class="headerlink" title="HDD类型"></a>HDD类型</h3><p>这种类型比较少见, 因为HDD的性能已经很差了, 相对于S3提升能力不多, 唯一优势是放在同一网络域中, 网络带宽会略会好.  </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/2020%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8A%E4%BA%91cn-06.svg" alt="img"></p><p>例如MRS服务, 就是云上的HDFS+Spark/Hive这套大数据组件,  用户结构数据有一些部分会上传到S3上, 这时使用Spark直接分析OBS数据的性能会非常差,  这时MRS服务做了一个数据导入的功能, 将S3的数据导入到MRS集群的HDFS上, 这样用Spark查询时,性能会提升一部分. 但是由于HDD糟糕的性能, 提升效果有限, 因此会加入<code>CarbonData</code>等有索引的数据结构查询.</p><p>另外随着S3Query的能力开放, 直接在S3上查询的能力也开始发力, 后续通过HDD的加速大概率会消失.</p><p>云上的HDFS最多是为了兼容企业的老接口, 一旦S3能全面兼容HDFS, MRS这种内建HDFS的方案将被取消.</p><p>目前看到快手是直接用<a href="https://www.infoq.cn/article/vGabIOdeUM87hv6X8qlL">基于HDD的HDFS做存算分离方案</a>的</p><h3 id="SSD加速"><a href="#SSD加速" class="headerlink" title="SSD加速"></a>SSD加速</h3><p>SSD是一种永久性的存储介质, 不会丢失数据, 因此经常会用到实时场景做缓存.</p><p>美团就有使用<a href="https://tech.meituan.com/2021/01/14/kafka-ssd.html">SSD加速Kafka</a>案例</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/2482fc361c82e1d35684394f3b34f1e5184835.png" alt="img"></p><p>相比于普通磁盘, SSD的性能有成倍的提升, 且成本增加较小.</p><p>但SSD也有自身的缺点:</p><ul><li>相对于普通, 磁盘容量比较小, 磁盘寿命降低</li><li>相对于内存, 性能查实差了1个数量级</li></ul><h3 id="内存加速"><a href="#内存加速" class="headerlink" title="内存加速"></a>内存加速</h3><p>内存加速的案例太多了, <code>Redis</code>和<code>MemCache</code>就是在业务里经常用到的内存加速. </p><p>源码级别也有各种类型的缓存库, 例如<code>guawa</code>的cache, 具体案例就不介绍了</p><p>内存缓存的缺点是:</p><ul><li>内存相对比较小,  需要小心控制一下内存的换入换出</li><li>内存相对来说比较昂贵, 只能应用局部数据量的查询</li></ul><h3 id="方案比较"><a href="#方案比较" class="headerlink" title="方案比较"></a>方案比较</h3><div class="table-container"><table><thead><tr><th style="text-align:center">缓存方案</th><th style="text-align:center">优点</th><th style="text-align:center">缺点</th><th style="text-align:center">数据量</th><th style="text-align:center">时延</th><th style="text-align:center">适用场景(OLAP)</th></tr></thead><tbody><tr><td style="text-align:center">磁盘缓存</td><td style="text-align:center">磁盘容量大</td><td style="text-align:center">性能差</td><td style="text-align:center">10PB</td><td style="text-align:center">分钟级</td><td style="text-align:center">离线加速</td></tr><tr><td style="text-align:center">SSD缓存</td><td style="text-align:center">磁盘存储性价比高</td><td style="text-align:center">存储量相对小</td><td style="text-align:center">100TB</td><td style="text-align:center">&lt;5秒</td><td style="text-align:center">实时查询</td></tr><tr><td style="text-align:center">内存缓存</td><td style="text-align:center">性能快</td><td style="text-align:center">数据易失<br/>成本高</td><td style="text-align:center">&lt;1TB</td><td style="text-align:center">&lt;1秒</td><td style="text-align:center">数据服务</td></tr></tbody></table></div><p>从目前Clickhouse的场景来看, SSD的缓存策略也许更加适合我们的要求.</p><h2 id="Clickhouse的存算分离方案"><a href="#Clickhouse的存算分离方案" class="headerlink" title="Clickhouse的存算分离方案"></a>Clickhouse的存算分离方案</h2><h3 id="设计要点"><a href="#设计要点" class="headerlink" title="设计要点"></a>设计要点</h3><p>首先明确一下设计的指标, 对照着之前的设计要点</p><div class="table-container"><table><thead><tr><th style="text-align:center">设计点</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">存储</td><td style="text-align:center">存储服务其实就那么几种, 如果在云下, 那么就使用HDFS, 如果在云上就使用S3服务.另外考虑到存储的性能, 可能会购买JuiceFS做存储的性能加速</td></tr><tr><td style="text-align:center">计算</td><td style="text-align:center">不准备支持异构机器, 但集群之间可以异构, 分为存储性和计算型集群; 计算弹性要求扩缩容的能够在5分钟处理完毕</td></tr><tr><td style="text-align:center">网络</td><td style="text-align:center">这个点能做的非常少, 有什么就用什么</td></tr><tr><td style="text-align:center">缓存</td><td style="text-align:center">采用SSD缓存, 将数据文件放置于计算机器的SSD中, 存储只是作为同步方案</td></tr></tbody></table></div><h3 id="设计图"><a href="#设计图" class="headerlink" title="设计图"></a>设计图</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210715142722756.png" alt="image-20210715142722756"></p><p>跟之前虚拟id的方式基本上一致,  但存储会统一放入S3中, 机器上会有一个同步备份, 另外会有一个manager管理这些数据, S3和HDFS更像一个数据备份和快速恢复的地方.</p><blockquote><p>之前Manager是游离整体架构之外的辅助角色, 现在是强相关组件, 因此写入了架构图</p></blockquote><ul><li><strong>数据写入</strong> <ul><li>实时数据, 会写入Clickhouse节点, 数据同MergeTree引擎同步到S3上</li><li>离线数据, 会直写拷贝到S3上, 由Manager将数据块Attach对对应的数据节点</li></ul></li><li><p><strong>数据读取</strong> </p><ul><li>读取CK节点上, 缓存的数据, 数据以CK上的为准</li><li>CK依然有副本节点, 用于加数读取</li></ul></li><li><p><strong>数据变更</strong></p><ul><li>由CK节点处理请求, 并CK管理HDFS上的DP</li></ul></li><li><p><strong>数据管理</strong></p><ul><li>小文件问题: HDFS上的一个DP文件夹会序列化为单个文件, 数据上传时序列化, 数据下载(attach时)反序列化为文件夹</li><li>元数据: 目前还不打算弄成统一的元数据系统, 由manager来管理员数据, 扩容机器的时候, manager先初始化所有的表, 那么这个时候, 就要<strong>禁止用户DDL操作</strong></li></ul></li></ul><h3 id="节点扩容"><a href="#节点扩容" class="headerlink" title="节点扩容"></a>节点扩容</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210715144933939.png" alt="image-20210715144933939"></p><p>依然和虚拟shard的方案很类似, 只是从S3地方完成了拷贝, 具体就不展开了.</p><h2 id="方案评价"><a href="#方案评价" class="headerlink" title="方案评价"></a>方案评价</h2><p>目前看来这个方案是可行的, 整体对Clickhouse引擎内部的改动也不多, 也决绝了运维最大的难题就是扩容的方案.</p><p>但是由于CK的配置和元数据都是单机式的, 更新集群配置需要重启节点, 这就会限制该方案云化的设计.</p><blockquote><p>细节设计还有一定的模糊, 主要是对CK引擎内部还没有那么清晰.</p></blockquote><p>下一步如果能够实现元数据和集群配置的集中化, 那么对自动化会有很大的优势.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse运维增强: 虚拟shard</title>
      <link href="/2021/07/13/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-%E8%99%9A%E6%8B%9Fshard/"/>
      <url>/2021/07/13/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-%E8%99%9A%E6%8B%9Fshard/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>书接上文, 上一篇文章提到<code>reHash</code>方案的缺点在于: <code>停写</code>和<code>数据拷贝</code>.</p><p>那么能不能通过<code>数据迁移</code>而不是<code>数据拷贝</code>的方式来处理<code>reHash</code>呢?</p><p>这个时候, 就需要用到<a href="https://zhuanlan.zhihu.com/p/98030096">一致性Hash算法</a></p><h2 id="jumpConsistentHash算法"><a href="#jumpConsistentHash算法" class="headerlink" title="jumpConsistentHash算法"></a>jumpConsistentHash算法</h2><p><a href="https://arxiv.org/abs/1406.2294">jumpConsistentHash</a>是一个一致性hash函数, Clickhouse本身已经实现该<a href="https://clickhouse.tech/docs/en/sql-reference/functions/hash-functions/#jumpconsistenthash">算法</a></p><p>假设value的取值范围是[0,1023), 如果shard的个数为3, 那么表达式<code>jumpConsistentHash(value, 3)</code>, 能够将1024个字符, 均衡的分配到3个桶内.</p><p>而如果此时shard个数升级为4, 我们再次计算表达式<code>jumpConsistentHash(value, 4)</code>,  就按照4个节点分区, 而一致性hash算法能够保证, <strong>只会抽取前面3个节点的某些值, 放入第四个节点, 而不是在前3个节点之间, 搞数据交换.</strong></p><p>就给我们不停服切换提供了能力.</p><h2 id="虚拟节点"><a href="#虚拟节点" class="headerlink" title="虚拟节点"></a>虚拟节点</h2><p>虚拟节点是一致性hash经常使用一个技术, 方便迁移时只移动某些数据.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210713184924276.png" alt="image-20210713184924276"></p><p>如果所示, 整个集群有3个真实的shard, 9个虚拟的shard.</p><p>分布式写入时, 分布式表会将数据按照9份分shard, 但是[0,2,7,8]编号的数据, 实际会发送到第一个节点上.</p><p>在存储层面, DataPart级别有明确的shard编号0, <strong>一个dataPart的数据绝对从属于一个shard</strong></p><p>这样, 迁移的时候, 只需要做到按照DataPart级别迁移就行.</p><h3 id="迁移过程"><a href="#迁移过程" class="headerlink" title="迁移过程"></a>迁移过程</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210713185412980.png" alt="image-20210713185412980"></p><p>按照一致性hash的算法, 我们了解到DP7和DP6会迁移到新的shard之中, 这时整个变更的流程为:</p><ol><li>上线新的shard</li><li>更新老的shard的配置, 配置时注明新加入的shard, 跟之前类似, 写入hash依然按照3处理</li><li>老的shard计算出自己要拷贝的shard编号(也就是6和7), 然后开始给新shard推送老的DP数据</li><li>老shard明确所有DP7数据已经推送到新的shard, 且自己的分布式表无数据积压, 然后将自己的分布式查询路由到新shard, 此时对于该shard来说读写为4节点, 而其他节点有可能为3节点, 因此实际上新加上的shard, 如果有数据写入, 也需要推送到对应的老shard中. 这步骤是为了做到不停写扩容, 如果可以停写的话, 只需要等待无积压即可切换路由.</li><li>当所有节点都是4节点配置时, 关闭数据同步的能力, 然后放开新节点的写入或者查询</li><li>最后再更新一下配置文件即可</li></ol><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210713190606189.png" alt="image-20210713190606189"></p><p>整个方案的难点在于实现<strong>数据同步的能力</strong>, 类似于ReplicaMergeTree的方式.</p><h3 id="方案评估"><a href="#方案评估" class="headerlink" title="方案评估"></a>方案评估</h3><p>这个方案对于之前的提升有:</p><ol><li>数据不需要拷贝了, 只需要迁移一部分数据</li><li>真实切换的时候, 可以做到近乎不停写</li></ol><p>但这个方案也会带来一个比较大的问题: <strong>shard变多, DataPart个数变多, 可能对集群节点的稳定性带来较大的影响</strong></p><ul><li>Shard变多, 主要风险来自于目前分布式表的实现</li><li>DataPart变多, 风险来自于磁盘读取方面的性能</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clickhouse运维增强: ReHash实现</title>
      <link href="/2021/07/12/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-ReHash%E5%AE%9E%E7%8E%B0/"/>
      <url>/2021/07/12/Clickhouse%E8%BF%90%E7%BB%B4%E5%A2%9E%E5%BC%BA-ReHash%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>转到Clickhouse已经3个月, 在了解CK原理和公司内部使用后发现:</p><ul><li>用户在功能端的需求不是很强烈,  除了<strong>实时去重</strong>和<strong>String类型求UV</strong>之外,  开源社区的功能基本满足了用户的所有的述求</li><li>运维方面问题不断,  除了计算隔离等Server类型常见的运维问题, 还有一些DDL一致性问题都通过治理手段处理了, 但是对于集群扩容目前却没啥特别好的方案</li></ul><p>这篇博客将来讨论一下CK扩容的难点以及应对方案, 由于这些方案还没有被组内采纳, 因此可以外网记录一下思考的成果.</p><p>扩容问题的讨论, 将分为3篇文章:</p><ol><li>第一篇文章, 将探讨一下<code>reHash</code>问题, 这是扩容最大的难点, 在该文章将提供一个简单的处理方案</li><li>第二篇文章, 将探讨一下<code>虚拟shard</code>的实现, 用以解决第一个方案的缺点</li><li>第三篇文章, 将探讨一下<code>存储计算分离</code>的架构, 实现简便的容错式的扩缩容方案.</li></ol><h2 id="数据视图"><a href="#数据视图" class="headerlink" title="数据视图"></a>数据视图</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210712151634570.png" alt="image-20210712151634570"></p><p>Clickhouse目前是按照机器节点进行<strong>物理隔离</strong>的,  一个集群对应一个专属的业务部门, 而非整一个大的集群给所有业务方使用.</p><p>无论离线还是离线的数据, 都会写入<strong>底表数据</strong>, 底表是一张分布式表,  通过<code>rand</code>方式分区到CK的本地表, 由于底表数据是<strong>随机分区</strong>, 数据插入时, 可以直接写入本地表, 而不用写一遍分布式表, 减少了数据传输消耗.</p><p>一部分用户, 就会直接在<strong>底表数据</strong>上做数据查询, 但也有一部分需要用到Clickhouse比较复杂的引擎, 例如<code>ReplacingMergeTree</code>或者<code>AggressivingMergeTree</code>等, 用以实现聚合运算或者去重计算.</p><p>这个时候就会使用<strong>物化视图</strong>, 注意<strong>物化视图</strong>是构建在<strong>底表数据</strong>之上的, 在这里面数据实际上复制了2份, 但正是由于这个重复, 让集群的整体容错能力大大加强, 即使要物化视图有问题, 依然可以通过重建的方式解决. <strong>推荐这么使用</strong></p><blockquote><p>但是这么做, 也有一个问题, 就是异常时数据重复, 直接写底表的, 会让相同的block写入不同的shard, 无法规避底表重复的问题.</p></blockquote><p><strong>物化视图</strong>是底层也是一张分布式表, 视图更像一个触发器, 将insert的数据写入到物化视图的数据表上. 由于物化视图的引擎多为<code>ReplacingMergeTree</code>和<code>AggressivingMergeTree</code>, 因此必须将相同Key的数据, 分布到同一个shard中, 这时就需要做数据分片.</p><p>常规做<strong>数据分片</strong>, 一般采用range分区, 例如HBase或者TiDB, 原因在于Range分区比较方面实现数据的Merge或者Spilt,  但range分区必须要求数据可排序,  对数据的组织形式有着严格的要求, Hash分区实现起来相对简单, 计算效率也高. </p><p>但Hash分区, 一旦遇到扩容, 就需要完成数据的重分区, 成本非常高. </p><p>Clickhouse采用的是实现相对简单的Hash分区, 从而导致扩容非常复杂, 具体有多复杂, 看下面的分析.</p><h2 id="底表扩容"><a href="#底表扩容" class="headerlink" title="底表扩容"></a>底表扩容</h2><p>底表由于是rand分区, 因此几乎不会有任何的成本和风险, 只需要将新的shard配置项, 加入到Clickhouse集群的配置就行.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210713155425389.png" alt="image-20210713155425389"></p><p>具体步骤:</p><ol><li>用4节点配置, 启动节点4, 接入集群</li><li>1~3节点更新4节点的配置</li><li>更新节点配置为4个节点, 开始4节点写入</li></ol><p>以上步骤可以做到不停写不停读</p><div class="table-container"><table><thead><tr><th>步骤</th><th>数据读取</th><th>数据写入</th></tr></thead><tbody><tr><td>步骤1</td><td>如果查询1~3, 读取3节点配置;如果查询节点4, 读取4节点;由于4节点无数据, 则无影响</td><td>3节点写入</td></tr><tr><td>步骤2</td><td>1~4都为4节点配置, 由于4节点无数据, 则无影响</td><td>3节点写入</td></tr><tr><td>步骤3</td><td>1~4都为4节点配置, 由于4节点无数据, 则无影响</td><td>4节点写入</td></tr></tbody></table></div><h3 id="数据均衡"><a href="#数据均衡" class="headerlink" title="数据均衡"></a>数据均衡</h3><p>节点4刚上线, 数据的查询, 大部分依然还会走到远1~3的节点, 因此需要做数据均衡的操作.</p><p>如果系统的负载降低, 可以根据数据TTL过期时间, 将老的非均衡的数据淘汰后, 新的数据将是均衡的.</p><p>如果系统负载已经很高了, 那就必须手动实现均衡操作,  可以通过<code>Fetch Partition</code>将数据从老节点拉去过来, 然后再<code>detach partition</code>将老集群的数据, 设置为失效状态. </p><p>不过这种方式会造成, 迁移过程中,<strong>数据查询重复</strong>,  需要放在低峰时期操作均衡.</p><h2 id="物化视图扩容"><a href="#物化视图扩容" class="headerlink" title="物化视图扩容"></a>物化视图扩容</h2><p>上面已经提到了, 物化视图的扩容难度, 主要是因为rehash的需求, 后面就看一下具体的步骤.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/img/image-20210713161038288.png" alt="image-20210713161038288"></p><p>整体的方案, 就是创建一个临时视图, 灌入数据, 然后重命名, 与mysql修改主键的流程类似, 但其中要做到 <strong>尽量少的停读写</strong>和<strong>尽量多的自动化</strong>以及<strong>尽可能的数据正确</strong></p><ol><li>原始状态有3个shard, 物化视图通过Hash键分区, 写入到shard中.</li><li>升级至4节点, 创建一个同结构的MV’, MV’数据hash到4个节点; 而此时MV依然写入到3个节点<ol><li>和<strong>底表扩容</strong>一样, 先4节点配置启动节点4, 然后再更新1~3的节点</li><li>新的shard, 有一个特殊的配置值, 名为status, 值为new;  MV读取到这类配置项, 会自动忽略这类shard, 因此对于MV来说, 尽管已经4节点配置, 但它自己依然是3节点</li><li>创建MV’时, 指定配置项include_state_shard=true, 新MV将hash到4个节点; 另外创建视图指定数据初始化能力, 这样就能需要不停服的回追底表数据了.</li></ol></li><li>MV’消费底表的历史数据, 等历史消费完毕后, 开始将MV’重命名为MV(MV则删除)<ol><li><strong>停止底表写入</strong>, 这个步骤是为了防止在rename阶段, 分布式表上有数据积压, 因此必须停写清空积压数据, 这个停写时间能够控制在分钟级别, 对于业务方的影响还算可控范围</li><li>MV 重命名为MV-Temp, 由于rename操作是一个元数据操作, 因此执行速度比较快<ol><li>删除物化视图转化器</li><li>重命名数据表的本地表和分布式表</li></ol></li><li>MV’ 重命名为MV, <ol><li>删除物化视图转化器</li><li>重命名数据表的本地表和分布式表</li><li>重建物化视图转化器(名字不一样了)</li></ol></li><li>所有MV都切换后, 更新shard配置项, 去除status关键字</li><li>修改MV的配置项, 删除include_state_shard配置项.</li></ol></li></ol><h3 id="方案点评"><a href="#方案点评" class="headerlink" title="方案点评"></a>方案点评</h3><p>这个方案依赖两个实现:</p><ol><li>分布式表根据不同配置项, 识别不同的shard的</li><li>物化视图不停写初始化构建</li></ol><p>方案的优点:</p><ol><li>能够实现扩shard</li><li>基本上可以实现自动化</li></ol><p>方案的缺点为:</p><ol><li>需要停读写, 虽然时间不长</li><li>需要重写一遍底表, 数据浪费严重.</li></ol><h2 id="读写优化"><a href="#读写优化" class="headerlink" title="读写优化"></a>读写优化</h2><p>目前物化视图的写入都是由Flink写入的, 而Flink有非常好的容错能力, 可以人为的让写入失败重试保证数据的正确, 而我们要确保的某一段时间, 数据无法写入到底表.</p><p>可以实现一个<strong>alter table语法</strong>, 在内存的<code>metadata</code>标识某个底表无法被写入, 这样物化视图也就无法更新了.</p><p>该标识是一个内存状态, 不需要持久化. 另外也需要有取消的指令.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> xxx disable write;</span><br><span class="line"><span class="comment">-- replace table</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> xxx enable write</span><br></pre></td></tr></table></figure><p>写入时, 发现表的状态为<code>disabled_write</code>就直接返回错误状态.</p><blockquote><p>或者直接用权限, 将用户的权限取消, 等rename完成后, 再开放写入; 但公司内部用了大账号的方式, 可能不太好实现.</p></blockquote><p>另外一个需要注意的点, 尽量要将禁写时间控制在Flink Sink算子的重试时间内, 不然会出现APP重启, 出现不量的告警.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]MPI-Operator介绍</title>
      <link href="/2019/09/22/Kubeflow%E7%B3%BB%E5%88%97-MPI-Operator%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/09/22/Kubeflow%E7%B3%BB%E5%88%97-MPI-Operator%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>系统里面默认是没有安装<code>mpi-operator</code>, 因此需要自行安装.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubeflow/mpi-operator/blob/master/deploy/mpi-operator.yaml</span><br><span class="line"><span class="comment"># 修改镜像地址</span></span><br><span class="line">kubectl create -f mpi-operator.yaml</span><br></pre></td></tr></table></figure></p><p>在国内永远有镜像替换的烦恼, 在mpi-operator之中需要自行替换这两个镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpioperator/mpi-operator:latest</span><br><span class="line">mpioperator/kubectl-delivery:latest</span><br></pre></td></tr></table></figure><p>你可以直接在<a href="https://hub.docker.com/u/mpioperator">DockerHub</a>上找到这两个镜像或者自行编译: <a href="https://github.com/kubeflow/mpi-operator/blob/master/Dockerfile">operator地址</a> <a href="https://github.com/kubeflow/mpi-operator/blob/master/cmd/kubectl-delivery/Dockerfile">delivery地址</a></p><p>根据以下命令查看是否安装成功(crd已经创建 &amp;&amp; pod为running)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get crd|grep mpi</span></span><br><span class="line">mpijobs.kubeflow.org                   2019-09-18T07:44:48Z</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl get pod -n mpi-operator|grep mpi</span></span><br><span class="line">mpi-operator-584466c4f6-frw4x          1/1       Running     1          3d</span><br></pre></td></tr></table></figure><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p>根据官网的介绍创建<code>tensorflow-benchmarks</code>的例子.</p><p> <a href="https://github.com/kubeflow/mpi-operator/blob/master/examples/v1alpha2/tensorflow-benchmarks.yaml">文件地址</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeflow.org/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MPIJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tensorflow-benchmarks</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">slotsPerWorker:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">cleanPodPolicy:</span> <span class="string">Running</span></span><br><span class="line">  <span class="attr">mpiReplicaSpecs:</span></span><br><span class="line">    <span class="attr">Launcher:</span></span><br><span class="line">      <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">         <span class="attr">spec:</span></span><br><span class="line">           <span class="attr">containers:</span></span><br><span class="line">           <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">mpioperator/tensorflow-benchmarks:latest</span></span><br><span class="line">             <span class="attr">name:</span> <span class="string">tensorflow-benchmarks</span></span><br><span class="line">             <span class="attr">command:</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">mpirun</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">--allow-run-as-root</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-np</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-bind-to</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">none</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-map-by</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">slot</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-x</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">NCCL_DEBUG=INFO</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-x</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">LD_LIBRARY_PATH</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-x</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">PATH</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-mca</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">pml</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">ob1</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">-mca</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">btl</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">^openib</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">python</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">--model=resnet101</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">--batch_size=64</span></span><br><span class="line">             <span class="bullet">-</span> <span class="string">--variable_update=horovod</span></span><br><span class="line">    <span class="attr">Worker:</span></span><br><span class="line">      <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">mpioperator/tensorflow-benchmarks:latest</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">tensorflow-benchmarks</span></span><br><span class="line">            <span class="attr">resources:</span></span><br><span class="line">              <span class="attr">limits:</span></span><br><span class="line">                <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>在这里定义了一个<code>Launcher</code>,2个<code>Worker</code>, Work使用GPU, 因此在<code>Launcher</code>的<code>mpirun</code>参数之中<code>-np</code>必须为2, 说明是使用了两个并发.</p><p>等待两个Worker启动之后, Launcher开始刷日志:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get pod -n mpi-operator</span></span><br><span class="line">NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">mpi-operator-584466c4f6-frw4x          1/1       Running   1          3d</span><br><span class="line">tensorflow-benchmarks-launcher-vsq8b   1/1       Running   0          40s</span><br><span class="line">tensorflow-benchmarks-worker-0         1/1       Running   0          40s</span><br><span class="line">tensorflow-benchmarks-worker-1         1/1       Running   0          40s</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubectl logs -f tensorflow-benchmarks-launcher-vsq8b -n mpi-operator</span></span><br><span class="line">Done warm up</span><br><span class="line">Step    Img/sec total_loss</span><br><span class="line">Done warm up</span><br><span class="line">Step    Img/sec total_loss</span><br><span class="line">1       images/sec: 109.6 +/- 0.0 (jitter = 0.0)        9.181</span><br><span class="line">1       images/sec: 109.9 +/- 0.0 (jitter = 0.0)        9.110</span><br><span class="line">10      images/sec: 108.1 +/- 0.7 (jitter = 1.3)        8.864</span><br><span class="line">10      images/sec: 108.1 +/- 0.7 (jitter = 1.5)        9.184</span><br><span class="line">20      images/sec: 107.9 +/- 0.8 (jitter = 1.1)        9.246</span><br><span class="line">20      images/sec: 107.9 +/- 0.8 (jitter = 1.2)        9.073</span><br><span class="line">30      images/sec: 107.7 +/- 0.6 (jitter = 1.5)        9.147</span><br><span class="line">30      images/sec: 107.7 +/- 0.6 (jitter = 1.5)        9.096</span><br><span class="line">40      images/sec: 107.9 +/- 0.4 (jitter = 1.8)        9.069</span><br><span class="line">40      images/sec: 107.9 +/- 0.4 (jitter = 1.8)        9.194</span><br><span class="line">50      images/sec: 108.3 +/- 0.4 (jitter = 2.2)        9.206</span><br><span class="line">50      images/sec: 108.3 +/- 0.4 (jitter = 2.0)        9.485</span><br><span class="line">60      images/sec: 108.3 +/- 0.3 (jitter = 2.1)        9.139</span><br><span class="line">60      images/sec: 108.3 +/- 0.3 (jitter = 2.0)        9.237</span><br><span class="line">70      images/sec: 107.8 +/- 0.5 (jitter = 2.2)        9.132</span><br><span class="line">70      images/sec: 107.8 +/- 0.5 (jitter = 2.2)        9.045</span><br><span class="line">80      images/sec: 107.8 +/- 0.4 (jitter = 2.3)        9.092</span><br><span class="line">80      images/sec: 107.8 +/- 0.4 (jitter = 2.2)        9.098</span><br><span class="line">90      images/sec: 107.7 +/- 0.4 (jitter = 2.2)        9.205</span><br><span class="line">90      images/sec: 107.7 +/- 0.4 (jitter = 2.3)        9.145</span><br><span class="line">100     images/sec: 107.7 +/- 0.4 (jitter = 2.1)        9.050</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">total images/sec: 215.33</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">100     images/sec: 107.7 +/- 0.4 (jitter = 2.1)        9.013</span><br><span class="line">----------------------------------------------------------------</span><br><span class="line">total images/sec: 215.32</span><br><span class="line">----------------------------------------------------------------</span><br></pre></td></tr></table></figure><blockquote><p>这里用到了<a href="https://hub.docker.com/r/mpioperator/tensorflow-benchmarks">mpioperator/tensorflow-benchmarks:latest</a>镜像, 需要说明的是, 最新的latest版本是基于cuda10的, 华为云的K8s环境目前只支持cuda9, 因此使用<code>0.2.0</code>的tag版本, 能够完成任务.</p></blockquote><h2 id="实现简析"><a href="#实现简析" class="headerlink" title="实现简析"></a>实现简析</h2><p>在<code>Launcher</code>的日志上, 首先出现的是分发命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">POD_NAME=tensorflow-benchmarks-worker-1</span><br><span class="line"><span class="built_in">shift</span></span><br><span class="line">/opt/kube/kubectl <span class="built_in">exec</span> tensorflow-benchmarks-worker-1 -- /bin/sh -c     PATH=/usr/local/bin:<span class="variable">$PATH</span> ; <span class="built_in">export</span> PATH ; LD_LIBRARY_PATH=/usr/local/lib:<span class="variable">$LD_LIBRARY_PATH</span> ; <span class="built_in">export</span> LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/usr/local/lib:<span class="variable">$DYLD_LIBRARY_PATH</span> ; <span class="built_in">export</span> DYLD_LIBRARY_PATH ;   /usr/local/bin/orted -mca ess <span class="string">&quot;env&quot;</span> -mca ess_base_jobid <span class="string">&quot;2828730368&quot;</span> -mca ess_base_vpid 2 -mca ess_base_num_procs <span class="string">&quot;3&quot;</span> -mca orte_node_regex <span class="string">&quot;tensorflow-benchmarks-launcher-[1:5]l8hm,tensorflow-benchmarks-worker-[1:0-1]@0(3)&quot;</span> -mca orte_hnp_uri <span class="string">&quot;2828730368.0;tcp://172.16.0.86:37557&quot;</span> -mca pml <span class="string">&quot;ob1&quot;</span> -mca btl <span class="string">&quot;^openib&quot;</span> -mca plm <span class="string">&quot;rsh&quot;</span> -mca plm_rsh_agent <span class="string">&quot;/etc/mpi/kubexec.sh&quot;</span> -mca orte_default_hostfile <span class="string">&quot;/etc/mpi/hostfile&quot;</span> -mca hwloc_base_binding_policy <span class="string">&quot;none&quot;</span> -mca rmaps_base_mapping_policy <span class="string">&quot;slot&quot;</span> -mca pmix <span class="string">&quot;^s1,s2,cray,isolated&quot;</span></span><br><span class="line">POD_NAME=tensorflow-benchmarks-worker-0</span><br><span class="line"><span class="built_in">shift</span></span><br><span class="line">/opt/kube/kubectl <span class="built_in">exec</span> tensorflow-benchmarks-worker-0 -- /bin/sh -c     PATH=/usr/local/bin:<span class="variable">$PATH</span> ; <span class="built_in">export</span> PATH ; LD_LIBRARY_PATH=/usr/local/lib:<span class="variable">$LD_LIBRARY_PATH</span> ; <span class="built_in">export</span> LD_LIBRARY_PATH ; DYLD_LIBRARY_PATH=/usr/local/lib:<span class="variable">$DYLD_LIBRARY_PATH</span> ; <span class="built_in">export</span> DYLD_LIBRARY_PATH ;   /usr/local/bin/orted -mca ess <span class="string">&quot;env&quot;</span> -mca ess_base_jobid <span class="string">&quot;2828730368&quot;</span> -mca ess_base_vpid 1 -mca ess_base_num_procs <span class="string">&quot;3&quot;</span> -mca orte_node_regex <span class="string">&quot;tensorflow-benchmarks-launcher-[1:5]l8hm,tensorflow-benchmarks-worker-[1:0-1]@0(3)&quot;</span> -mca orte_hnp_uri <span class="string">&quot;2828730368.0;tcp://172.16.0.86:37557&quot;</span> -mca pml <span class="string">&quot;ob1&quot;</span> -mca btl <span class="string">&quot;^openib&quot;</span> -mca plm <span class="string">&quot;rsh&quot;</span> -mca plm_rsh_agent <span class="string">&quot;/etc/mpi/kubexec.sh&quot;</span> -mca orte_default_hostfile <span class="string">&quot;/etc/mpi/hostfile&quot;</span> -mca hwloc_base_binding_policy <span class="string">&quot;none&quot;</span> -mca rmaps_base_mapping_policy <span class="string">&quot;slot&quot;</span> -mca pmix <span class="string">&quot;^s1,s2,cray,isolated&quot;</span></span><br></pre></td></tr></table></figure><p>通过<code>/opt/kube/kubectl exec</code>的命令方式, 将执行的真实命令发送到<code>worker</code>上, 这两个命令唯一的差别为<code>-mca ess_base_vpid</code>的序号.</p><blockquote><p><code>worker</code>的启动命令为<code>sleep 365d</code>, 除了接受<code>Launcher</code>的命令之外, 不做其他任何的东西.</p></blockquote><p>那么<code>Launcher</code>是如何实现命令的封装的呢?</p><p>通过mpi-job启动的时候, 将<code>hostfile</code>和<code>kubexec.sh</code>封装在<code>configmap</code>之中.</p><p><code>hostfile</code>通过<code>name-worker-id</code> 的方式组装, 而<code>kubexec.sh</code>应该每个都一样</p><blockquote><p>因此, 每个mpi-job都会有对应的config</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get configmap tensorflow-benchmarks-config -o yaml -n mpi-operator</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  hostfile: |</span><br><span class="line">    tensorflow-benchmarks-worker-0 slots=1</span><br><span class="line">    tensorflow-benchmarks-worker-1 slots=1</span><br><span class="line">  kubexec.sh: |</span><br><span class="line">    <span class="comment">#!/bin/sh</span></span><br><span class="line">    <span class="built_in">set</span> -x</span><br><span class="line">    POD_NAME=<span class="variable">$1</span></span><br><span class="line">    <span class="built_in">shift</span></span><br><span class="line">    /opt/kube/kubectl <span class="built_in">exec</span> <span class="variable">$&#123;POD_NAME&#125;</span> -- /bin/sh -c <span class="string">&quot;$*&quot;</span></span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2019-09-22T04:07:36Z</span><br><span class="line">  labels:</span><br><span class="line">    app: tensorflow-benchmarks</span><br><span class="line">  name: tensorflow-benchmarks-config</span><br><span class="line">  namespace: mpi-operator</span><br><span class="line">  ownerReferences:</span><br><span class="line">  - apiVersion: kubeflow.org/v1alpha2</span><br><span class="line">    blockOwnerDeletion: <span class="literal">true</span></span><br><span class="line">    controller: <span class="literal">true</span></span><br><span class="line">    kind: MPIJob</span><br><span class="line">    name: tensorflow-benchmarks</span><br><span class="line">    uid: 82cd05da-dcee-11e9-ac58-fa163e3a1ebd</span><br><span class="line">  resourceVersion: <span class="string">&quot;39344012&quot;</span></span><br><span class="line">  selfLink: /api/v1/namespaces/mpi-operator/configmaps/tensorflow-benchmarks-config</span><br><span class="line">  uid: 82cef441-dcee-11e9-860a-fa163e132ef9</span><br></pre></td></tr></table></figure><p>那么这两个文件是如何注入到mpi的流程之中呢?</p><p>应该是通过环境变量<code>OMPI_MCA_plm_rsh_agent</code>和<code>OMPI_MCA_orte_default_hostfile</code>, 这两个应该会像回调函数一样, mpirun的过程之中被执行(这部分是猜测的).</p><h2 id="MPI到底是什么"><a href="#MPI到底是什么" class="headerlink" title="MPI到底是什么?"></a>MPI到底是什么?</h2><p>说了这么久的Kubeflow的MPI Operator, 但对于MPI陌生的人, 应该是完全陌生的领域.</p><p>在这里例子之中, 最后有个参数是<code>--variable_update=horovod</code>, <a href="https://github.com/horovod/horovod">Horovod</a>就是一种基于MPI架构实现分布式训练框架, 我准备再开个番外篇专门介绍一下<code>Horovod</code>, 在其中学习一下MPI.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JupyterLab插件整理</title>
      <link href="/2019/09/05/JupyterLab%E6%8F%92%E4%BB%B6%E6%95%B4%E7%90%86/"/>
      <url>/2019/09/05/JupyterLab%E6%8F%92%E4%BB%B6%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Jupyter架构"><a href="#Jupyter架构" class="headerlink" title="Jupyter架构"></a>Jupyter架构</h2><p><a href="https://jupyter.readthedocs.io/en/latest/index.html">文档</a></p><p>交互图:</p><p><img src="https://jupyter.readthedocs.io/en/latest/_images/notebook_components.png" alt=""></p><p>组件图:</p><p><img src="https://jupyter.readthedocs.io/en/latest/_images/repos_map.png" alt=""></p><h2 id="插件列表"><a href="#插件列表" class="headerlink" title="插件列表"></a>插件列表</h2><p><a href="https://github.com/markusschanta/awesome-jupyter">awesome-jupyter</a></p><p><a href="https://github.com/mauhai/awesome-jupyterlab">awesome-jupyterlab</a></p><h3 id="jupyterlab-toc"><a href="#jupyterlab-toc" class="headerlink" title="jupyterlab-toc)"></a><a href="[jupyterlab-toc](https://github.com/ian-r-rose/jupyterlab-toc">jupyterlab-toc</a>)</h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @jupyterlab/toc</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/ian-r-rose/jupyterlab-toc/raw/master/toc.gif" alt=""></p><h3 id="jupyterlab-tensorboard"><a href="#jupyterlab-tensorboard" class="headerlink" title="jupyterlab_tensorboard"></a><a href="https://github.com/chaoleili/jupyterlab_tensorboard">jupyterlab_tensorboard</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install jupyterlab_tensorboard</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/chaoleili/jupyterlab_tensorboard/raw/master/image/launcher.png" alt=""></p><p><img src="https://github.com/chaoleili/jupyterlab_tensorboard/raw/master/image/commands-input.png" alt=""></p><h3 id="JupyterLab-drawio"><a href="#JupyterLab-drawio" class="headerlink" title="JupyterLab drawio"></a><a href="https://github.com/QuantStack/jupyterlab-drawio">JupyterLab drawio</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install jupyterlab-drawio</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/QuantStack/jupyterlab-drawio/raw/master/drawio.gif" alt=""></p><h3 id="fasta-extension"><a href="#fasta-extension" class="headerlink" title="fasta-extension"></a><a href="https://github.com/jupyterlab/jupyter-renderers/tree/master/packages/fasta-extension">fasta-extension</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @jupyterlab/fasta-extension</span><br></pre></td></tr></table></figure><p><img src="https://camo.githubusercontent.com/6aa00b126595f41bc5bee84a6696234b63036fda/687474703a2f2f672e7265636f726469742e636f2f74656d697a6a616539582e676966" alt=""></p><h3 id="jupyterlab-git"><a href="#jupyterlab-git" class="headerlink" title="jupyterlab-git"></a><a href="https://github.com/jupyterlab/jupyterlab-git">jupyterlab-git</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @jupyterlab/git</span><br><span class="line">pip install --upgrade jupyterlab-git</span><br><span class="line">jupyter serverextension <span class="built_in">enable</span> --py jupyterlab_git</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://camo.githubusercontent.com/8e2f2b6abdaff6b180bf6aee95b288a7af0fde4d/687474703a2f2f672e7265636f726469742e636f2f4e39496b7a62796b38502e676966" alt=""></p><h3 id="jupyterlab-variableInspector"><a href="#jupyterlab-variableInspector" class="headerlink" title="jupyterlab-variableInspector"></a><a href="https://github.com/lckr/jupyterlab-variableInspector">jupyterlab-variableInspector</a></h3><p>安装命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter labextension install @lckr/jupyterlab_variableinspector</span><br></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://github.com/lckr/jupyterlab-variableInspector/raw/master/early_demo.gif" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JupyterLab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]KNative介绍</title>
      <link href="/2019/08/24/Kubeflow%E7%B3%BB%E5%88%97-KNative%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/08/24/Kubeflow%E7%B3%BB%E5%88%97-KNative%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>KNative目前的一些介绍文章:</p><ol><li><a href="https://knative.dev/">官网</a></li><li><a href="https://www.servicemesher.com/getting-started-with-knative/installing-knative.html">ServiceMesher介绍</a></li><li><a href="https://zhuanlan.zhihu.com/p/53597915">蚂蚁金服的布道师</a></li><li><a href="https://www.infoq.cn/article/PEOIcPk4lZRg-fAwry8H">华为云介绍系列</a></li></ol></blockquote><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>第一次关注到KNative这个项目是在Info上<a href="https://www.infoq.cn/article/6TzPqrVwv-YJYGQpKyzk">Kubernetes 上领先的开源 Serverless 解决方案有哪些</a>的文章上, 文章里面对比了常见的Serverless框架, KNative是其中的一个, 又由于这个项目是Google推出的, 因此当时觉得这个项目应该是最有希望的.</p><blockquote><p>参考Service Mesh之争, 2018年年中的时候,  还有一些框架例如Istio和Linkred再竞争, 但是到2019年中基础上Istio已经变成事实标准了. 具体可以看<a href="https://www.infoq.cn/article/DtxylyFwlyl7K5Jte*WI">这盘文章</a></p></blockquote><p>目前KNative已经迭代到0.8版本, 按照google对版本命名的方式, 应该离正式版本近了.</p><p>这次随着KFServing依赖KNative组件, 因此安装了一下KNative体验了一下, 就顺便记录一下吧.</p><h2 id="KNative介绍"><a href="#KNative介绍" class="headerlink" title="KNative介绍"></a>KNative介绍</h2><p>KNative是Google在2018年7月份发布的, 定位为基于 Kubernetes 的 Serverless 解决方案，旨在标准化 Serverless，简化其学习成本.</p><p>Serverless 大体上可以分为两种类型：“Backend as a Service” 和 “Functions as a Service”</p><p>BaaS(Backend as a Service) 后端即服务，服务商为客户 (开发者) 提供整合云后端的服务，如提供文件存储、数据存储、推送服务、身份验证服务等功能，以帮助开发者快速开发应用。</p><p>FaaS(Function as a Service) 函数即服务，服务商提供一个平台，允许客户开发、运行和管理应用程序功能，而无需构建和维护基础架构。按照此模型构建应用程序是实现“无服务器”体系结构的一种方式，通常在构建微服务应用程序时使用。</p><p>而现在看KNative算是FAAS的一员,  FAAS必须要解决的三个问题以及KNative对应方案为:</p><ol><li>函数程序如何编译, 因此KNative有对应的Builder模块, 解决代码转化为镜像的问题</li><li>任务如何触发, 对应KNative的Event模块,  使用一些消息中间件来缓存/发送请求, 例如Kafka</li><li>函数如何服务化, 对应的是KNative的Serving模块, 将用户的代码服务化, 并有服务该有的能力, 例如流量控制,自动伸缩等</li></ol><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/knative/three-compomnent.jpg" alt=""></p><p>这个三组件是KNative的核心, 整个流程都按照三个模块来构建, 下面就看一下每个模块之中的架构概念.</p><blockquote><p>目前是0.8版本, 现在Build模块已经被移除, 链接到了另外一个开源库.</p><p>就整个架构而言,  Build应该算是整个框架的接口, 如果这个Build不在自己做的话, 无法完成平台的闭环</p></blockquote><h2 id="KNative架构"><a href="#KNative架构" class="headerlink" title="KNative架构"></a>KNative架构</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/knative/knative-audience.svg" alt=""></p><p>首先看一下KNative的顶层架构:</p><ul><li><p>KNative依赖于Kubernetes, 三大组件的物理实现都是Kubernetes的CRD</p></li><li><p>而对外接口又依赖于Istio,  将一些服务治理网关路由的工作交由Istio完成</p></li><li><p>而KNative自己致力于完成开发者服务开发的工作</p></li></ul><blockquote><p>分工很明确, 比较看好这种方式</p></blockquote><h3 id="KNative-Serving模块"><a href="#KNative-Serving模块" class="headerlink" title="KNative Serving模块"></a>KNative Serving模块</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/knative/serving.png" alt=""></p><p>Serving主要是服务管理的能力, KNative创造了这几个概念:</p><ol><li><p>Service: 工作负载的底层概念, 管理整个生命周期</p></li><li><p>Route: 用于流量控制, 将不同的请求转发到不同的Revision</p></li><li><p>Configuration: 用于维护Service的配置</p></li><li><p>Revision: 每次代码或者配置修改, 都会生成一个Revision, 一个Revision会对应一个K8s的Deployment</p></li></ol><p>KNative的Serving的概念,与Istio有部分的重复, 但是比Istio更好理解.</p><p>下面看一个真实的<a href="https://knative.dev/docs/serving/samples/hello-world/helloworld-go/index.html">例子</a>:</p><p>代码和docker的构建略过, 只看如何部署Service</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">serving.knative.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span> <span class="comment"># KNative的Serving定义</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">helloworld-go</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">docker.io/&#123;username&#125;/helloworld-go</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">TARGET</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">&quot;Go Sample v1&quot;</span></span><br></pre></td></tr></table></figure><p>创建完成这个步骤, 会依次创建Service/Route/Configuration/Revision以及真实的Deployment.</p><p>该Deployment会注入Istio-proxy, 最后访问由Istio路由来决定, 最后访问该Service也是通过Istio的网关</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@gpu-01-client ~]<span class="comment">#kubectl get svc istio-ingressgateway --namespace istio-system</span></span><br><span class="line">NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGE</span><br><span class="line">istio-ingressgateway   LoadBalancer   10.247.103.192   100.95.144.30   80:31651/TCP,443:30856/TCP   9d</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>获取KNative的serviceURL</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@gpu-01-client ~]<span class="comment">#kubectl get ksvc helloworld-go  --output=custom-columns=NAME:.metadata.name,URL:.status.url</span></span><br><span class="line">NAME            URL</span><br><span class="line">helloworld-go   http://helloworld-go.default.example.com</span><br></pre></td></tr></table></figure><p>测试<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@gpu-01-client ~]<span class="comment"># curl -H &quot;Host:helloworld-go.default.example.com&quot; http://100.95.144.30</span></span><br><span class="line">Hello Go Sample v1!</span><br></pre></td></tr></table></figure></p><blockquote><p>KNative能够Scala-to-Zero, 这样也有一个坏处, 当一个新请求到来的时候, 第一次启动会相对较长的时间(需要启动一个工作的容器)</p></blockquote><h3 id="KNative-Event模块"><a href="#KNative-Event模块" class="headerlink" title="KNative Event模块"></a>KNative Event模块</h3><blockquote><p>这部分待续, 还没有看完</p></blockquote><h2 id="KNative总结"><a href="#KNative总结" class="headerlink" title="KNative总结"></a>KNative总结</h2><p>目前KNative的版本还在0.8版本, 还不算正式release, 后续可能会有不少变动.</p><p>而且Build模块的去除, 感觉已经缺少了一个对于开发者的Interface, 不知道他们未来打算拿什么来弥补.</p><p>而对于Serverless框架来说, 我感觉近2年之内应该无法决出真正的事实标准框架, 所以对KNative来说, 还有时间.</p><p>对我们来说, 在1.0版本release之前, 不要将KNative纳入到系统的构架之中, 后续在持续关注社区后期的动向.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[转]MySQL与PostgreSQL对比</title>
      <link href="/2019/08/24/%E8%BD%AC-MySQL%E4%B8%8EPostgreSQL%E5%AF%B9%E6%AF%94/"/>
      <url>/2019/08/24/%E8%BD%AC-MySQL%E4%B8%8EPostgreSQL%E5%AF%B9%E6%AF%94/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>文章出在<a href="https://www.biaodianfu.com/mysql-vs-postgresql.html">此处</a>, 这篇文章就像文章摘要一样, 里面的结论也无法保证正确, 需要等后续实践之中来验证.</p><p>今后还会收集其他的一些文章补充这篇文章的内容</p></blockquote><p>最近项目之中, 再选型关系型数据库的类型, 之前团队里一直都是用MySQL + MyBatis的方案, 但是其他项目组选型了PostgreSQL, 现在正在考虑是否要迁移数据库系统.</p><h2 id="MySQL的优势项"><a href="#MySQL的优势项" class="headerlink" title="MySQL的优势项"></a>MySQL的优势项</h2><ol><li><strong>流行度高</strong>, 因此相应的第三方工具会更加齐全</li><li><strong>回滚更好</strong>, PG需要定时触发VACUUM, 否则数据可能膨胀</li><li><strong>Windows支持好</strong></li><li><strong>线程模式</strong>, 相比PG资源利用率高</li><li><strong>用户权限更加完善</strong>, PG只有表级权限, 而MySQL支持列权限</li><li><strong>存储引擎插件化</strong>, innodb适合事务处理场景外, myisam适合静态数据的查询场景</li><li><strong>24*7小时运行</strong></li><li><strong>支持堆表和索引表</strong>, PG只支持堆表</li></ol><h2 id="PostgreSQL的优势项"><a href="#PostgreSQL的优势项" class="headerlink" title="PostgreSQL的优势项"></a>PostgreSQL的优势项</h2><ol><li><strong>Json和Array格式支持</strong>, MySQL5.7版本之后, 也支持JSon, 但是能力略为落后</li><li><strong>GIS支持</strong>, 一般都用PG</li><li><strong>PostgREST提供API能力</strong></li><li><strong>支持树状结构</strong>, 例如R-Tree</li><li><strong>SQL编程</strong>, 使用各种语言来编程, 对标的是MySQL的存储过程</li><li><strong>支持外部数据源</strong>, 这儿估计一堆的限制</li><li><strong>Text</strong>没有长度限制, MySQL需要区分small text, middle text, large text, 但实际上我觉得程序员定义这个长度是比较好的, 就像int和long类型一个意思</li><li><strong>支持图结构数据存储</strong></li><li><strong>支持窗口函数</strong>, OVER语句, 没想到MySQL竟然没有支持这个, SparkSQL都实现了</li><li><strong>更多索引类型</strong>, </li><li><strong>集群支持更好</strong>, 这个点需要存疑, 因为mysql的分布式中间件那么多, 感觉有点不对</li><li><strong>事务隔离做的更好</strong></li><li><strong>对于字符支持更好一些</strong></li><li><strong>对表连接支持较完整</strong>, 真的很难相信MySQL不支持HashJoin和SortMergeJoin, 之前数据库确实用的太简单了</li><li><strong>存储方式支持更大的数据量</strong></li><li><strong>时间精度更高</strong></li><li><strong>优化器的功能较完整</strong></li><li><strong>序列支持更好</strong></li><li><strong>对子查询支持更好</strong></li><li><strong>增加列更加简单</strong></li></ol><h2 id="两者选择规则"><a href="#两者选择规则" class="headerlink" title="两者选择规则"></a>两者选择规则</h2><ol><li>如果是非常简单的场景, 直接使用MySQL</li><li>如果涉及到数据完整性和可靠性的时候, 使用PG</li><li>如果是地理数据, 使用PG</li><li>如果有嵌入式SQL场景, 使用PG</li><li>如果你想学习一个经典的关系型数据库, 使用PG吧</li></ol><h2 id="MyBatis兼容PG和MySQL"><a href="#MyBatis兼容PG和MySQL" class="headerlink" title="MyBatis兼容PG和MySQL"></a>MyBatis兼容PG和MySQL</h2><p>如果上层使用了MyBatis的话, MyBatis有个叫做<code>DatabaseIdProvider</code>的能力可以支持多个不同的数据库.</p><p>在我们的例子里面, 需要支持MySQL/PG/HSQL三种数据库, 就需要在MyBatis的配置文件, 写在如下配置:</p><blockquote><p> HSQL是本地测试常用的数据库</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">databaseIdProvider</span> <span class="attr">type</span>=<span class="string">&quot;DB_VENDOR&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;MySQL&quot;</span> <span class="attr">value</span>=<span class="string">&quot;mysql&quot;</span>/&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;PostgreSQL&quot;</span> <span class="attr">value</span>=<span class="string">&quot;postgresql&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;HSQL&quot;</span> <span class="attr">value</span>=<span class="string">&quot;hsqldb&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">databaseIdProvider</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在具体的查询语句的配置项文件里面, 可以使用如下配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">choose</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">when</span> <span class="attr">test</span>=<span class="string">&quot;_databaseId == &#x27;postgresql&#x27;&quot;</span>&gt;</span></span><br><span class="line">    LIMIT #&#123;pageSize&#125; OFFSET #&#123;offset&#125;;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">when</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">when</span> <span class="attr">test</span>=<span class="string">&quot;_databaseId == &#x27;mysql&#x27;&quot;</span>&gt;</span></span><br><span class="line">    LIMIT #&#123;offset&#125;, #&#123;pageSize&#125;;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">when</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">otherwise</span>&gt;</span></span><br><span class="line">    LIMIT #&#123;offset&#125;, #&#123;pageSize&#125;;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">otherwise</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">choose</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在这个例子里面, PG和MySQL的Limit语法不同, 因此根据不同情况来生成不同的SQL语句.</p><p>在这儿使用的是MyBatis里面的<code>choose-when-otherwise</code>的语法, 这个语法含义和Java中的<code>match</code>含义类似</p><p>在每个<code>when</code>中判断内置的<code>DatabaseIdProvider</code>字段<code>_databaseId</code>是否为PG或者MySQL, 这儿的字符串<code>mysql</code>和<code>postgresql</code>就是在<code>databaseIdProvider</code>定义的那个两个数据库</p>]]></content>
      
      
      <categories>
          
          <category> 转载文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]KFServing介绍</title>
      <link href="/2019/08/23/Kubeflow%E7%B3%BB%E5%88%97-KFServing%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/08/23/Kubeflow%E7%B3%BB%E5%88%97-KFServing%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>之前调研了<a href="https://saintbacchus.github.io/2019/08/11/TensorServing%E4%BE%8B%E5%AD%90/">TensorFlow Serving</a>的功能, 这周又抽出来一点时间, 调研一下<a href="https://github.com/kubeflow/kfserving.git">KubeFlow Serving</a></p><h2 id="KFServing的定位"><a href="#KFServing的定位" class="headerlink" title="KFServing的定位"></a>KFServing的定位</h2><p>之前在Kubeflow的<a href="https://saintbacchus.github.io/2019/07/14/Kubeflow%E4%BB%8B%E7%BB%8D/">介绍文章</a>有提过, Kubeflow支持Training和Serving, 但是如果仔细看Serving的<a href="https://www.kubeflow.org/docs/components/serving/">官网</a>会发现, 目前这个Serving似乎只是把这种各样的Serving镜像给安装部署起来就好了, 感觉游离在系统之外的.</p><p>所以TFServing的目标就是设计一套接口, 将各个Serving模块抽象化, 变成一套系统.</p><blockquote><p> 但是, 目前TFServing虽然提交活跃度挺高, 但是还没有挂在官网介绍文章之中,  版本非常新, 目前才v0.2版本, 未来接口可能会有很大变化</p></blockquote><h2 id="KFServing架构"><a href="#KFServing架构" class="headerlink" title="KFServing架构"></a>KFServing架构</h2><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kfserving/kfserving.png" alt=""></p><p>从架构图上可以看出, KFServing在非常高的位置之上,  它底层的服务能力依赖于<a href="https://knative.dev/docs/">KNative</a>(<em>主要是KNative的Serving模块</em>), 上层支持的框架有:</p><ol><li>TensorFlow: 镜像由于TensorFlow官网提供</li><li>PyTorch:  镜像由KFServing制作, 代码逻辑位于<a href="https://github.com/kubeflow/kfserving/tree/master/python">此</a></li><li>SKLearn: 同上</li><li>XGBoost: 同上</li><li>TensorRT: 镜像由NVIDIA提供</li></ol><p><strong>基本上主流的一些机器学习框架都已经支持</strong></p><p>下面看一下KFServing的服务能力,  数据流图如下所以: </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kfserving/dataplane.jpg" alt=""></p><p>这个是一个典型的灰度发布场景,  一个默认环境, 一个灰度环境, 由KNative的来控制流量.</p><blockquote><p>KFServing似乎目前只支持一个灰度实例</p></blockquote><h2 id="KFServing的使用案例"><a href="#KFServing的使用案例" class="headerlink" title="KFServing的使用案例"></a>KFServing的使用案例</h2><p>看个简单的TensorFlow Serving的例子,  所有的样例在这个<a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples">代码路径</a>之中</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;serving.kubeflow.org/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;KFService&quot;</span> <span class="comment"># `KFService`是KFServing在K8s上定义的CRD.</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;flowers-sample&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">default:</span></span><br><span class="line">    <span class="attr">tensorflow:</span></span><br><span class="line">      <span class="attr">modelUri:</span> <span class="string">&quot;gs://kfserving-samples/models/tensorflow/flowers&quot;</span></span><br></pre></td></tr></table></figure><p>这段YAML的意思为, 创建一个TensorFlow类型的Serving, 模型文件存在<code>Google Cloud</code>的<code>gs://kfserving-samples/models/tensorflow/flowers</code>的路径之中.</p><blockquote><p>TensorFlow Serving并不支持GCS的路径, 这里TF Serving通过一个<a href="https://github.com/kubeflow/kfserving/blob/master/python/model-initializer.Dockerfile">model-initializer</a>的<code>init-container</code>提前下载到容器内部, TF Serving实例上读取是本地路径,  所以模型不能太大, 因为目前还不支持挂载PVC</p></blockquote><p>当然也可以设置一个灰度服务: </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;serving.kubeflow.org/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;KFService&quot;</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;flowers-sample&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">default:</span></span><br><span class="line">   <span class="comment"># 90% of traffic is sent to this model</span></span><br><span class="line">    <span class="attr">tensorflow:</span></span><br><span class="line">      <span class="attr">modelUri:</span> <span class="string">&quot;gs://kfserving-samples/models/tensorflow/flowers&quot;</span></span><br><span class="line">  <span class="attr">canaryTrafficPercent:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">canary:</span></span><br><span class="line">   <span class="comment"># 10% of traffic is sent to this model</span></span><br><span class="line">    <span class="attr">tensorflow:</span></span><br><span class="line">      <span class="attr">modelUri:</span> <span class="string">&quot;gs://kfserving-samples/models/tensorflow/flowers-2&quot;</span></span><br></pre></td></tr></table></figure><p>这个YAML的含义是, 90%走到default服务, 也就是<code>flowers</code>模型, 而剩下的10%的模型走到<code>canary</code>去, 模型为<code>flowers-2</code></p><blockquote><p>这里会生成KNative的资源有: 1个Service, 1个Route, 2个Configuration, 2个Revision</p></blockquote><p>你可以使用KNative的<a href="https://github.com/kubeflow/kfserving/tree/master/docs/samples/tensorflow#knative-cli">客户端</a>, 来完成两个灰度升级回滚等操作.</p><p>访问流量需要获取<code>istio-ingressgateway</code>的地址, 加上定义的url就能访问了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MODEL_NAME=flowers-sample</span><br><span class="line">INPUT_PATH=@./input.json</span><br><span class="line">CLUSTER_IP=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=<span class="string">&#x27;&#123;.status.loadBalancer.ingress[0].ip&#125;&#x27;</span>)</span><br><span class="line">SERVICE_HOSTNAME=$(kubectl get kfservice <span class="variable">$&#123;MODEL_NAME&#125;</span> -o jsonpath=<span class="string">&#x27;&#123;.status.url&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">curl -v -H <span class="string">&quot;Host: <span class="variable">$&#123;SERVICE_HOSTNAME&#125;</span>&quot;</span> http://<span class="variable">$CLUSTER_IP</span>/v1/models/<span class="variable">$MODEL_NAME</span>:predict -d <span class="variable">$INPUT_PATH</span></span><br></pre></td></tr></table></figure><blockquote><p>YAML的字段定义在<a href="https://github.com/kubeflow/kfserving/blob/master/docs/control-plane.md">这篇文档</a>之中</p></blockquote><h2 id="KFServing总结"><a href="#KFServing总结" class="headerlink" title="KFServing总结"></a>KFServing总结</h2><p>首先, KFServing目前是一个非常前期的项目, 这就注定了可能很多功能, 它目前都不支持, 但是目前它的目标我们实际上是接受的, 等这个版本再经过几轮迭代之后可以跟进.</p><blockquote><p>模型资产存储在OBS上, Serving获取模型, 自动部署模型推理服务</p></blockquote><p>其次, KFServing的架构依赖实在太多了, 特别依赖是KNative, 目前还不能确定KNative能够在Serverless框架的竞争之中胜出, 而且KNative的版本也比较前期, 问题也挺多, 对于KFServing质量评分造成了不少影响.</p><blockquote><p>实际上灰度发布和流量控制, Istio能力已经完全具备, 不知道为什么必须依赖KNative, 未来能解除KNative的依赖就好了</p></blockquote><p>再次, KFServing有Python的<a href="https://github.com/kubeflow/kfserving/tree/master/python/kfserving">SDK</a>, 可以使用代码直接部署KFServing的Service, 这对工程人员是一个很方便的能力.</p><p>此外, 它由一个叫做<a href="https://github.com/kubeflow/kfserving/tree/master/tools/tf2openapi">TF2OpenAPI</a>的小工具, 能够将TF的模型的API描述自动生成, 又是对工程人员的工具.</p><p>最后, KFServing的<a href="https://github.com/kubeflow/kfserving/blob/master/ROADMAP.md">路标</a>之中提到, KFServing马上会在Kubeflow 0.7版本之中就集成了, 而且9月初就会支持PVC啦,  赶紧迭代起来吧.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu实用工具集</title>
      <link href="/2019/08/18/Ubuntu%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E9%9B%86/"/>
      <url>/2019/08/18/Ubuntu%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h2><ol><li><a href="https://code.visualstudio.com/">Visual Code</a>应该是最好用的代码编辑器了</li><li><a href="https://typora.io/">Typora</a>是Markdown编辑器</li><li><a href="https://github.com/notepadqq/notepadqq">notepadqq</a>Windows上Notepad++的替代者, 没那么好用</li><li><a href="https://github.com/shiftkey/desktop">GithubDestop</a>图像化管理GitRepository库</li><li><a href="https://www.jetbrains.com/idea/">Idea intellij</a>全家桶</li><li><a href="https://snapcraft.io">snap</a>Ubuntu上的homebrew, 不过挺难用的</li><li><a href="https://www.getpostman.com/downloads/">PostMan</a> 图形化API发送工具</li><li><a href="https://docs.conda.io/en/latest/miniconda.html">MiniConda</a>Python的环境管理工具</li><li><a href="https://zealdocs.org/download.html">Zeal</a>查看SDK的工具, MacOS上Dash的替代品</li></ol><h2 id="办公工具"><a href="#办公工具" class="headerlink" title="办公工具"></a>办公工具</h2><ol><li><a href="https://www.wps.com/linux">WPS</a>: 国产的Office工具</li><li><a href="https://github.com/shadowsocks">Shadowsocks</a>: 翻墙工具</li><li><a href="https://www.jianguoyun.com/s/downloads">坚果云</a>: 个人云盘, 有2G免费空间</li><li><a href="https://www.google.com/chrome/">Chrome</a>: Google浏览器</li><li><a href="https://bbs.360.cn/thread-15529293-1-1.html">360浏览器</a>: 确实没想到360竟然也有linux浏览器</li><li><a href="https://www.xmind.net/download/">XMind</a>: 思维导图工具</li><li>Shutter截图工具</li><li><a href="https://www.virtualbox.org/wiki/Linux_Downloads">VisualBox</a>偶尔可以切换一下Windows</li><li><a href="https://www.teamviewer.com/en-us/download/linux/">TeamViewer</a>可以控制远程的桌面</li></ol><h2 id="娱乐工具"><a href="#娱乐工具" class="headerlink" title="娱乐工具"></a>娱乐工具</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/40419090">网易云音乐</a>: 官网链接找不到了, 只有其他的安装文档</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[转]OAuth学习</title>
      <link href="/2019/08/18/%E8%BD%AC-OAuth%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/08/18/%E8%BD%AC-OAuth%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p> <a href="http://www.ruanyifeng.com/blog/2019/04/oauth_design.html">文章转载自此</a></p></blockquote><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>简单说，OAuth 就是一种<strong>授权机制</strong>。数据的所有者告诉系统，同意授权第三方应用进入系统，获取这些数据。系统从而产生一个短期的进入令牌（token），用来代替密码，供第三方应用使用</p><p>平时我们遇到的使用微信授权登录, 就是OAuth的一个应用. 在国外经常使用github或者google/facebook账号用于登录各种类似的网站.</p><p><img src="https://www.wangbase.com/blogimg/asset/201904/bg2019042101.jpg" alt=""></p><h2 id="令牌和密码"><a href="#令牌和密码" class="headerlink" title="令牌和密码"></a>令牌和密码</h2><p><code>OAuth</code>的一个特点是使用Token.</p><p>令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。</p><ol><li><p>令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。</p></li><li><p>令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。</p></li><li><p>令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。</p></li></ol><p>上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。</p><p>注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以<strong>令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。</strong>这也是为什么令牌的有效期，一般都设置得很短的原因。</p><h2 id="四种授权方式"><a href="#四种授权方式" class="headerlink" title="四种授权方式"></a>四种授权方式</h2><p>具体描述详见<a href="http://www.ruanyifeng.com/blog/2019/04/oauth-grant-types.html">文章</a>, 具体的认证流程的就不摘录了, 只摘录特点</p><h3 id="授权码"><a href="#授权码" class="headerlink" title="授权码"></a>授权码</h3><p>这种方式是最常用的流程，安全性也最高，它<strong>适用于那些有后端的 Web 应用</strong>。授权码通过前端传送，令牌则是储存在后端，而且所有与资源服务器的通信都在后端完成。这样的前后端分离，可以避免令牌泄漏。</p><h3 id="隐藏式"><a href="#隐藏式" class="headerlink" title="隐藏式"></a>隐藏式</h3><p>有些 Web 应用是纯前端应用，没有后端。这时就不能用上面的方式了，必须将令牌储存在前端。<strong>RFC 6749 就规定了第二种方式，允许直接向前端颁发令牌。这种方式没有授权码这个中间步骤，所以称为（授权码）”隐藏式”（implicit）。</strong></p><p>这种方式把令牌直接传给前端，是很不安全的。因此，只能用于一些安全要求不高的场景，并且令牌的有效期必须非常短，通常就是会话期间（session）有效，浏览器关掉，令牌就失效了。</p><h3 id="密码式"><a href="#密码式" class="headerlink" title="密码式"></a>密码式</h3><p><strong>如果你高度信任某个应用，RFC 6749 也允许用户把用户名和密码，直接告诉该应用。该应用就使用你的密码，申请令牌，这种方式称为”密码式”（password）。</strong></p><p>这种方式需要用户给出自己的用户名/密码，显然风险很大，因此只适用于其他授权方式都无法采用的情况，而且必须是用户高度信任的应用。</p><h3 id="凭证式"><a href="#凭证式" class="headerlink" title="凭证式"></a>凭证式</h3><p><strong>最后一种方式是凭证式（client credentials），适用于没有前端的命令行应用，即在命令行下请求令牌。</strong></p><p>这种方式给出的令牌，是针对第三方应用的，而不是针对用户的，即有可能多个用户共享同一个令牌。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>授权码</strong>是最常用的, 其他三个主要是为了针对特定的场景.</p><h2 id="代码案例"><a href="#代码案例" class="headerlink" title="代码案例"></a>代码案例</h2><p>参见这篇<a href="http://www.ruanyifeng.com/blog/2019/04/github-oauth.html">博客</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQLFlow深度解析</title>
      <link href="/2019/08/17/SQLFlow%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/08/17/SQLFlow%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="SQLFLow介绍"><a href="#SQLFLow介绍" class="headerlink" title="SQLFLow介绍"></a>SQLFLow介绍</h2><p>SQLFlow是阿里巴巴蚂蚁金服开源的一个AI On SQL的项目, 目标是<strong>SQL 引擎和 AI 引擎连接起来，让用户仅需几行 SQL 代码就能描述整个应用或者产品背后的数据流和 AI 构造</strong></p><p>SQLFlow 最早的初衷，就是希望解决分析师既要操作数据又要使用 AI、往往需要在两个甚至更多的系统之间切换、工作效率低的窘境。</p><p>目前业界已有的AI ON SQL的方案:</p><ul><li>Microsoft SQL Server：Microsoft SQL Server 支持机器学习服务，可以将 R 或 Python 编写的机器学习程序作为外部脚本运行.缺点: 需要编写R或者Python程序代码</li><li>Teradata SQL for DL：Teradata 也提供了 RESTful 服务，可以通过扩展的 SQL SELECT 语法调用. 缺点: 语法耦合了它的Rest服务 </li><li>Google BigQuery：Google BigQuery 通过引入 CREATE MODEL 语句让用 SQL 实现机器学习成为可能. 缺点: 支持的模型有点少, 深度学习还不支持.</li></ul><p>针对以上三个方案的缺点, SQLFlow的定义了自己的三个设计目标:</p><ul><li>这一解决方案应与许多 SQL 引擎都兼容，而不是只能兼容特定版本或类型的 SQL 引擎。</li><li>它应该支持复杂的机器学习模型，包括用于深度学习的 TensorFlow 和用于树模型的 XGBoost。</li><li>能够灵活地配置和运行前沿机器学习算法，包括指定特征交叉，无需在 SQL 语句中嵌入 Python 或 R 代码，以及完全集成超参数估计等。</li></ul><p>目前阶段来说, SQLFlow已经支持MySQL/MaxCompute/Hive, 机器学习框架已经支持TF和XGBoost.</p><p><strong>资源:</strong></p><p><a href="https://www.infoq.cn/article/vlVqC68h2MT-028lh68C">宣传文章</a></p><p><a href="https://github.com/sql-machine-learning/sqlflow">Github官网</a></p><p><a href="https://sqlflow.org">官方文档</a></p><blockquote><p>Spark社区既有MLib这样的机器学习框架, 也有一些基于深度学习的扩展, 例如<a href="https://github.com/horovod/horovod">Uber的horovod</a>还有<a href="https://github.com/yahoo/TensorFlowOnSpark">Yahoo的TensorFlowOnSpark</a>, 但是这些框架都是基于的是Spark DataSet的接口联通的, 你可以在DataSet API上使用SQL, 也可以使用AI接口,  你可以认为是<code>AI + SQL</code>的模式, 而不是<code>AI ON SQL</code>的模式</p></blockquote><h2 id="SQLFlow试用"><a href="#SQLFlow试用" class="headerlink" title="SQLFlow试用"></a>SQLFlow试用</h2><p>社区提供了一个官方的试用的Docker镜像, 只要键入以下命令启动容器即可:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 8888:8888 sqlflow/sqlflow:latest</span><br></pre></td></tr></table></figure><p>试用浏览器打开容器机器所在的8888端口, 可以看到一个notebook的页面, 默认已经有一个<code>example.ipynb</code>的样例文件了</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/2.png" alt=""></p><p>打开这个<code>example.ipynb</code>文件, 这个镜像里面已经安装了<code>mysql</code>, 同时也把部分测试数据导入到数据库里面, 你不需要做任何数据处理的工作, 就可以直接运行Notebook里面的Cell. <em>蚂蚁的开发人员真的很贴心啊</em></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/3.png" alt=""></p><p>执行推理过程, 使用<code>DNNClassifier</code>算法模型, 并将结果写入到<code>sqlflow_models.my_dnn_model</code>表中, <code>my_dnn_model</code>只有两个字段: ID和BLOB(存放模型序列化后的字节流)</p><p>执行对于测试集(<code>iris.test</code>)进行推理, 并写入到预测结果表之中(<code>iris.predict.class</code>)</p><p>最后展示推理结果集<code>iris.predict</code></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/4.png" alt=""></p><h2 id="架构解析"><a href="#架构解析" class="headerlink" title="架构解析"></a>架构解析</h2><p>这个AI ON SQL系统里面, 首先要回到的一个问题是, <strong>AI系统的计算层和SQL系统的计算层是什么关系?</strong></p><p>例如Spark(BigQuery大概率也是, 但是因为闭源不能确定)AI引擎代码是内嵌于SQL计算系统之中的, 并行执行的能力由Spark管理, AI系统就像代码库一样被Spark系统所调用而已.</p><p>但SQLFlow明显不是这种模式的:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/1.png" alt=""></p><p>从SQLFlow的架构图上看的出来, <strong>AI Engine和SQL Engine之间是独立的</strong>, 两者通过RPC交互<strong>数据和模型</strong></p><ol><li>AI Engine训练或者推理计算的时候, 从SQLEngine获取数据</li><li>AI Engine完成训练过程, 将模型写入到SQL Engine; 推理过程从SQL Engine读取模型</li></ol><p>整个SQLFlow的流程大致如下(图上红色部分为SQLFlow):</p><ol><li>Notebook输入SQL之后, 送入到<code>Parser</code>之内, 这儿的语法解析借用了Hive/MaxCompute等引擎</li><li>解析完SQL语法后, 进行<code>Schema Verification</code></li><li>然后根据SQL语法, 产生对应的Code(根据不同模型和不同引擎产生不同的Code)</li><li>最后执行Code</li></ol><p><a href="https://sql-machine-learning.github.io/sqlflow/doc/syntax.html">设计文档</a></p><h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><p>最后我们来简单过一遍SQLFlow的代码, 提炼一下找代码框架的思路:</p><blockquote><p>SQLFlow代码量目测在1-1.5万行左右, 半天就能看懂基础流程了</p></blockquote><h3 id="找到入口"><a href="#找到入口" class="headerlink" title="找到入口"></a>找到入口</h3><p>首先, 找到整个项目的Dockerfile, 就在根目录下</p><blockquote><p>如果项目有Docker镜像, Dockerfile就是个个进程的入口, 比<code>main</code>函数还在前面</p></blockquote><figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ADD</span><span class="language-bash"> scripts/start.sh /</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;bash&quot;</span>, <span class="string">&quot;/start.sh&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>看到最后一行启动的命令为<code>start.sh</code>, 源码文件在<code>scripts/start.sh</code></p><h3 id="找到启动文件"><a href="#找到启动文件" class="headerlink" title="找到启动文件"></a>找到启动文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">print_usage</span></span>() &#123;</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;Usage: /bin/bash start.sh [OPTION]\n&quot;</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;\tpopulate-example-dataset-mysql: populate an existing mysql instance with the example dataset.&quot;</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;\tmysql: setup the mysql server with the example dataset initialized.&quot;</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;\tsqlflow_server: setup the sqlflow gRPC server.&quot;</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;\tsqlflow_notebook: setup the Jupyter Notebook server.&quot;</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;\tall(default): setup a MySQL server instance, a sqlflow gRPC server and a Jupyter Notebook server sequentially.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">main</span></span>() &#123;</span><br><span class="line">  ARG=<span class="variable">$&#123;1:-all&#125;</span></span><br><span class="line">  <span class="keyword">case</span> <span class="variable">$ARG</span> <span class="keyword">in</span></span><br><span class="line">    all)</span><br><span class="line">      <span class="built_in">echo</span> <span class="string">&quot;setup all-in-one&quot;</span></span><br><span class="line">      setup_mysql</span><br><span class="line">      setup_sqlflow_server &amp;</span><br><span class="line">      setup_sqlflow_notebook</span><br><span class="line">      ;;</span><br><span class="line">    *)</span><br><span class="line">      print_usage</span><br><span class="line">      ;;</span><br><span class="line">  <span class="keyword">esac</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到启动文件启动了三个内容: </p><ol><li><p>mysql: 默认的SQL Engine, 已经初始化了数据</p></li><li><p>sqlflow_server: 我们要找的程序, 找到里面真正的执行命令<code>sqlflowserver --datasource=$&#123;DS&#125;</code></p></li><li><p>sqlflow_notebook: Notebook交互式界面</p></li></ol><h3 id="找到main函数入口"><a href="#找到main函数入口" class="headerlink" title="找到main函数入口"></a>找到main函数入口</h3><p>全局搜索<code>function main()</code>, 发现只有<code>cmd/sqlflowserver/main.go</code>有.</p><p>在<code>start</code>函数里面找到关键的<code>proto.RegisterSQLFlowServer(s, server.NewServer(sql.Run, nil, modelDir, enableSession))</code>服务启动代码, 其中的<code>sql.Run</code>就整个SQL处理代码.</p><blockquote><p>不太熟悉go语言是怎么出包的</p></blockquote><h3 id="SQL-Run"><a href="#SQL-Run" class="headerlink" title="SQL.Run"></a>SQL.Run</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Run</span><span class="params">(slct <span class="type">string</span>, db *DB, modelDir <span class="type">string</span>, session *pb.Session)</span></span> *PipeReader &#123;</span><br><span class="line">splittedSQL, err := splitExtendedSQL(slct)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">rd, wr := Pipe()</span><br><span class="line"><span class="comment">// return the lexer error message to client side</span></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wr.Close()</span><br><span class="line">wr.Write(err)</span><br><span class="line">&#125;()</span><br><span class="line"><span class="keyword">return</span> rd</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(splittedSQL) == <span class="number">2</span> &#123;</span><br><span class="line"><span class="keyword">return</span> runExtendedSQL(slct, db, modelDir, session)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> runStandardSQL(slct, db)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果是SQL语句, 走入到<code>runStandardSQL</code>分支, 如果有训练或者推理语法, 就会走入<code>runExtendedSQL</code>分支</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">runExtendedSQL</span><span class="params">(slct <span class="type">string</span>, db *DB, modelDir <span class="type">string</span>, session *pb.Session)</span></span> *PipeReader &#123;</span><br><span class="line">rd, wr := Pipe()</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> wr.Close()</span><br><span class="line"></span><br><span class="line">err := <span class="function"><span class="keyword">func</span><span class="params">()</span></span> <span class="type">error</span> &#123;</span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">(startAt time.Time)</span></span> &#123;</span><br><span class="line">log.Debugf(<span class="string">&quot;runExtendedSQL %v finished, elapsed:%v&quot;</span>, slct, time.Since(startAt))</span><br><span class="line">&#125;(time.Now())</span><br><span class="line">pr, e := newParser().Parse(slct) <span class="comment">// 语言解析</span></span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line">cwd, e := ioutil.TempDir(<span class="string">&quot;/tmp&quot;</span>, <span class="string">&quot;sqlflow&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">defer</span> os.RemoveAll(cwd)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> pr.train &#123;</span><br><span class="line">ds, e := newTrainAndValDataset(db, pr.standardSelect.String(), pr.standardSelect.tables[<span class="number">0</span>], <span class="number">0.8</span>)</span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> train(wr, pr, db, cwd, modelDir, slct, ds) <span class="comment">// 调用训练</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> pred(wr, pr, db, cwd, modelDir) <span class="comment">// 调用推理部分</span></span><br><span class="line">&#125;()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorf(<span class="string">&quot;runExtendedSQL error:%v&quot;</span>, err)</span><br><span class="line"><span class="keyword">if</span> err != ErrClosedPipe &#123;</span><br><span class="line"><span class="keyword">if</span> err := wr.Write(err); err != <span class="literal">nil</span> &#123;</span><br><span class="line">log.Errorf(<span class="string">&quot;runExtendedSQL error(piping):%v&quot;</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;()</span><br><span class="line"><span class="keyword">return</span> rd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到, 先进行<strong>语法解析</strong>, 然后进行<strong>训练或者推理</strong>逻辑, 我们简单看一眼<strong>训练</strong>过程:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">train</span><span class="params">(wr *PipeWriter, tr *extendedSelect, db *DB, cwd <span class="type">string</span>, modelDir <span class="type">string</span>, slct <span class="type">string</span>, ds *trainAndValDataset)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">fts, e := verify(tr, db) <span class="comment">// 语法校验</span></span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> program bytes.Buffer</span><br><span class="line"><span class="keyword">if</span> e := genTF(&amp;program, tr, ds, fts, db); e != <span class="literal">nil</span> &#123; <span class="comment">// 生成代码</span></span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">&quot;genTF %v&quot;</span>, e)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cw := &amp;logChanWriter&#123;wr: wr&#125;</span><br><span class="line"><span class="keyword">defer</span> cw.Close()</span><br><span class="line">cmd := tensorflowCmd(cwd, db.driverName) <span class="comment">// 执行命令</span></span><br><span class="line">cmd.Stdin = &amp;program</span><br><span class="line">cmd.Stdout = cw</span><br><span class="line">cmd.Stderr = cw</span><br><span class="line"><span class="keyword">if</span> e := cmd.Run(); e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">&quot;training failed %v&quot;</span>, e)</span><br><span class="line">&#125;</span><br><span class="line">m := model&#123;workDir: cwd, TrainSelect: slct&#125;</span><br><span class="line"><span class="keyword">if</span> modelDir != <span class="string">&quot;&quot;</span> &#123;</span><br><span class="line"><span class="keyword">return</span> m.saveTar(modelDir, tr.save) <span class="comment">// 保存模型到本地文件夹</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> m.save(db, tr.save) <span class="comment">// 保存模型到数据库</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到, 进入训练过程之后, 先做了<strong>语法校验</strong>, 然后<strong>生成的对应的TF的代码</strong>, 然后调用<code>tensorflowCmd</code><strong>执行命令</strong>, 最后将<strong>模型保存</strong>完毕, 完成训练过程.</p><p><strong>语法校验</strong>先略过, <strong>代码生成</strong>过程比较复杂后面再介绍, 我们先关注于<strong>命令执行</strong>过程:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">tensorflowCmd</span><span class="params">(cwd, driverName <span class="type">string</span>)</span></span> (cmd *exec.Cmd) &#123;</span><br><span class="line"><span class="keyword">if</span> hasPython() &amp;&amp; hasTensorFlow() &amp;&amp; hasDatabaseConnector(driverName) &#123;</span><br><span class="line">log.Printf(<span class="string">&quot;tensorflowCmd: run locally&quot;</span>)</span><br><span class="line">cmd = exec.Command(<span class="string">&quot;python&quot;</span>, <span class="string">&quot;-u&quot;</span>)</span><br><span class="line">cmd.Dir = cwd</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> hasDocker() &#123;</span><br><span class="line">log.Printf(<span class="string">&quot;tensorflowCmd: run in Docker container&quot;</span>)</span><br><span class="line"><span class="keyword">const</span> tfImg = <span class="string">&quot;sqlflow/sqlflow&quot;</span></span><br><span class="line"><span class="keyword">if</span> !hasDockerImage(tfImg) &#123;</span><br><span class="line">log.Printf(<span class="string">&quot;No local Docker image %s.  It will take a long time to pull.&quot;</span>, tfImg)</span><br><span class="line">&#125;</span><br><span class="line">cmd = exec.Command(<span class="string">&quot;docker&quot;</span>, <span class="string">&quot;run&quot;</span>, <span class="string">&quot;--rm&quot;</span>,</span><br><span class="line">fmt.Sprintf(<span class="string">&quot;-v%s:/work&quot;</span>, cwd),</span><br><span class="line"><span class="string">&quot;-w/work&quot;</span>, <span class="string">&quot;--network=host&quot;</span>, <span class="string">&quot;-i&quot;</span>, tfImg, <span class="string">&quot;python&quot;</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">log.Fatalf(<span class="string">&quot;No local TensorFlow or Docker.  No way to run TensorFlow programs&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> cmd</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>tensorflowCmd</code>有两种执行模式: <strong>本地执行</strong>和<strong>容器执行</strong>, 目前这两种方式都是单机执行模型, 实际上这儿就印证了<strong>AI Engine和SQL Engine分离</strong>的架构</p><p>未来这儿可以很方便的将这个扩展为分布式任务, 例如Kubeflow的TFJob,这个这块需要跟代码生成那儿一起修改.</p><h3 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a>代码生成</h3><p>让我们在回到<code>genTF</code>这个函数</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">genTF</span><span class="params">(w io.Writer, pr *extendedSelect, ds *trainAndValDataset, fts fieldTypes, db *DB)</span></span> <span class="type">error</span> &#123;</span><br><span class="line">r, e := newFiller(pr, ds, fts, db) <span class="comment">// 应该是按照字段的过滤吧, 没仔细看, 可能会出错</span></span><br><span class="line"><span class="keyword">if</span> e != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> e</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> pr.train &#123;</span><br><span class="line"><span class="keyword">return</span> tfTrainTemplate.Execute(w, r) <span class="comment">// 根据训练模板生成code</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> tfPredTemplate.Execute(w, r) <span class="comment">// 根据推理模板生成code</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> tfTrainTemplate = template.Must(template.New(<span class="string">&quot;codegenTfTrain&quot;</span>).Parse(tfTrainTemplateText)) <span class="comment">// 训练模板</span></span><br><span class="line"><span class="keyword">var</span> tfPredTemplate = template.Must(template.New(<span class="string">&quot;codegenTfPred&quot;</span>).Parse(tfPredTemplateText)) <span class="comment">// 推理模板</span></span><br></pre></td></tr></table></figure><p>这儿实际上最关键的是两个训练模板, 这两个模块在<code>template_tf.go</code>里面定义</p><blockquote><p>除了TF的模板, 还有<code>template_alps</code>和<code>template_elasticdl</code>这两个</p></blockquote><p>模板里面就是Python代码了, 截取里面的一部分说明一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># Disable Tensorflow INFO and WARNING logs</span></span><br><span class="line">os.environ[<span class="string">&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;</span>] = <span class="string">&#x27;3&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys, json</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> sqlflow_models <span class="comment"># 注意点1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sqlflow_submitter.db <span class="keyword">import</span> connect, db_generator <span class="comment"># 注意点2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略一部分代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">input_fn</span>(<span class="params">datasetStr</span>):</span><br><span class="line">    feature_types = []</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> feature_column_names:</span><br><span class="line">        <span class="keyword">if</span> feature_metas[name][<span class="string">&quot;is_sparse&quot;</span>]:</span><br><span class="line">            feature_types.append((tf.int64, tf.int32, tf.int64))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            feature_types.append(get_dtype(feature_metas[name][<span class="string">&quot;dtype&quot;</span>]))</span><br><span class="line"></span><br><span class="line">    gen = db_generator(driver, conn, datasetStr, feature_column_names, <span class="string">&quot;&#123;&#123;.Y.FeatureName&#125;&#125;&quot;</span>, feature_metas)</span><br><span class="line">    dataset = tf.data.Dataset.from_generator(gen, (<span class="built_in">tuple</span>(feature_types), tf.&#123;&#123;.Y.Dtype&#125;&#125;)) <span class="comment"># 注意点3</span></span><br><span class="line">    ds_mapper = functools.partial(_parse_sparse_feature, feature_metas=feature_metas)</span><br><span class="line">    <span class="keyword">return</span> dataset.<span class="built_in">map</span>(ds_mapper)</span><br></pre></td></tr></table></figure><p><strong>注意点1</strong>是<code>sqlflow_models</code>, 这个是定义在同组织的<a href="https://github.com/sql-machine-learning/models">Model</a>里面,  目前只实现了2个实现<code>dnnclassifier</code>和<code>lstmclassifier</code>, 这里这个<code>dnnclassifier</code>就是试用里面<code>DNNClassifier</code>算法模型的定义所在</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/5.png" alt=""></p><p><strong>注意点2</strong>是<code>sqlflow_submitter</code>这个定义在<code>sql/python/sqlflow_submmiter</code>包目录下, 在这儿你可以执行本地的Python文件, 也可以定义自己的submit将<code>CodeGen</code>的代码当做客户端代码, 给远程的深度学习服务提交自己的学习任务. 同时这儿也定义了与<code>SQL Engine</code>的交互代码逻辑, 就是里面的<code>connect</code>和<code>db_generator</code></p><p><strong>注意点3</strong>关注于TensorFlow框架是如何读取从数据库里面的数据的, 使用的接口为<code>tf.data.Dataset.from_generator</code></p><p><strong>至此代码分析已经完成, 主流程已经明确了.</strong></p><h2 id="社区动态"><a href="#社区动态" class="headerlink" title="社区动态"></a>社区动态</h2><p>蚂蚁对于这个项目的投入还是很大的, 应该由专门的人在投入这个项目, 更新频率还是相当快的</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/sqlflow/6.png" alt=""></p><p>但是贡献者还是比较少的, 应该目前看只有21贡献者, 基本上是蚂蚁金服内部员工.</p><p>之前看到他们2019年的<a href="https://github.com/sql-machine-learning/sqlflow/issues/327">路标</a>, 2019年的目标预定是支持各种框架, 例如<strong>Calcite支持</strong>或者<strong>GPU TF支持</strong></p><p>总体来说2019年主要在完善功能, 但是后来这个ISSUE关闭了, 不确定19年能否完成这些内容.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>SQLFlow目前看还处于原型阶段, 整体支持的能力还非常欠缺: GPU的支持, 模型的定义等功能目前好像都不具备.</p><p>就目前整体的设计, 我认为以下三点未来需要加强:</p><ol><li>模型定义接口太复杂了: 1. 实现一个模板;2. 实现一个模型算法;3. 实现一个submitter. 这套逻辑对于工程师可能比较简单(实际上定义地方太多, 也麻烦), 但是对于AI算法的人, 肯定不是用</li><li><code>AI Engine</code>和<code>SQL Engine</code>分离带来的性能问题, 这套架构的问题就是AI系统离数据远了一点, 所有数据都是通过<code>SQL Engine</code>计算而来, 而且是通过RPC获取的数据, 比起Spark这种直接在内存中获取数据, 这里会成为正式商用时候的大瓶颈点</li><li>数据分布式化工程量太大, 这其实是问题二的引申版, 未来肯定要实现数据分布化, AI计算分布式化, 这钟模式我还没有想到如何分布式化(<em>回去再好好想想</em>)</li></ol><p>另外对于这个项目的商业前景如何, 这个确实存疑的, 也许在阿里内部可能有这部分需求, 但对我们来说却不是.</p><p>2018年8月BigQuery出了ML之后, 我们也有计划跟进, 调研了<a href="https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html">XGBoost On Spark</a>, 但最后还是没决定要做, 最主要的原因是没有客户明确需要这个能力.</p><blockquote><p> 值得表扬的是: 蚂蚁的文档和demo做的是真好, 做技术调研能遇到这样的项目, 确实让我省了不少功夫. </p></blockquote><h2 id="附录-Go语言环境安装"><a href="#附录-Go语言环境安装" class="headerlink" title="附录: Go语言环境安装"></a>附录: Go语言环境安装</h2><blockquote><p>SQLFlow的主编程语言为Go语言, 安装部署也相对方便, 不记录过程了, 只放置一些安装资源链接</p></blockquote><p><a href="https://dl.google.com/go/go1.12.9.windows-amd64.msi">Go安装包</a></p><p><a href="https://www.jetbrains.com/go/download/#section=windows">GoLand下载地址</a></p><p><a href="http://idea.lanyus.com/">GoLand破解</a></p><p><a href="http://blog.lanyus.com/archives/174.html">IDEA License Server搭建</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]番外:TensorServing例子</title>
      <link href="/2019/08/11/Kubeflow%E7%B3%BB%E5%88%97-%E7%95%AA%E5%A4%96-TensorServing%E4%BE%8B%E5%AD%90/"/>
      <url>/2019/08/11/Kubeflow%E7%B3%BB%E5%88%97-%E7%95%AA%E5%A4%96-TensorServing%E4%BE%8B%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>最近做产品的时候, 由算法团队写完AI算法模型, 由我们产品团队完成对工程化的改造, 然后再把API上线.</p><p>但是这个时候遇到沟通成本很高, AI算法大多是探索式的,  输入是比较少边的, 但是输出可能会多次变化, 这样对产品化人员又很麻烦, 本来就是很简单的代码, 就是需要不停的修改, 用来适配整体的API.</p><blockquote><p>这里有个前提: 尽量让算法的人员减少对产品的代码的感知, 毕竟他们不是专业的</p></blockquote><p>因此, 我查了一下开源社区对这个问题的处理方式, 最后还真查到了<code>TF Serving</code>这个工程, 决定调研一下是否能够解决我的问题.</p><p><code>TF Serving</code>是<a href="https://www.tensorflow.org/tfx">TensorFlow Extended</a>工程的一个子工程, <code>TFX</code>是一个端到端的AI平台, 简单理解就是:</p><p><strong>他的关注点在于AI机器学习的工程化部分</strong></p><p><code>TFX</code>总共有4个模块:</p><ol><li>TensorFlow Data Validation: <em>TensorFlow Data Validation (TFDV) 能够帮助开发者大规模地理解、验证和监控机器学习数据。Google 每天都使用 TFDV 分析和验证 PB 级的数据，并且在帮助 TFX 用户维护机器学习流水线正常运行方面，TFDV 一贯表现良好</em></li><li>TensorFlow Transform: <em>在将机器学习应用于现实世界的数据集时，需要投入很多精力才能将数据预处理为合适的格式，其中包括在各种格式之间进行转换、对文本进行标记化和词干化、创建词汇表、执行归一化等各种数字操作。您可以使用 tf.Transform 完成所有这些操作</em></li><li>TensorFlow Model Analysis: <em>TensorFlow Model Analysis (TFMA) 让开发者能够计算和可视化模型的评估指标。在部署任何机器学习 (ML) 模型之前，机器学习开发者需要评估模型的性能，以确保其达到特定的质量阈值，并且能够针对所有相关数据切片展示出与预期相符的行为。例如，模型针对整个评估数据集的 AUC 可能是可接受的，但针对特定切片却表现不佳。TFMA 为开发者提供了深入了解其模型性能的工具</em></li><li>TensorFlow Serving: <em>机器学习 (ML) 应用系统必须支持模型版本控制（用于实现包含回滚选项的模型更新）和多个模型（用于实现通过 A/B 测试进行的实验），同时还要确保并发模型能够在硬件加速器（GPU 和 TPU）上以较低的延迟实现较高的吞吐量。TensorFlow Serving 每秒能为 Google 处理数千万次推断</em></li></ol><p>这4个模块都是很有用的点, 但这次我的重点在<code>TensorFlow Serving</code></p><h2 id="TensorFlow-Serving"><a href="#TensorFlow-Serving" class="headerlink" title="TensorFlow Serving"></a>TensorFlow Serving</h2><p>在网上的教程之中, 就一个<strong>提供模型</strong>的<a href="https://www.tensorflow.org/tfx/tutorials/serving/rest_simple">章节</a>详细介绍了<code>TF Serving</code>的一个例子</p><p>按照Notebook的教程运行一下这个例子(不需要使用GPU, 使用CPU的速度就可以了), 我们简单的分解一下这个过程</p><blockquote><p>直接在本地安装一个Notebook就可以运行, 除了安装<code>tensorflow-model-server</code>可能需要翻墙, 需要手动设置一起, 其他的可以直接运行出来</p></blockquote><p>训练过程:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line">model = keras.Sequential([</span><br><span class="line">  keras.layers.Conv2D(input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>), filters=<span class="number">8</span>, kernel_size=<span class="number">3</span>, </span><br><span class="line">                      strides=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>, name=<span class="string">&#x27;Conv1&#x27;</span>),</span><br><span class="line">  keras.layers.Flatten(),</span><br><span class="line">  keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax, name=<span class="string">&#x27;Softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">testing = <span class="literal">False</span></span><br><span class="line">epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.train.AdamOptimizer(), </span><br><span class="line">              loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">model.fit(train_images, train_labels, epochs=epochs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用测试集推理</span></span><br><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\nTest accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(test_acc))</span><br></pre></td></tr></table></figure><p>实际上在<code>模型定义</code>的步骤之中, 模型的输入输出实际上已经定了,  但是这里的<strong>输入应该只能是tensor类型的</strong></p><p>使用工具, 查看查看模型输入输出:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!saved_model_cli show --<span class="built_in">dir</span> &#123;export_path&#125; --all</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">MetaGraphDef with tag-set: &#x27;serve&#x27; contains the following SignatureDefs:</span><br><span class="line"></span><br><span class="line">signature_def[&#x27;serving_default&#x27;]:</span><br><span class="line">  The given SavedModel SignatureDef contains the following input(s):</span><br><span class="line">    inputs[&#x27;input_image&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 28, 28, 1)</span><br><span class="line">        name: Conv1_input:0</span><br><span class="line">  The given SavedModel SignatureDef contains the following output(s):</span><br><span class="line">    outputs[&#x27;Softmax/Softmax:0&#x27;] tensor_info:</span><br><span class="line">        dtype: DT_FLOAT</span><br><span class="line">        shape: (-1, 10)</span><br><span class="line">        name: Softmax/Softmax:0</span><br><span class="line">  Method name is: tensorflow/serving/predict</span><br></pre></td></tr></table></figure><p>创建Serving时候, 参数只要指定模型的路径即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> tensorflow_model_server \</span><br><span class="line">  --rest_api_port=8501 \</span><br><span class="line">  --model_name=fashion_model \</span><br><span class="line">  --model_base_path=<span class="string">&quot;<span class="variable">$&#123;MODEL_DIR&#125;</span>&quot;</span> &gt;server.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p>使用客户端, 给<code>8501</code>端口发送请求, 地址为<code>http://localhost:8501/v1/models/fashion_model:predict</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">headers = &#123;<span class="string">&quot;content-type&quot;</span>: <span class="string">&quot;application/json&quot;</span>&#125;</span><br><span class="line">json_response = requests.post(<span class="string">&#x27;http://localhost:8501/v1/models/fashion_model:predict&#x27;</span>, data=data, headers=headers)</span><br><span class="line">predictions = json.loads(json_response.text)[<span class="string">&#x27;predictions&#x27;</span>]</span><br><span class="line"></span><br><span class="line">show(<span class="number">0</span>, <span class="string">&#x27;The model thought this was a &#123;&#125; (class &#123;&#125;), and it was actually a &#123;&#125; (class &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">  class_names[np.argmax(predictions[<span class="number">0</span>])], test_labels[<span class="number">0</span>], class_names[np.argmax(predictions[<span class="number">0</span>])], test_labels[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p>除了HTTP请求, <code>TFServing</code>还支持<code>GRPC</code>协议, 端口默认开在<code>8500</code>端口</p><p>GRPC的例子可以看github上的<a href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/example/mnist_client.py">例子</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>从这个例子上来看, <code>TF Serving</code>并不能完全解决上面的问题, 因为模型的输入输出的定义肯定是矩阵模式, 而实际产品之中的输入是非常多样化的, 可以是一个npp文件也可以是一张图片, 显然还需要一个预处理的步骤,才能达到Serving的步骤.</p><p>但<code>TF Serving</code>也是用的:</p><ol><li>不需要写模型推理代码了, 只要统一使用了TensorFlow框架, 推理就简单了</li><li>它有简单的模型版本控制, 用于不同版本之间的升级测试等问题(实际上就是一个简单的版本编号而已)</li></ol><p>那么回过头来, 如果想要解决产品和算法团队之间的GAP呢?</p><p>我想到的方式是, <strong>使用Kubeflow的Pipeline + TFServing</strong>:</p><ol><li>将常见的输入输出算法转化为component, 直接可以使用</li><li>允许算法人员自己定义component, 使用方式和本地变成类似</li><li>Pipeline能够可视化的将各个component连接在一起</li><li>模型推理使用<code>TFServing</code>发布</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装nvidia-docker</title>
      <link href="/2019/08/11/%E5%AE%89%E8%A3%85nvidia-docker/"/>
      <url>/2019/08/11/%E5%AE%89%E8%A3%85nvidia-docker/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>测试环境为: ubuntu == 18.04</p></blockquote><p>官方文档: <a href="https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0">https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0</a>)</p><blockquote><p>ubuntu18.04版本需要安装的是V2.0版本, 网上文章大多是V1.0的, 在这个版本的ubutnu无法安装</p></blockquote><p>安装命令: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加源</span></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \</span><br><span class="line">  sudo apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> $ID<span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | \</span><br><span class="line">  sudo <span class="built_in">tee</span> /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">sudo apt-get install nvidia-docker2</span><br><span class="line"><span class="comment"># 重启</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>启动镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-docker run -p 8888:8888   --dns 8.8.8.8 --dns 8.8.4.4 gcr.io/kubeflow-images-public/tensorflow-1.12.0-notebook-gpu:v0.5.0</span><br></pre></td></tr></table></figure><blockquote><p>—dns 8.8.8.8 —dns 8.8.4.4指定公网DNS, 不然容器里面无法访问网络</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker设置配置代理</title>
      <link href="/2019/08/11/Docker%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86/"/>
      <url>/2019/08/11/Docker%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E4%BB%A3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>测试环境为: ubuntu == 18.04,  docker == 19.03.1</p></blockquote><p>有些docker容器是在Google Cloud上的是, 因此需要下载的时候, 需要配置代理才能访问</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 创建docker启动文件夹</span></span><br><span class="line">sudo <span class="built_in">mkdir</span> -p /etc/systemd/system/docker.service.d</span><br><span class="line"><span class="comment">## 创建proxy配置文件</span></span><br><span class="line">sudo vim /etc/systemd/system/docker.service.d/http-proxy.conf</span><br></pre></td></tr></table></figure><p>写入以下配置项</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">Environment=<span class="string">&quot;HTTP_PROXY=http://127.0.0.1:8118/&quot;</span> <span class="string">&quot;HTTPS_PROXY=http://127.0.0.1:8118/&quot;</span> <span class="string">&quot;NO_PROXY=localhost,127.0.0.1,registry.docker-cn.com,hub-mirror.c.163.com&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p><code>http://127.0.0.1:8118</code>是shadowsocks转出来的http端口</p><p>不需要走代理的镜像仓库, 在<code>NO_PROXY</code>里配置</p></blockquote><p>配置生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 刷新配置项</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line"><span class="comment"># 重启docker服务</span></span><br><span class="line">sudo systemctl restart docker</span><br><span class="line"><span class="comment"># 查看docker配置项</span></span><br><span class="line">sudo systemctl show --property=Environment docker</span><br></pre></td></tr></table></figure><p>测试是否生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull gcr.io/kubeflow-images-public/tensorflow-1.12.0-notebook-cpu:v0.5.0</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>翻墙代理设置</title>
      <link href="/2019/08/03/%E7%BF%BB%E5%A2%99%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/"/>
      <url>/2019/08/03/%E7%BF%BB%E5%A2%99%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近从同事手中借了一个<code>ShadowSocks</code>的账号, 用来翻墙使用, Windows和Mac使用的时候都比较简单, 因为有图形化的页面, 但是在Ubuntu上遇到不大不小的问题, 特意记录一下.</p><h2 id="安装客户端"><a href="#安装客户端" class="headerlink" title="安装客户端"></a>安装客户端</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果没有安装过pip, 请先安装</span></span><br><span class="line">sudo apt install python-pip</span><br><span class="line">sudo pip install shadowsocks</span><br><span class="line">sudo vim ~/shadowsocks.json</span><br></pre></td></tr></table></figure><p>填入你服务端的配置项:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">   <span class="attr">&quot;server&quot;</span><span class="punctuation">:</span><span class="string">&quot;远端ip&quot;</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;server_port&quot;</span><span class="punctuation">:</span><span class="number">2443</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;local_port&quot;</span><span class="punctuation">:</span><span class="number">1080</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span><span class="string">&quot;密码&quot;</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;timeout&quot;</span><span class="punctuation">:</span><span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">   <span class="attr">&quot;method&quot;</span><span class="punctuation">:</span><span class="string">&quot;aes-256-cfb&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>启动客户端</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sslocal -c ~/shadowsocks.json</span><br></pre></td></tr></table></figure><p>这个时候, 如果你的shadowsocks版本是2.8.2的话, 就会出现以下的异常信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undefined symbol EVP_CIPHER_CTX_cleanup</span><br></pre></td></tr></table></figure><p>找到安装目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip list -v|grep shadowsocks</span><br><span class="line"><span class="comment"># shadowsocks       2.8.2             /home/wiiliam/miniconda3/lib/python3.6/site-packages                      pip </span></span><br><span class="line"><span class="built_in">cd</span> /home/wiiliam/miniconda3/lib/python3.6/site-packages/shadowsocks</span><br></pre></td></tr></table></figure><p>进入到安装后执行以下命令替换py的源码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&quot;s/cleanup/reset/g&quot;</span>  crypto/openssl.py</span><br></pre></td></tr></table></figure><p>重新之后, 看到端口绑定日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">019-08-03 23:01:59 WARNING  warning: your <span class="built_in">timeout</span> 30 seems too short</span><br><span class="line">2019-08-03 23:01:59 INFO     loading libcrypto from libcrypto.so.1.1</span><br><span class="line">2019-08-03 23:01:59 INFO     starting <span class="built_in">local</span> at 127.0.0.1:1080</span><br></pre></td></tr></table></figure><blockquote><p>注意: 在linux上1080端口监听的类型是<code>socks5</code>, 而如果使用windows版本, 1080监听的是<code>http</code>类型. 这个时候使用<code>lsof -i:1080</code>是看不到socks5监听进程的.</p></blockquote><h2 id="Chrome代理设置"><a href="#Chrome代理设置" class="headerlink" title="Chrome代理设置"></a>Chrome代理设置</h2><p>Chrome有比较好的插件: <code>SwitchyOmega</code></p><p>在配置页面直接导入配置项:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/shadowsocks/1.png" alt=""></p><p>然后修改<code>FWWed</code>的代理端口已经被正确设置</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/shadowsocks/2.png" alt=""></p><p>最后在Chrome的插件按钮上切换地址的时候, 使用<code>自动切换</code>即可</p><h2 id="Socks5代理转HTTP代理"><a href="#Socks5代理转HTTP代理" class="headerlink" title="Socks5代理转HTTP代理"></a>Socks5代理转HTTP代理</h2><p>国产的浏览器一般是不支持socks代理的, 所以需要讲sock5代理转为http代理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install privoxy</span><br><span class="line">sudo vi /etc/privoxy/config</span><br></pre></td></tr></table></figure><p>修改以下的配置项:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">isten-address localhost:8118</span><br><span class="line">forward-socks5t / 127.0.0.1:1080 .</span><br></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /etc/init.d/privoxy restart</span><br></pre></td></tr></table></figure><h2 id="开机启动"><a href="#开机启动" class="headerlink" title="开机启动"></a>开机启动</h2><blockquote><p>ubuntu18.04不再使用initd管理系统，改用systemd,为了像以前一样，在/etc/rc.local中设置开机启动程序，需要以下几步：systemd默认读取/etc/systemd/system下的配置文件，该目录下的文件会链接/lib/systemd/system/下的文件。一般系统安装完/lib/systemd/system/下会有rc-local.service文件，即我们需要的配置文件</p></blockquote><p>链接服务并修改服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">ln</span> -fs /lib/systemd/system/rc-local.service /etc/systemd/system/rc-local.service</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /etc/systemd/system/</span><br><span class="line">sudo vim rc-local.service</span><br></pre></td></tr></table></figure><p>加入部分设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  SPDX-License-Identifier: LGPL-2.1+</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  This file is part of systemd.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#  systemd is free software; you can redistribute it and/or modify it</span></span><br><span class="line"><span class="comment">#  under the terms of the GNU Lesser General Public License as published by</span></span><br><span class="line"><span class="comment">#  the Free Software Foundation; either version 2.1 of the License, or</span></span><br><span class="line"><span class="comment">#  (at your option) any later version.</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># This unit gets pulled automatically into multi-user.target by</span></span><br><span class="line"><span class="comment"># systemd-rc-local-generator if /etc/rc.local is executable.</span></span><br><span class="line">[Unit]</span><br><span class="line">Description=/etc/rc.local Compatibility</span><br><span class="line">Documentation=man:systemd-rc-local-generator(8)</span><br><span class="line">ConditionFileIsExecutable=/etc/rc.local</span><br><span class="line">After=network.target</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/etc/rc.local start</span><br><span class="line">TimeoutSec=0</span><br><span class="line">RemainAfterExit=<span class="built_in">yes</span></span><br><span class="line">GuessMainPID=no</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 这部分要新添加</span></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">Alias=rc-local.service</span><br></pre></td></tr></table></figure><p>创建<code>/etc/rc.local</code>文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/rc.local</span><br></pre></td></tr></table></figure><p>填入以下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh -e</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># rc.local</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># This script is executed at the end of each multiuser runlevel.</span></span><br><span class="line"><span class="comment"># Make sure that the script will &quot;exit 0&quot; on success or any other</span></span><br><span class="line"><span class="comment"># value on error.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># In order to enable or disable this script just change the execution</span></span><br><span class="line"><span class="comment"># bits.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># By default this script does nothing.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 填写脚本区域</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;test&quot;</span> &gt; /var/log/ss.log</span><br><span class="line">/home/wiiliam/miniconda3/bin/python /home/wiiliam/miniconda3/bin/sslocal -c /home/wiiliam/shadowsocks.json &gt; /home/wiiliam/shadowsocks.log 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="comment">### 脚本区域结束</span></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><blockquote><p>注意这个脚本是以root用户启动的, 所以一定要确保root一定能访问相应的文件</p></blockquote><p>添加执行权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">chown</span> 755 /etc/rc.local</span><br></pre></td></tr></table></figure><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://kionf.com/2016/12/15/errornote-ss/">SS客户端异常</a></p><p><a href="https://cao0507.github.io/2018/08/21/ubuntu%E9%85%8D%E7%BD%AEshadowsocks%E5%AE%A2%E6%88%B7%E7%AB%AF/">HTTP代理</a></p><p><a href="https://github.com/FelisCatus/SwitchyOmega/wiki/GFWList">Chrome设置SwitchyOmega</a></p><p><a href="https://blog.csdn.net/zhengchaooo/article/details/80202599">开机启动设置</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes知识点</title>
      <link href="/2019/08/01/Kubernetes%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2019/08/01/Kubernetes%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="污点和容忍"><a href="#污点和容忍" class="headerlink" title="污点和容忍"></a>污点和容忍</h2><p>如果你不想在某个节点上, 执行任何Pod, 你可以通过Kubernetes的<strong>Taints</strong>特性实现</p><p>每个污点的组成如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">key=value:effect</span><br></pre></td></tr></table></figure><p>每个污点有一个key和value作为污点的标签，其中value可以为空，effect描述污点的作用。当前taint effect支持如下三个选项：</p><ul><li><code>NoSchedule</code>：表示k8s将不会将Pod调度到具有该污点的Node上</li><li><code>PreferNoSchedule</code>：表示k8s将尽量避免将Pod调度到具有该污点的Node上</li><li><code>NoExecute</code>：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去</li></ul><p><strong>污点的设置和去除</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置污点</span></span><br><span class="line">kubectl taint nodes node1 key1=value1:NoSchedule</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除污点</span></span><br><span class="line">kubectl taint nodes node1 key1:NoSchedule-</span><br></pre></td></tr></table></figure><p><strong>容忍</strong></p><p>设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。 但我们可以在Pod上设置容忍(Toleration)，意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上。</p><p>通过在Pod的spec中设置tolerations字段，给Pod设置上容忍点Toleration：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;key1&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;value1&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">  <span class="attr">tolerationSeconds:</span> <span class="number">3600</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;key1&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;value1&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;key2&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure><p><strong>参考</strong></p><p><a href="https://blog.frognew.com/2018/05/taint-and-toleration.html">博客</a></p><h2 id="Kubernetes的调度器"><a href="#Kubernetes的调度器" class="headerlink" title="Kubernetes的调度器"></a>Kubernetes的调度器</h2><p>Kubernetes的默认调度器叫做<code>kube-scheduler</code>, 但是你可以自己实现一个调度器, 例如<a href="https://github.com/volcano-sh/volcano">volcano</a></p><p>创建Pod的时候可以指定<code>spec.schedulerName</code>为自己的调度器</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">annotation-second-scheduler</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">multischeduler-example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedulerName:</span> <span class="string">my-scheduler</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pod-with-second-annotation-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/pause:2.0</span></span><br></pre></td></tr></table></figure><p><strong>参考</strong></p><p><a href="https://k8smeetup.github.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">配置多个调度器</a></p><p><a href="https://zhuanlan.zhihu.com/p/47047550">如何实现一个调度器</a></p><h2 id="Kubernetes-Operator入门"><a href="#Kubernetes-Operator入门" class="headerlink" title="Kubernetes Operator入门"></a>Kubernetes Operator入门</h2><p><a href="https://www.qikqiak.com/post/k8s-operator-101/">参考</a></p><h2 id="强制删除资源"><a href="#强制删除资源" class="headerlink" title="强制删除资源"></a>强制删除资源</h2><p>问题：kubelet delete pod之后总处于Terminating，无法移除</p><p>解决：加参数 —force —grace-period=0，grace-period表示过渡存活期，默认30s，在删除POD之前允许POD慢慢终止其上的容器进程，从而优雅退出，0表示立即终止POD</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete po -n --force --grace-period=0</span><br></pre></td></tr></table></figure><h2 id="kubernetes的ipvs模式和iptables模式"><a href="#kubernetes的ipvs模式和iptables模式" class="headerlink" title="kubernetes的ipvs模式和iptables模式"></a>kubernetes的ipvs模式和iptables模式</h2><h3 id="什么是IPVS？"><a href="#什么是IPVS？" class="headerlink" title="什么是IPVS？"></a>什么是IPVS？</h3><p>IPVS (IP Virtual Server，IP虚拟服务器)是基于Netfilter的、作为linux内核的一部分实现传输层负载均衡的技术，通常称为第4层LAN交换。</p><p>IPVS集成在LVS(Linux Virtual Server)中，它在主机中运行，并在真实服务器集群前充当负载均衡器。IPVS可以将对TCP/UDP服务的请求转发给后端的真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。因此IPVS天然支持Kubernetes Service。</p><h3 id="为什么选择IPVS"><a href="#为什么选择IPVS" class="headerlink" title="为什么选择IPVS"></a>为什么选择IPVS</h3><p>随着kubernetes使用量的增长，其资源的可扩展性变得越来越重要。特别是对于使用kubernetes运行大型工作负载的开发人员或者公司来说，service的可扩展性至关重要。</p><p>kube-proxy是为service构建路由规则的模块，之前依赖iptables来实现主要service类型的支持，比如(ClusterIP和NodePort)。但是iptables很难支持上万级的service，因为iptables纯粹是为防火墙而设计的，并且底层数据结构是内核规则的列表。</p><p>kubernetes早在1.6版本就已经有能力支持5000多节点，这样基于iptables的kube-proxy就成为集群扩容到5000节点的瓶颈。举例来说，如果在一个5000节点的集群，我们创建2000个service，并且每个service有10个pod，那么我们就会在每个节点上有至少20000条iptables规则，这会导致内核非常繁忙。</p><p>基于IPVS的集群内负载均衡就可以完美的解决这个问题。IPVS是专门为负载均衡设计的，并且底层使用哈希表这种非常高效的数据结构，几乎可以允许无限扩容。</p><h3 id="IPVS-vs-IPTABLES区别"><a href="#IPVS-vs-IPTABLES区别" class="headerlink" title="IPVS vs. IPTABLES区别"></a>IPVS vs. IPTABLES区别</h3><p>IPVS模式在Kubernetes v1.8中引入，并在v1.9中进入了beta。 1.11中实现了GA(General Availability)。IPTABLES模式在v1.1中添加，并成为自v1.2以来的默认操作模式。 IPVS和IPTABLES都基于netfilter。 IPVS模式和IPTABLES模式之间的差异如下：</p><ul><li>IPVS为大型集群提供了更好的可扩展性和性能。</li><li>IPVS支持比iptables更复杂的负载平衡算法（最小负载，最少连接，位置，加权等）。</li><li>IPVS支持服务器健康检查和连接重试等。</li></ul><p><a href="https://blog.csdn.net/fanren224/article/details/86548398">参考1</a></p><p><a href="https://zhuanlan.zhihu.com/p/37230013">参考2</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华为云上安装helm</title>
      <link href="/2019/08/01/%E5%8D%8E%E4%B8%BA%E4%BA%91%E4%B8%8A%E5%AE%89%E8%A3%85helm/"/>
      <url>/2019/08/01/%E5%8D%8E%E4%B8%BA%E4%BA%91%E4%B8%8A%E5%AE%89%E8%A3%85helm/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Helm是Kubernetes生态系统中的一个软件包管理工具.</p><p>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</p><p>除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。</p><h2 id="Helm概念"><a href="#Helm概念" class="headerlink" title="Helm概念"></a>Helm概念</h2><ul><li>Helm: Kubernetes的应用打包工具，也是命令行工具的名称。</li><li>Tiller: Helm的服务端，部署在Kubernetes集群中，用于处理Helm的相关命令。</li><li>Chart: Helm的打包格式，内部包含了一组相关的kubernetes资源。</li><li>Repoistory: Helm的软件仓库，repository本质上是一个web服务器，该服务器保存了chart软件包以供下载，并有提供一个该repository的chart包的清单文件以供查询。在使用时，Helm可以对接多个不同的Repository。</li><li>Release: 使用Helm install命令在Kubernetes集群中安装的Chart称为Release。</li></ul><p>下图展示这个概念之间的联系:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/helm-architecture.png" alt=""></p><h2 id="华为云上安装"><a href="#华为云上安装" class="headerlink" title="华为云上安装"></a>华为云上安装</h2><h3 id="Helm安装"><a href="#Helm安装" class="headerlink" title="Helm安装"></a>Helm安装</h3><p>从Github的<a href="https://github.com/helm/helm/releases">发布页</a>或者<a href="https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz">GoogleCloud</a>下载</p><blockquote><p>helm v2.11版本在K8s v1.11.7测试通过</p></blockquote><p>上传到客户端节点后, 解压并将文件拷贝到系统路径之中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xzvf helm-v2.11.0-linux-amd64.tar.gz</span><br><span class="line"><span class="built_in">mv</span> linux-amd64/helm /usr/local/bin/helm</span><br></pre></td></tr></table></figure><h3 id="Helm初始化"><a href="#Helm初始化" class="headerlink" title="Helm初始化"></a>Helm初始化</h3><p>创建一个tiller的service_account: <code>tiller_service_account.yaml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>创建角色</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f tiller_service_account.yaml</span><br></pre></td></tr></table></figure><p>Helm初始化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init --service-account tiller --skip-refresh</span><br><span class="line"><span class="comment">## 更新镜像地址, 最好你从官网下载到tiller镜像, 再将镜像传入到华为云的SWR, 此处填写SWR地址</span></span><br><span class="line">helm init --tiller-image swr.cn-north-1.myhuaweicloud.com/hzw/tiller:v2.11.0 --upgrade </span><br></pre></td></tr></table></figure><blockquote><p>docker tag registry.cn-hangzhou.aliyuncs.com/acs/tiller:v2.11.0 swr.cn-north-1.myhuaweicloud.com/hzw/tiller:v2.11.0</p></blockquote><p>执行<code>helm version</code>查看结果, 发现<code>Error: could not find a ready tiller pod</code>错误</p><p>查找原因</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod --all-namespaces|grep tiller</span><br><span class="line"><span class="comment"># 如果没找到pod, 说明前面的步骤还有问题</span></span><br><span class="line">kubectl describe pod <span class="variable">$Pod_Name</span> -n kube-system</span><br></pre></td></tr></table></figure><p>发现问题还是<code>Error: ImagePullBackOff</code></p><p>问题和之前一样, 华为云的k8s拉取镜像需要<code>imagePullSecret</code></p><p>编辑<code>tiller-deploy</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit deploy tiller-deploy -n kube-system</span><br></pre></td></tr></table></figure><p>将以下内容加入到对应的模块之中</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">imagePullSecrets:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">default-secret</span></span><br></pre></td></tr></table></figure><p>过段时间之后,发现tiller已经正常</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@/home]<span class="comment"># kubectl get pod -n kube-system|grep tiller</span></span><br><span class="line">tiller-deploy-759d874f-4dgmv     1/1       Running   0          56m</span><br></pre></td></tr></table></figure><p>Helm安装和初始化工作已经完成</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@gpu-01-client volcano]<span class="comment"># helm version</span></span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">&quot;v2.11.0&quot;</span>, GitCommit:<span class="string">&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;</span>, GitTreeState:<span class="string">&quot;clean&quot;</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">&quot;v2.11.0&quot;</span>, GitCommit:<span class="string">&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;</span>, GitTreeState:<span class="string">&quot;clean&quot;</span>&#125;</span><br></pre></td></tr></table></figure><h2 id="参考说明"><a href="#参考说明" class="headerlink" title="参考说明"></a>参考说明</h2><p><a href="https://blog.csdn.net/wzygis/article/details/84346573">安装指南</a></p><p><a href="https://support.huaweicloud.com/bestpractice-cce/cce_bestpractice_0024.html">华为云官网指南</a></p><p><a href="https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/">Helm介绍</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu使用Git报错</title>
      <link href="/2019/07/30/Ubuntu%E4%BD%BF%E7%94%A8Git%E6%8A%A5%E9%94%99/"/>
      <url>/2019/07/30/Ubuntu%E4%BD%BF%E7%94%A8Git%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近使用公司的开发机器(ubuntu系统), 执行<code>git clone</code>一直会出现如下错误:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">error: RPC failed; curl 56 GnuTLS recv error (-110): The TLS connection was non-properly terminated.</span><br></pre></td></tr></table></figure></p><p>据说产生这个错误的原因, ubuntu上使用的https认证组件有问题, 使用如下步骤可以解决该问题.</p><p>大部分的内容参考自<a href="https://zhuanlan.zhihu.com/p/53961303">这篇博客</a>,但是文章的步骤在公司的机器还是有一些问题,我会注明.</p><p><strong>安装必要依赖</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential fakeroot dpkg-dev</span><br><span class="line">sudo apt-get install libcurl4-openssl-dev</span><br></pre></td></tr></table></figure></p><p><strong>下载源码并重新编译安装</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建临时目录</span></span><br><span class="line"><span class="built_in">mkdir</span> ~/git-rectify</span><br><span class="line"><span class="built_in">cd</span> ~/git-rectify</span><br><span class="line"><span class="comment"># 下载源码</span></span><br><span class="line">apt-get <span class="built_in">source</span> git</span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">apt-get build-dep git</span><br><span class="line"><span class="comment"># 进入源码目录</span></span><br><span class="line"><span class="built_in">cd</span> git-2*</span><br><span class="line"><span class="comment"># 修改配置文件</span></span><br><span class="line">sed -i -- <span class="string">&#x27;s/libcurl4-gnutls-dev/libcurl4-openssl-dev/&#x27;</span> ./debian/control</span><br><span class="line">sed -i -- <span class="string">&#x27;/TEST\s*=\s*test/d&#x27;</span> ./debian/rules</span><br><span class="line"><span class="comment"># 重新编译(注意要加入`-uc -us`,不然会出现`secret key not available`错误)</span></span><br><span class="line">dpkg-buildpackage -rfakeroot -b -uc -us</span><br><span class="line"><span class="comment"># 返回上一级,重新安装</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">dpkg -i git_2*.deb</span><br></pre></td></tr></table></figure></p><p><strong>锁住git不更新</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果不锁定, 下次更新git的时候, 会重新覆盖</span></span><br><span class="line">sudo apt-mark hold git</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 软件安装 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>蓝绿发布/灰度发布和AB测试</title>
      <link href="/2019/07/30/%E8%93%9D%E7%BB%BF%E5%8F%91%E5%B8%83-%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E5%92%8CAB%E6%B5%8B%E8%AF%95/"/>
      <url>/2019/07/30/%E8%93%9D%E7%BB%BF%E5%8F%91%E5%B8%83-%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E5%92%8CAB%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>之前想做过一个多版本发布的系统,  当时了解了不少微服务部署的概念, 其中就包括蓝绿发布, 灰度发布以及AB测试.</p><p>但是当时对这些概念理会的不是很深, 大致认为他们都是差不多的东西. 这次研究ServiceMesh的时候, 这些概念又出现了, 所以现在写个文章把这些概念理清楚.</p><blockquote><p>内容不少节选自<a href="https://cloud.tencent.com/developer/article/1449209">这篇文章</a>和<a href="http://www.bewindoweb.com/231.html">这篇文章</a></p></blockquote><h2 id="蓝绿发布"><a href="#蓝绿发布" class="headerlink" title="蓝绿发布"></a>蓝绿发布</h2><blockquote><p>蓝绿部署中，一共有两套系统：一套是正在提供服务系统，标记为“绿色”；另一套是准备发布的系统，标记为“蓝色”。两套系统都是功能完善的，并且正在运行的系统，只是系统版本和对外服务情况不同。</p></blockquote><p>蓝绿发布最大的特点就是: </p><ol><li><strong>两套运行环境</strong></li><li><strong>流量一把切</strong></li></ol><p>特别要注意的, 两套运行环境只要是指<strong>无状态服务</strong>那个一部分,  对于原数据库或者其他有状态的实例, 始终使用的都是同一套系统.</p><blockquote><p>如果想要将这些<strong>有状态实例</strong>进行切换, 必须保证数据同步的, 这样来就必然会涉及到了CAP理论, 整个系统复杂性就高了.</p></blockquote><h2 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a>灰度发布</h2><blockquote><p>灰度发布又叫做<strong>金丝雀发布</strong>，以前矿工下矿洞前，会放一只金丝雀去试探是否有瓦斯（金丝雀对瓦斯很敏感），映射到这里就是先发布一小部分来试探整体是否能够正常运行，如果能正常运行则进行完全部署的发布方式，目前仍然是不少成长型技术组织的主流发布方式</p></blockquote><p>灰度发布最大特点是:</p><ol><li>流量渐切</li><li>影响可控</li></ol><p>蓝绿发布的时候, <strong>流量一把切</strong>导致变更出现严重bug的时候, 是无法评估影响的, 因为你无法知道变更时间窗口之中, 有多少用户使用了你的服务.</p><p>而灰度发布可以指定新版本的用户, 收集<strong>金丝雀</strong>的反馈意见, 然后进行决策是否放开流程, 或者回退版本.</p><h2 id="A-B测试"><a href="#A-B测试" class="headerlink" title="A/B测试"></a>A/B测试</h2><blockquote><p>A/B测试指的是同时上线V1和V2版本，根据一定条件将流量分别导入V1和V2版本，收集感兴趣的数据，来对比产品功能的效果。</p></blockquote><p>A/B测试经常会和灰度发布在一起说, 因为:</p><ol><li>两者都有多个版本(蓝绿发布一般只有两个版本, 而灰度可以存在多个版本)</li><li>两者都有流量路由控制</li></ol><p>但是A/B测试是一种测试方法, <strong>关注的是旧版本和新版本的效果好坏，比如流量转化率、用户体验等等</strong><br>因此, 两者不是一个维度的事情, 只是一般合起来使用: 应用通过<strong>灰度发布</strong>, 然后进行<strong>A/B测试</strong>,发现问题, 进行改进.</p><p>游戏领域经常有<strong>内测活动</strong>或者<strong>体验服</strong>, 是一个典型的A/B测试</p><blockquote><p>A/B测试存在的问题是<strong>金丝雀可能会死</strong>, 因此还有一种通过<strong>流量回放</strong>技术做的测试方法, 将现网的流程全部回放, 测试新版本的时候将现网的业务全部测试一遍. 但这种技术实现复杂, 因为客户的应用不一定全是幂等的, 所以必须同步备份数据库以及运行环境, 还得有环境版本控制. 总之, 不是那种质量非常高的应用, 一般不会去搞这么一套系统的.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>比较两种发布模式:</p><ol><li>蓝绿发布的实现更加简单, 但是部署资源占用多, 并且影响不可控</li><li>灰度发布的实现确实复杂, 但影响小, 部署也可以采用滚动升级的方式</li></ol><p>现在一般大多采用灰度发布为多.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kube-Debug定位工具</title>
      <link href="/2019/07/28/Kube-Debug%E5%AE%9A%E4%BD%8D%E5%B7%A5%E5%85%B7/"/>
      <url>/2019/07/28/Kube-Debug%E5%AE%9A%E4%BD%8D%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>之前逛技术博客的时候, 看了一个<code>kubectl-debug</code>的<a href="https://aleiwu.com/post/kubectl-debug-intro/">介绍</a>, 可以使用外部的工具, 来处理容器内部的信息, 这个能力非常有用. </p><p>因为容器制作的时候有个原则就是<strong>容器尽量的小</strong>, 导致基础镜像上连一些基础的命令都没有. 例如<code>vi</code>, <code>ping</code>这些基本的命令, 都可能要自己安装, 但是很多云环境上, 又没法正常的访问互联网, 导致严重浪费时间.</p><h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><p>实现原理说起来很简单, 就是用到容器注入的方式处理,  假设你已经启动了一个<code>TARGET_CONTAINER</code>容器, 它的ID为<code>TARGET_ID</code>, 你可以将<code>busybox</code>的镜像注入到<code>TARGET_CONTAINER</code>里面, 此时容器空间内的命令为<code>busybox</code>镜像所提供的, 但是网络或者进程空间全是<code>TARGET_CONTAINER</code>的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TARGET_ID=666666666</span><br><span class="line"><span class="comment"># 加入目标容器的 network, pid 以及 ipc namespace</span></span><br><span class="line">docker run -it --network=container:<span class="variable">$TARGET_ID</span> --pid=container:<span class="variable">$TARGET_ID</span> --ipc=container:<span class="variable">$TARGET_ID</span> busybox</span><br></pre></td></tr></table></figure><p>这个技术点确实很牛逼, 之前都不了解, 后面在<code>kubernete</code>上的实现, 就顺理成章了, 只需要通过API接口将需要的<code>TARGET_ID</code>查询出来, 注入镜像即可</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前看这个<code>kubectl-debug</code>只是个人项目,  可能会有一些场景支持的并不好, 特别是各个云上的安全系统. 所以有需要可以拿过来改改代码试用一下, 如果偶尔使用的话, 直接在<code>kubernete</code>的节点上使用容器注入会更快和可控一点.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]Pipleline介绍</title>
      <link href="/2019/07/21/Kubeflow%E7%B3%BB%E5%88%97-Pipleline%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/07/21/Kubeflow%E7%B3%BB%E5%88%97-Pipleline%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="总览"><a href="#总览" class="headerlink" title="总览"></a>总览</h2><p><code>Pipeline</code>在Kubeflow里面是一个最重要的能力, 因为需要它来串流整个机器学习任务的组件.<br>因此我们首先介绍一下<code>pipeline</code>的定义: 它是一个工作流平台，能够编译部署机器学习的工作流, 可以定义复杂的数据DAG流程, 并提供可视化的流程展示和结果展示.</p><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p><code>pipelines</code>实现了一个工作流模型。所谓工作流，或者称之为流水线（pipeline），可以将其当做一个有向无环图（DAG）。其中的每一个节点，在<code>pipelines</code> 的语义下被称作组件（component）。组件在图中作为一个节点，其会处理真正的逻辑，比如预处理，数据清洗，模型训练等等。每一个组件负责的功能不同，但有一个共同点，即组件都是以 Docker 镜像的方式被打包，以容器的方式被运行的。这也是与 kubeflow 社区的 Run ML on Kubernetes 这一愿景相统一的。</p><p>实验（experiment）是一个工作空间，在其中可以针对流水线尝试不同的配置。运行（run）是流水线的一次执行，用户在执行的过程中可以看到每一步的输出文件，以及日志。步（step）是组件的一次运行，步与组件的关系就像是运行与流水线的关系一样。步输出工件（step output artifacts）是在组件的一次运行结束后输出的，能被系统的前端理解并渲染可视化的文件。</p><p><strong>Kubeflow Pipeline中概念列表</strong>:</p><ol><li>Pipeline: 定义一组操作的流水线，其中每一步都由component组成。 背后是一个Argo的模板配置</li><li>Component: 一个容器操作，可以通过pipeline的sdk 定义。每一个component 可以定义定义输出（output）和产物（artifact）， 输出可以通过设置下一步的环境变量，作为下一步的输入， artifact 是组件运行完成后写入一个约定格式文件，在界面上可以被渲染展示。</li><li>Graph: 就是上面看到流程图, 一个pipeline可以转化为一个DAG图</li><li>Experiment:  可以看做一个工作空间，管理一组运行任务</li><li>Run: 真实任务,  pipeline真正在K8s上实例化的产物, 会启动多个对应的Pod, 开始真正的计算</li><li>Recurring Run: 定时任务, 可以由<code>Run Trigger</code>触发</li><li>Step: Component对应实体, 就是上图之中的方框, 应该是一个Step会真实对应一个K8s的Pod</li></ol><h2 id="官方案例"><a href="#官方案例" class="headerlink" title="官方案例"></a>官方案例</h2><p>官方的<strong>XGBoost - Training with Confusion Matrix</strong>案例:</p><h3 id="流程可视化页面"><a href="#流程可视化页面" class="headerlink" title="流程可视化页面:"></a>流程可视化页面:</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/2.png" alt=""></p><h3 id="提交任务页面"><a href="#提交任务页面" class="headerlink" title="提交任务页面:"></a>提交任务页面:</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/6.png" alt=""></p><h3 id="Artifacts页面"><a href="#Artifacts页面" class="headerlink" title="Artifacts页面"></a>Artifacts页面</h3><p>运行起来的时候, 在Artifacts之中看到一些metric信息(这些信息需要在流程之中自定义)</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/3.png" alt="">  <img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/4.png" alt=""></p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/5.png" alt=""></p><h2 id="Pipeline的流程及架构"><a href="#Pipeline的流程及架构" class="headerlink" title="Pipeline的流程及架构"></a>Pipeline的流程及架构</h2><p>使用Pipeline一般会有以下的步骤:</p><ol><li>使用python代码编写pipeline定义代码</li><li>通过编译脚本编译生成压缩包</li><li>上传压缩文件, 创建pipeline</li><li>使用该pipeline, 创建任务, 并输入指定的参数值</li><li>查看任务完成状态及输出结果</li></ol><p>Python代码定义再放在下一章节介绍, 这个章节介绍2-5步骤后台实现的流程. </p><p>下面先介绍两个基础知识:</p><h3 id="Argo"><a href="#Argo" class="headerlink" title="Argo"></a>Argo</h3><p>我们在Pipeline的页面上可以看到一个<code>Source</code>的子页面, 里面的内容是这个流程yaml格式的定义文件:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/7.png" alt=""></p><p>可以看到叫做<a href="https://argoproj.github.io/">argo</a>的组件,  它是一个开源的基于容器的工作流引擎，实现了一个K8S的CRD(用户自定义的资源):</p><ul><li>用容器实现工作流的每一个步骤</li><li>用DAG的形式描述多个任务之间的关系的依赖</li><li>支持机器学习和数据处理中的计算密集型任务</li></ul><p>实际上pipeline上传的就是argo的配置文件, 由这个配置文件来定义整个流程任务.</p><p>这个配置文件需要<code>pipeline sdk</code>对用户Python代码进行编译产生.</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/8.png" alt=""></p><p>pipeline的架构, 主要由以下模块组成:</p><ol><li>编译模块(SDK), 将Python代码转化为argo配置文件</li><li>页面模块(Web Server), 用于创建job以及监控任务运行结果</li><li>存储模块, 包含历史任务元数据信息, Artifact的数据缓存</li><li>服务模块, 一个由go编写的后端，提供kubernetes ApiServer 风格的Restful API。处理前端以及SDK发起的操作请求。 Pipeline/Experiment 之类的请求会直接存入元数据。和Run 相关的请求除了写入元数据以外还会通过APIServer 同步操作Argo实例。</li><li>控制模块:  Argo模块注册在K8s之中, 将argo的配置文件翻译为真正的K8s执行流程</li></ol><h3 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h3><ol><li>客户编写python代码, 经过编译之后生产argo配置项</li><li>客户调用上传接口(可以是网页调用), 上传argo配置项到<code>Pipeline Service</code></li><li><code>Pipeline Service</code>将元数据信息存储到<code>Meta Service</code>之中</li><li>客户触发任务执行,  <code>Pipeline Service</code>调用请求到<code>K8s API Service</code></li><li><code>K8s API Service</code>根据资源类型(<code>argo</code>资源), 触发Argo的编排调度框架</li><li>框架完成任务启动之后, 并将部分Artifact写入到<code>Artifact Storage</code>之中</li><li>pipeline时刻监控K8s的任务信息, 并在更新任务状态</li></ol><h2 id="GATK最佳实践案例"><a href="#GATK最佳实践案例" class="headerlink" title="GATK最佳实践案例"></a>GATK最佳实践案例</h2><p>下面以GATK最佳实践为例, 看一下如何实现一个流程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> kfp <span class="keyword">import</span> dsl</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">align_fastq</span>(<span class="params">fastq1, fastq2</span>):</span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">&#x27;aligin_fastq_with_bwa&#x27;</span>,</span><br><span class="line">        image=<span class="string">&#x27;swr.cn-north-5.myhuaweicloud.com/kubeflow/hzw-bwa:1.0&#x27;</span>,</span><br><span class="line">        command=[<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>],</span><br><span class="line">        arguments=[<span class="string">&quot;/bwa mem -t 4 -R &#x27;@RG\\tID:foo\\tLB:bar\\tPL:illumina\\tPU:illumina\\tSM:NA12878&#x27; /data/ref/hg19.fa $0 $1 &gt; /data/sample/test.sam;echo &#x27;/data/sample/test.sam&#x27; &gt; /output.txt&quot;</span>, fastq1, fastq2],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">&#x27;output&#x27;</span>: <span class="string">&#x27;/output.txt&#x27;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">&#x27;/data&#x27;</span>: dsl.PipelineVolume(pvc=<span class="string">&quot;cce-sfs-notebook&quot;</span>, name=<span class="string">&quot;notebook-pvc&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conver_sam</span>(<span class="params">samfile</span>):</span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">&#x27;convert sam into bam&#x27;</span>,</span><br><span class="line">        image=<span class="string">&#x27;swr.cn-north-5.myhuaweicloud.com/kubeflow/samtools:latest&#x27;</span>,</span><br><span class="line">        command=[<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>],</span><br><span class="line">        arguments=[<span class="string">&#x27;samtools sort -@ 4  -O bam -o /data/sample/test.bam $0; echo &quot;/data/sample/test.bam&quot; &gt; /output.txt&#x27;</span>, samfile],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">&#x27;output&#x27;</span>: <span class="string">&#x27;/output.txt&#x27;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">&#x27;/data&#x27;</span>: dsl.PipelineVolume(pvc=<span class="string">&quot;cce-sfs-notebook&quot;</span>, name=<span class="string">&quot;notebook-pvc&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">markdup</span>(<span class="params">bamfile</span>):</span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">&#x27;mark dup&#x27;</span>,</span><br><span class="line">        image=<span class="string">&#x27;swr.cn-north-5.myhuaweicloud.com/kubeflow/gatk:latest&#x27;</span>,</span><br><span class="line">        command=[<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>],</span><br><span class="line">        arguments=[<span class="string">&#x27;gatk MarkDuplicates -I $0 -O /data/sample/test.markup.bam -M /data/sample/test.metrics.txt; echo &quot;/data/sample/test.markup.bam&quot; &gt; /output.txt&#x27;</span>, bamfile],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">&#x27;output&#x27;</span>: <span class="string">&#x27;/output.txt&#x27;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">&#x27;/data&#x27;</span>: dsl.PipelineVolume(pvc=<span class="string">&quot;cce-sfs-notebook&quot;</span>, name=<span class="string">&quot;notebook-pvc&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bqsr</span>(<span class="params">marddup</span>):</span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">&#x27;bqsr&#x27;</span>,</span><br><span class="line">        image=<span class="string">&#x27;swr.cn-north-5.myhuaweicloud.com/kubeflow/gatk:latest&#x27;</span>,</span><br><span class="line">        command=[<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>],</span><br><span class="line">        arguments=[<span class="string">&#x27;gatk BaseRecalibrator -I $0 -O /data/sample/test.table -R /data/ref/hg19.fa.gz --known-sites /data/ref/1000g_omni2.5.hg19.sites.vcf.gz;&#x27;</span></span><br><span class="line">                   <span class="string">&#x27;gatk ApplyBQSR -R /data/ref/hg19.fa.gz -I $0 -O /data/sample/test.bqsr.bam --bqsr-recal-file /data/sample/test.table;&#x27;</span></span><br><span class="line">                   <span class="string">&#x27;echo &quot;/data/sample/test.bqsr.bam&quot; &gt; /output.txt&#x27;</span>, marddup],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">&#x27;data&#x27;</span>: <span class="string">&#x27;/output.txt&#x27;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">&#x27;/data&#x27;</span>: dsl.PipelineVolume(pvc=<span class="string">&quot;cce-sfs-notebook&quot;</span>, name=<span class="string">&quot;notebook-pvc&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">haplotype</span>(<span class="params">bqsr</span>):</span><br><span class="line">    <span class="keyword">return</span> dsl.ContainerOp(</span><br><span class="line">        name=<span class="string">&#x27;haplotype&#x27;</span>,</span><br><span class="line">        image=<span class="string">&#x27;swr.cn-north-5.myhuaweicloud.com/kubeflow/gatk:latest&#x27;</span>,</span><br><span class="line">        command=[<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>],</span><br><span class="line">        arguments=[<span class="string">&#x27;gatk HaplotypeCaller -I $0 -O /data/sample/test.vcf.gz -R /data/ref/hg19.fa.gz;&#x27;</span></span><br><span class="line">                   <span class="string">&#x27;echo &quot;/data/sample/test.vcf.gz&quot; &gt; /output.txt&#x27;</span>, bqsr],</span><br><span class="line">        file_outputs=&#123;</span><br><span class="line">            <span class="string">&#x27;data&#x27;</span>: <span class="string">&#x27;/output.txt&#x27;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        pvolumes=&#123;</span><br><span class="line">            <span class="string">&#x27;/data&#x27;</span>: dsl.PipelineVolume(pvc=<span class="string">&quot;cce-sfs-notebook&quot;</span>, name=<span class="string">&quot;notebook-pvc&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@dsl.pipeline(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="meta">    name=<span class="string">&quot;Gatk Basic pipeline&quot;</span>,</span></span></span><br><span class="line"><span class="params"><span class="meta">    description=<span class="string">&quot;A Basic pipeline for gatk.&quot;</span></span></span></span><br><span class="line"><span class="params"><span class="meta"></span>)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pipeline</span>(<span class="params">fastq1=<span class="string">&#x27;/data/sample/200M_1_NA12878_clean_1.fastq.gz&#x27;</span>, fastq2=<span class="string">&#x27;/data/sample/200M_1_NA12878_clean_2.fastq.gz&#x27;</span></span>):</span><br><span class="line">    samfile = align_fastq(fastq1, fastq2)</span><br><span class="line">    bamfile = conver_sam(samfile.output)</span><br><span class="line">    markfile = markdup(bamfile.output)</span><br><span class="line">    bqsr_file = bqsr(markfile.output)</span><br><span class="line">    haplotype(bqsr_file.output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> kfp.compiler <span class="keyword">as</span> compiler</span><br><span class="line">    compiler.Compiler().<span class="built_in">compile</span>(pipeline, __file__ + <span class="string">&quot;.tar.gz&quot;</span>)r</span><br></pre></td></tr></table></figure><ol><li><code>dsl</code>就是pipeline sdk,  由<code>@dsl.pipeline</code>标识pipeline的定义: 下一个函数即为定义函数, 函数的参数列表翻译为argo的input字段</li><li><code>dsl.ContainerOp</code>定义了镜像操作, 在这个例子之中有5个<code>ContainerOp</code>就会对应五个step, 启动5个pod进行计算.</li><li><code>dsl.PipelineVolume</code>定义了磁盘挂载信息, 作为参数输入到<code>ContainerOp</code>之中, 说明将该名字的PVC挂载到镜像里之中, 挂载目录为<code>/data</code></li><li><code>bamfile = conver_sam(samfile.output)</code>定义了依赖关系, 说明<code>conver_sam</code>依赖<code>align_fastq</code>的输出, 这个例子是个串行任务,  pipeline支持并发支持, 实现如下所示<code>bamfile = conver_sam(samfile1.output, samfile2.output)</code></li></ol><p>最后, 执行<code>main</code>函数, 可以生成一个tar.gz包, 将这个包上传到流程页面上, 就能开始运行任务.</p><h2 id="Pipeline的SDK简要说明"><a href="#Pipeline的SDK简要说明" class="headerlink" title="Pipeline的SDK简要说明"></a>Pipeline的SDK简要说明</h2><p><a href="https://kubeflow-pipelines.readthedocs.io/en/latest/index.html">API文档</a>之中详细写明了SDK目前的功能, 这儿就简单啰嗦一句.</p><p>目前SDK只支持如下几个模块的功能:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/18.png" alt="img"></p><ol><li>首先是<code>compiler</code>模块, 主要是将python代码转化为<code>argo</code>的yaml配置项</li><li><code>components</code>模型实现如何导入外部模块, 以及如何构建模块</li><li><code>dsl</code>是最重要的模块, 定义了<code>ContainerOp</code>以及<code>VolumeOp</code>等真正和K8s交互的模块</li><li><code>client</code>的模块主要是如何提交任务的模块</li><li><code>notebook</code>目前还是空的</li><li><code>extension</code>主要是云上的扩展模块, 目前主要支持谷歌亚马逊和微软的云</li></ol><p>整个SDK的代码量非常少, 可以直接看<a href="https://github.com/kubeflow/pipelines/tree/master/sdk/python/kfp">源码</a>了解更多的内容.</p><h2 id="完结撒花"><a href="#完结撒花" class="headerlink" title="完结撒花"></a>完结撒花</h2><p>走马观花一样的看了一下<code>Pipleline</code>的能力, 接下来总结一下优点和缺点:</p><ol><li>整体上<code>pipeline</code>有好的编程入口, 比较适合程序员, 但缺乏页面定制能力, 无法面向非程序员群体</li><li>有简单的任务调度能力, 支持定时任务, 但是功能有限</li><li>整体架构简洁明了,  但微服务众多, 运维是个压力</li><li>整体来说, pipeline功能还不完善, 需要深度定制, 但K8s + AI是趋势, 未来肯定会越来越好, 越来越庞大.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>缓存一致性问题</title>
      <link href="/2019/07/14/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/"/>
      <url>/2019/07/14/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缓存读"><a href="#缓存读" class="headerlink" title="缓存读"></a>缓存读</h2><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/cache_consistency/1.png" alt=""></p><h2 id="缓存更新"><a href="#缓存更新" class="headerlink" title="缓存更新"></a>缓存更新</h2><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/cache_consistency/2.png" alt=""></p><h2 id="缓存并发问题"><a href="#缓存并发问题" class="headerlink" title="缓存并发问题"></a>缓存并发问题</h2><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/cache_consistency/3.png" alt=""></p><p>缓存系统存在根本的问题是<strong>缓存的读写和数据库读写不是原子的</strong>, 因此它必然会出现并发的问题.</p><h2 id="缓存并发解决思路"><a href="#缓存并发解决思路" class="headerlink" title="缓存并发解决思路"></a>缓存并发解决思路</h2><h3 id="引入全局锁"><a href="#引入全局锁" class="headerlink" title="引入全局锁"></a>引入全局锁</h3><p>例如<code>Redis</code>就有锁机制, 通过<code>redis</code>实现全局锁(行锁),  更新的时候设置排它锁, 不允许读取操作即可.</p><p>这就和数据库之中的<code>悲观锁</code>是一个性质的, 这种系统最大的问题就是吞吐量限制, 每次必须查询锁的状态</p><h3 id="引入消息队列"><a href="#引入消息队列" class="headerlink" title="引入消息队列"></a>引入消息队列</h3><p>或者使用<code>乐观锁</code>的实现方式, 就是引入消息队列,  消息队列相对于全局锁的优势在于: </p><ol><li>可以合并一部分的更新操作, 例如2次更新之间没有任何读取, 就可以将更新合并</li><li>可以和流控系统结合, 消息队列可以用于请求的流控, 通过流控系统可以实现缓存读取的分布式化</li></ol><p>缺点就是, 系统架构越来越复杂了, 流控系统的可靠性成为系统的关键.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]总览介绍</title>
      <link href="/2019/07/14/Kubeflow%E7%B3%BB%E5%88%97-%E6%80%BB%E8%A7%88%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/07/14/Kubeflow%E7%B3%BB%E5%88%97-%E6%80%BB%E8%A7%88%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="什么是Kubeflow"><a href="#什么是Kubeflow" class="headerlink" title="什么是Kubeflow"></a>什么是Kubeflow</h2><p>对于<a href="https://www.kubeflow.org/">官网</a>的定义:</p><blockquote><p>The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. </p></blockquote><p>我们可以看出<code>kubeflow</code>是在<code>Kubernetes</code>之上构建<code>Machine Learning</code>的项目,  他本身不会创建任何AI或者Kubernetes的服务,  而是为了<code>Machine Learning</code>项目构建的更加简单, 更加可扩展.</p><p>为了更好的学习Kubeflow, 我们需要有以下的基础知识:</p><ol><li>容器技术, 包含docker使用, 镜像制作, Kubernetes 任务提交等一系列的Paas能力</li><li>AI技术, 包括各种AI框架, 以及分布式AI模型等</li><li>Python语言, kubeflow的代码库大多以Python编写</li></ol><h2 id="Kubeflow有什么"><a href="#Kubeflow有什么" class="headerlink" title="Kubeflow有什么"></a>Kubeflow有什么</h2><p><code>Kubeflow</code>有很多的组件,  可以在官网文档的<a href="https://master.kubeflow.org/docs/components/">Components</a>模块之中找到所有能力组件, 我把这些模块整理了一下, 画出脑图, 如下:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/1.png" alt=""></p><p><strong>左边的一列</strong>为kubeflow面向机器学习领域提供的能力, 包括模型的训练推理以及自动学习的超参调优组件, Kubeflow已经支持了市面上大多数的训练和推理框架, 其中最完善的就是google自己家的<code>TensorFlow</code>.</p><p>当然也是最重要的一个模块, 毕竟Kubeflow的名字有一般来自于它.</p><p><strong>右边的一列</strong>为Kubeflow的基础能力, 主要为pipeline, notebook, fairing以及一些公共的组件, 例如元数据管理等, 这个部分是围绕AI能力外围的基础组件, 主要关注于工程部分, 例如使用pipeline进行编排, 使用notebook进行开发等.</p><p>下面准备简单介绍一下这几个模块,  某些组件特别大, 我准备单独写个文章介绍, 这里就会一笔带过. </p><h3 id="TFJob-和-TF-Serving"><a href="#TFJob-和-TF-Serving" class="headerlink" title="TFJob 和 TF Serving"></a>TFJob 和 TF Serving</h3><p>模型训练是机器学习最主要的实践场景，尤其以使用机器学习框架TensorFlow进行模型训练最为流行，但是随着机器学习的平台由单机变成集群，这个问题变得复杂了。GPU的调度和绑定，涉及到分布式训练的编排和集群规约属性的配置（cluster spec）也成了数据科学家们巨大的负担。</p><p>为了解决这一问题，一个新的资源类型TFJob，即TensorFlow Job被定义出来了。通过这个资源类型，使用TensorFlow的数据科学家无需编写复杂的配置，只需要关注数据的输入，代码的运行和日志的输入输出。</p><p>下面看一个<strong>官方案例</strong>来看一下<code>TFJob</code>的定义, 除了<code>tfReplicaSpecs</code>定义为<code>PS</code> <code>Worker</code>等概念之外, 其实和没有K8s的POD定位没有太多的区别.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;kubeflow.org/v1beta1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;TFJob&quot;</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;tf-smoke-gpu&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&quot;kubeflow&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">tfReplicaSpecs:</span></span><br><span class="line">    <span class="attr">PS:</span></span><br><span class="line">      <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">metadata:</span></span><br><span class="line">          <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">python</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">tf_cnn_benchmarks.py</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--batch_size=32</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--model=resnet50</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--variable_update=parameter_server</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--flush_stdout=true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--num_gpus=1</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--local_parameter_device=cpu</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--device=cpu</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--data_format=NHWC</span></span><br><span class="line">            <span class="attr">image:</span> <span class="string">swr.cn-north-5.myhuaweicloud.com/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">            <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">2222</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">tfjob-port</span></span><br><span class="line">            <span class="attr">resources:</span></span><br><span class="line">              <span class="attr">limits:</span></span><br><span class="line">                <span class="attr">cpu:</span> <span class="string">&#x27;1&#x27;</span></span><br><span class="line">            <span class="attr">workingDir:</span> <span class="string">/opt/tf-benchmarks/scripts/tf_cnn_benchmarks</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">    <span class="attr">Worker:</span></span><br><span class="line">      <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">metadata:</span></span><br><span class="line">          <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">python</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">tf_cnn_benchmarks.py</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--batch_size=32</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--model=resnet50</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--variable_update=parameter_server</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--flush_stdout=true</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--num_gpus=1</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--local_parameter_device=cpu</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--device=gpu</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--data_format=NHWC</span></span><br><span class="line">            <span class="attr">image:</span> <span class="string">swr.cn-north-5.myhuaweicloud.com/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">tensorflow</span></span><br><span class="line">            <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">2222</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">tfjob-port</span></span><br><span class="line">            <span class="attr">resources:</span></span><br><span class="line">              <span class="attr">limits:</span></span><br><span class="line">                <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">workingDir:</span> <span class="string">/opt/tf-benchmarks/scripts/tf_cnn_benchmarks</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br></pre></td></tr></table></figure><p><strong>TensorFlow Serving</strong>是Google开源的一个灵活的、高性能的机器学习模型服务系统，能够简化并加速从模型到生产应用的过程。它除了原生支持TensorFlow模型，还可以扩展支持其他类型的机器学习模型。</p><p>部署完成之后, 可以通过API访问TF Serving或者GRpc的方式来进行推理服务.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl get svc mnist-service</span><br><span class="line">curl -X POST -d @input.json http://EXTERNAL_IP:8500/v1/models/mnist:predict</span><br></pre></td></tr></table></figure><blockquote><p>这个部分可以参考<a href="https://yq.aliyun.com/articles/601779?utm_content=m_1000003286">阿里云的文章</a>, 后续再写详细介绍的文章</p></blockquote><p>其中有一点需要的注意的是: <strong>TFJob是Kubeflow在K8s上的CRD, 而TF-Serving是TensorFlow的内容, Kubeflow只不过在K8s上启动一个镜像实例而已</strong>, 所以你也可以来完成TF-Serving功能</p><h3 id="Katib超参数训练系统"><a href="#Katib超参数训练系统" class="headerlink" title="Katib超参数训练系统"></a>Katib超参数训练系统</h3><blockquote><p>Katib is a Kubernetes Native System for <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">Hyperparameter Tuning</a> and <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">Neural Architecture Search</a>. The system is inspired by <a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/bcb15507f4b52991a0783013df4222240e942381.pdf">Google vizier</a> and supports multiple ML/DL frameworks (e.g. TensorFlow, MXNet, and PyTorch).</p></blockquote><p>Katib 也是对 Google Vizier 的开源实现，因此也遵循其中对问题的抽象模型：Study，Trial 和 Suggestion.<br>Trial 代表着一个由超参数的取值组成的列表，每个 Trial 都需要一次运行来得到这些超参数取值对应的结果。这里提到的运行就是一次训练的过程。Study 代表着在可行空间上运行的单个优化。每个 Study 都有一个配置，来描述可能取值的空间，超参数推荐算法等。此外，Study 包含一组 Trial，代表着算法在超参数集合中选取推荐值的多次尝试。如图所示，是创建一次 Study 并且进行 Trial 的验证的过程。</p><blockquote><p>Currently Katib supports the following exploration algorithms in v1alpha1:</p><ul><li>random search</li><li>grid search</li><li><a href="https://arxiv.org/pdf/1603.06560.pdf">hyperband</a></li><li><a href="https://arxiv.org/pdf/1012.2599.pdf">bayesian optimization</a></li><li><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1alpha1/NAS_Reinforcement_Learning">NAS based on reinforcement learning</a></li><li><a href="https://github.com/kubeflow/katib/tree/master/pkg/suggestion/v1alpha1/NAS_Envelopenet">NAS based on EnvelopeNets</a><br>And Katib supports the following exploration algorithms in v1alpha2:</li><li>random search</li></ul><p>这块非常不懂唉, 后续等恶补一下知识了.  现在摘要了这篇<a href="http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/katib">博客文章</a>的内容</p></blockquote><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>这块的目的就是为了编排Kubeflow任务用的, 在<a href="https://saintbacchus.github.io/2019/07/21/Kubeflow-pipleline%E4%BB%8B%E7%BB%8D/">这篇文章</a>之中介绍.</p><h3 id="Jupiter-Notebook"><a href="#Jupiter-Notebook" class="headerlink" title="Jupiter Notebook"></a>Jupiter Notebook</h3><p>Notebook是数据科学家比较喜欢的编程工具, kubeflow已经集成:</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/9.png" alt=""></p><p>上图是notebook的创建页面,  可以看出来kubeflow的notebook有以下能力:</p><ol><li>自定义镜像</li><li>支持资源定义</li><li>支持各种数据盘挂载</li><li>支持GPU调度</li></ol><p><strong> 使用notebook提交pipeline</strong></p><p>以官方的<strong>Lightweight Python components - basics</strong>为例子</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/10.png" alt=""></p><ul><li>上边的cell定了流程算子</li><li>中间的cell编译流程算子, 生成<code>argo</code>的配置文件<code>pipeline.zip</code></li><li>下面的cell创建了<code>Pipeline Service</code>的客户端, 创建了<code>experiment</code>和<code>run</code>执行了任务</li></ul><h3 id="Fairing-SDK"><a href="#Fairing-SDK" class="headerlink" title="Fairing SDK"></a>Fairing SDK</h3><p>Kubeflow Fairing is <strong>a Python package</strong> that makes it easy to train and deploy ML models on <a href="https://www.kubeflow.org/docs/about/kubeflow/">Kubeflow</a>. Kubeflow Fairing can also been extended to train or deploy on other platforms. Currently, Kubeflow Fairing has been extended to train on <a href="https://cloud.google.com/ml-engine/docs/">Google AI Platform</a>.</p><p>Kubeflow Fairing <strong>packages your Jupyter notebook, Python function, or Python file as a Docker image</strong>, then <strong>deploys and runs the training job</strong> on Kubeflow or AI Platform. After your training job is complete, you can use Kubeflow Fairing to <strong>deploy your trained model as a prediction endpoint</strong> on Kubeflow.</p><p>官网上介绍Fairing的文字, fairing的定位就是机器学习的SDK, 但他最大的一个好处就是简化构建镜像的麻烦, 它能自动帮你构建镜像, 并将镜像部署在k8s上运行, 因为之前的开发模式是这样子的:</p> <div id="flowchart-0" class="flow-chart"></div><p>引入Kubeflow变成这样了:</p><div id="flowchart-1" class="flow-chart"></div><p>平白多了几部做镜像的步骤, 而且做镜像对数据科学家来说是个难活, 因此引入Fairing工程是十分必要的</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>上面已经把kubeflow最重要的内容将完, 但是k8s还有很多其他子项目, 虽然不然之前的模块那么重要, 但是也是必不可少的内容. 这些组件很多都不在k8s之中, 在k8s的其他项目里面.</p><h4 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h4><p>元数据模块主要是为了用户能更好的管理和追踪kubeflow的流程,  它后台使用的是Google的<a href="https://github.com/google/ml-metadata/blob/master/g3doc/get_started.md">ML-Metadata</a>来管理, 而且暴露了<a href="https://github.com/kubeflow/metadata/blob/master/api/service.swagger.json">REST API</a>来跟用户调用, 同时也有<a href="https://github.com/kubeflow/metadata/tree/master/sdk/python#python-client">Python SDK</a>远程调用</p><h4 id="kustomize"><a href="#kustomize" class="headerlink" title="kustomize"></a>kustomize</h4><p><code>kustomize</code>并不是kubeflow的一个项目, 而是<a href="https://github.com/kubernetes-sigs/kustomize">kubernetes-sigs</a>的一个子项目, 主要简化部署K8s服务的麻烦.</p><p>他的idea的是这样的: 之前部署k8s需要定义很多的YAML文件, 这些文件都要自己管理, 而kustomize可以将这些文件管理起来, 貌似还有版本管理的能力</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/1.jpg" alt=""></p><p>看一下这个图片示意, 基本上就了解.</p><p><a href="https://github.com/kubernetes-sigs/kustomize">官网地址</a></p><h4 id="Nuclio-functions"><a href="#Nuclio-functions" class="headerlink" title="Nuclio functions"></a>Nuclio functions</h4><p>Nuclio 是开源的一个实时无服务器平台, 支持可插拔的事件源和数据源, 下面是它的整体架构.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/19.png" alt=""></p><blockquote><p>目前没看出来这个Serverless的框架和Kubeflow有太多的联系, 而且FaaS战场上Nuclio也不是真正的主流, <code>knative</code>是google推的Serverless解决方案, 所以这个部分不多介绍了.  </p></blockquote><h4 id="ksonnet"><a href="#ksonnet" class="headerlink" title="ksonnet"></a>ksonnet</h4><p>ksonnet 是一个基于jsonnet的快速简化kubernetes yaml 配置的工具，可以实现配置的复用 </p><p>同时也包含一个registry 的概念，可以实现可复用组件的分发，同时支持helm</p><p>Kubeflow里面主要使用<code>ksonnet</code>完成安装的时候参数的配置.</p><p>具体内容可以看<a href="https://ksonnet.io/">官网文档</a></p><h4 id="Microk8s"><a href="#Microk8s" class="headerlink" title="Microk8s"></a>Microk8s</h4><p>单机K8s测试环境的库,  这个安装起来比<code>Minikube</code>和<code>MiniKF</code>更加方便</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/canonical-labs/kubernetes-tools</span><br><span class="line">sudo kubernetes-tools/setup-microk8s.sh</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/canonical-labs/kubeflow-tools</span><br><span class="line">kubeflow-tools/install-kubeflow.sh</span><br></pre></td></tr></table></figure><h4 id="Volcano’s-scheduler"><a href="#Volcano’s-scheduler" class="headerlink" title="Volcano’s scheduler"></a>Volcano’s scheduler</h4><p>Volcano调度是华为公司贡献给社区的批处理调度器,  支持AI和大数据计算</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/volcano-intro.png" alt=""></p><p>给K8s提供以下能力:</p><ol><li>Job management extensions and improvements, e.g:<ol><li>Multi-pod jobs</li><li>Lifecycle management extensions including suspend/resume and restart.</li><li>Improved error handling</li><li>Indexed jobs</li><li>Task dependencies</li></ol></li><li>Scheduling extensions, e.g:<ol><li>Co-scheduling</li><li>Fair-share scheduling</li><li>Queue scheduling</li><li>Preemption and reclaims</li><li>Reservations and backfills</li><li>Topology-based scheduling</li></ol></li><li>Runtime extensions, e.g:<ol><li>Support for specialized container runtimes like Singularity, with GPU accelerator extensions and enhanced security features.</li></ol></li><li>Other<ol><li>Data locality awareness and intelligent scheduling</li><li>Optimizations for data throughput, round-trip latency, etc.</li></ol></li></ol><h5 id="Kube-Batch"><a href="#Kube-Batch" class="headerlink" title="Kube-Batch"></a>Kube-Batch</h5><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/kube-batch.png" alt=""></p><p><code>kube-batch</code>是一个为kubernetes实现批量任务调度的一个调度器，主要用于<code>机器学习</code>，<code>大数据</code>，<code>HPC</code>等场景.</p><p>而Volcano正是构建在Kube-Batch之上的.</p><h5 id="Gang-Scheduler"><a href="#Gang-Scheduler" class="headerlink" title="Gang Scheduler"></a>Gang Scheduler</h5><p><code>gang-schedule</code>是什么概念? 用用户提交一个 batch job, 这个batch job 包含100个任务，要不这100个任务全部调度成功，要么一个都调度不成功。这种<code>all or nothing</code>调度场景，就被称作:<code>gang-schedule</code>，通常用于集群资源不足的场景，比如 AI 场景下调用GPU资源</p><p>Gang Scheduler特别适合机器学习场景, 引入Vocalno和Kube-bath最终目的就是为了引入Gang-Scheduler</p><blockquote><p><a href="https://xigang.github.io/2019/02/17/gang-scheduler/">参考文章</a></p></blockquote><h4 id="Dex"><a href="#Dex" class="headerlink" title="Dex"></a>Dex</h4><p><a href="https://github.com/coreos/dex">dex</a> 是一个统一认证的服务，支持各种认证协议如Ouath2 ldap等，自己可以作为一个identity provider,也可以连到别的id provider(如github)上,dex作为一个中间代理.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/20.png" alt=""></p><p>安装官网给出的推荐案例, 使用dex进行租户认证</p><blockquote><p><a href="https://github.com/dexidp/dex">Github首页</a></p></blockquote><h4 id="Istio"><a href="#Istio" class="headerlink" title="Istio"></a>Istio</h4><p>Istio是钟Service Mesh的实现, Service Mesh这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。</p><p>Istio 提供了一个完整的解决方案，通过为整个服务网格提供行为洞察和操作控制来满足微服务应用程序的多样化需求.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow_introduce/arch.svg" alt=""></p><blockquote><p><a href="https://istio.io/">官网首页</a>以及<a href="https://istio.io/zh/docs/concepts/what-is-istio/">中文官网</a></p></blockquote><h2 id="完结撒花"><a href="#完结撒花" class="headerlink" title="完结撒花"></a>完结撒花</h2><p>今天把kubeflow的一些概念都整理了一下, 除了AI那块不是特别懂, 所以介绍内容比较少,  后面需要再调研一下, 再写介绍文章, 至于工程方面的都基本上算调研好了, 后续深入使用的时候再写文章介绍</p><p>后续至少需要隆重写文章介绍的有:</p><ol><li>Pipeline的文章</li><li>TFJob的文章</li><li>TF-Serving的文章</li><li>Katib的文章</li><li>Istio的文章<script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: 写机器学习代码op=>operation: 执行训练过程, 并调试e=>end: 模型训练完毕, 进行部署st->op->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><textarea id="flowchart-1-code" style="display: none">st=>start: 写机器学习代码op1=>operation: 做镜像op2=>operation: 训练并调试op3=>operation: 继续做镜像e=>end: 进行部署st->op1->op2->op3->e</textarea><textarea id="flowchart-1-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-1", options);</script></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker in docker</title>
      <link href="/2019/07/13/Docker-in-docker/"/>
      <url>/2019/07/13/Docker-in-docker/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="HostPath挂载"><a href="#HostPath挂载" class="headerlink" title="HostPath挂载"></a>HostPath挂载</h2><p>容器启动时, 挂载<code>/var/run/docker.soc</code>k和<code>/usr/bin/docker</code>,即可在容器中执行docker命令。此时docker server仍运行在host机上，docker in docker 实际操作的是宿主机docker。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">docker-in-docker-mount</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">docker-in-docker-mount</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-socket</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-cmd</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/usr/bin/docker</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container-0</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">&#x27;ubuntu-docker-in-docker:latest&#x27;</span></span><br><span class="line">          <span class="attr">command:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/bin/bash</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&#x27;-c&#x27;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&#x27;echo hello;docker image ls&#x27;</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-socket</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-cmd</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/usr/bin/docker</span></span><br></pre></td></tr></table></figure><h2 id="docker-dind"><a href="#docker-dind" class="headerlink" title="docker dind"></a>docker dind</h2><p>docker 官方提供了docker in docker镜像: <code>docker pull docker:18.09.7-dind</code>, 直接使用此镜像即可在容器中在运行容器. 但是此种方式运行权限需要为<code>特权容器</code>, 在K8s设置<code>securityContext.privileged</code>为true<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dind-test10</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">dind-test10</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">dind-test10</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container-0</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">&#x27;docker:18.09.7-dind&#x27;</span></span><br><span class="line">          <span class="attr">command:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&#x27;-c&#x27;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&#x27;echo hello;docker image ls&#x27;</span>  </span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">procMount:</span> <span class="string">Default</span></span><br></pre></td></tr></table></figure></p><h2 id="两者比较"><a href="#两者比较" class="headerlink" title="两者比较"></a>两者比较</h2><div class="table-container"><table><thead><tr><th style="text-align:center">比较项</th><th style="text-align:center">HostPath</th><th style="text-align:center">Dind</th></tr></thead><tbody><tr><td style="text-align:center">安全性</td><td style="text-align:center">中, docker命令可以注入宿主机之中</td><td style="text-align:center">低, 特权容器所有权限开放, 不能有用户代码存在容器之中</td></tr><tr><td style="text-align:center">简便性</td><td style="text-align:center">中, 需要额外设置HostPath</td><td style="text-align:center">高, 配置简单, 一键使用</td></tr><tr><td style="text-align:center">并发性</td><td style="text-align:center">高, 可以用于任何容器</td><td style="text-align:center">低, 只能基于Dind基础镜像之上构做镜像</td></tr></tbody></table></div><p>结论: 能用<code>HostPath</code>模式尽量使用:</p><ul><li>如果需要在容器运行<code>UDF</code>, 这两种权限都不能使用, 相对来说<code>HostPath</code>的威胁小一点点</li><li>如果要自定义镜像, 使用<code>HostPath</code></li><li>如果只是调用docker命令, 可以使用<code>Dind</code>, 但还是推荐使用<code>HostPath</code>, 因为<strong>你也许知道权限问题, 但是他人接手你工作的时候, 可能并不清楚</strong></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubeflow系列]Kubeflow的部署</title>
      <link href="/2019/07/10/Kubeflow%E7%B3%BB%E5%88%97-Kubeflow%E7%9A%84%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/07/10/Kubeflow%E7%B3%BB%E5%88%97-Kubeflow%E7%9A%84%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先,  官网上有对应的安装文档, 地址在这里<a href="https://www.kubeflow.org/docs/started/getting-started-k8s/">官方安装指南</a>.</p><p>但是在国内总是有一些中国特设的问题, 所以在这儿记录一下安装的流程.</p><blockquote><p>安装的环境是在华为云的云容器引擎服务<a href="https://www.huaweicloud.com/product/cce.html">CCE</a>, 其他环境可能略有不同.</p><p>此外华为云上已有一个<a href="https://bbs.huaweicloud.com/blogs/413d1821c1a211e89fc57ca23e93a89f">KubeFlow安装指南</a></p></blockquote><h2 id="依赖安装"><a href="#依赖安装" class="headerlink" title="依赖安装"></a>依赖安装</h2><p>首先安装官方文档, 安装如下的三个依赖:</p><ol><li><a href="https://support.huaweicloud.com/usermanual-cce/cce_01_0107.html">kubectl</a></li><li><a href="https://github.com/ksonnet/ksonnet/releases/">ks</a></li><li><a href="https://github.com/kubeflow/kubeflow/releases/">kfctl</a></li></ol><p>将这三个依赖都放入到环境之中, <code>kubectl</code>需要设置好环境, <code>kubectl get pod</code>不能有一次</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> ks kubectl kfctl /usr/local/bin/</span><br></pre></td></tr></table></figure><blockquote><p>我安装的是v0.5版本的kubeflow, 版本升级之后不保证成功</p></blockquote><h2 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h2><p>执行以下命令生成配置文件, <strong>这个步骤需要在联网环境下执行</strong>, 在华为云上虚拟机就需要绑定弹性IP.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /root/kubeflow</span><br><span class="line"><span class="built_in">cd</span> /root/kubeflow</span><br><span class="line"><span class="built_in">export</span> KFAPP=kubeflow</span><br><span class="line">kfctl init <span class="variable">$&#123;KFAPP&#125;</span></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$&#123;KFAPP&#125;</span></span><br><span class="line">kfctl generate all -V</span><br></pre></td></tr></table></figure><p>完成后,  会在目录下生成如下文件:</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/1.png" alt=""></p><h2 id="替换镜像"><a href="#替换镜像" class="headerlink" title="替换镜像"></a>替换镜像</h2><p>默认生成的配置, 其中的镜像都是GoogleCloud的公开镜像, 在国内网络之中, 肯定无法访问, 需要将这些镜像想办法下载下来传入到华为云的<a href="https://www.huaweicloud.com/product/swr.html">容器镜像服务</a>之内.</p><h3 id="镜像下载上传"><a href="#镜像下载上传" class="headerlink" title="镜像下载上传"></a>镜像下载上传</h3><p>我将我用到的镜像都列举出来, 配置代理之后, 再拉取到本地</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">docker pull  gcr.io/ml-pipeline/api-server:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/persistenceagent:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/scheduledworkflow:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/frontend:0.1.16</span><br><span class="line">docker pull  gcr.io/ml-pipeline/viewer-crd-controller:0.1.16</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/pytorch-operator:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/tf_operator:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/centraldashboard:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/jupyter-web-app:v0.5.0</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/katib-ui:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  gcr.io/kubeflow-images-public/katib/suggestion-random:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker pull  tensorflow/tensorflow:1.8.0</span><br><span class="line">docker pull  mysql:5.6</span><br><span class="line">docker pull  mysql:8.0.3</span><br><span class="line">docker pull  quay.io/datawire/ambassador:0.37.0</span><br><span class="line">docker pull  argoproj/argoui:v2.2.0</span><br><span class="line">docker pull  argoproj/workflow-controller:v2.2.0</span><br><span class="line">docker pull  metacontroller/metacontroller:v0.3.0</span><br><span class="line">docker pull  minio/minio:RELEASE.2018-02-09T22-40-05Z</span><br></pre></td></tr></table></figure><p>而后再加入SWR的tag</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">docker tag  gcr.io/ml-pipeline/api-server:0.1.16                                                              swr.cn-north-5.myhuaweicloud.com/kubeflow/api-server:0.1.16                                                          </span><br><span class="line">docker tag  gcr.io/ml-pipeline/persistenceagent:0.1.16                                                        swr.cn-north-5.myhuaweicloud.com/kubeflow/persistenceagent:0.1.16                                                    </span><br><span class="line">docker tag  gcr.io/ml-pipeline/scheduledworkflow:0.1.16                                                       swr.cn-north-5.myhuaweicloud.com/kubeflow/scheduledworkflow:0.1.16                                                   </span><br><span class="line">docker tag  gcr.io/ml-pipeline/frontend:0.1.16                                                                swr.cn-north-5.myhuaweicloud.com/kubeflow/frontend:0.1.16                                                            </span><br><span class="line">docker tag  gcr.io/ml-pipeline/viewer-crd-controller:0.1.16                                                   swr.cn-north-5.myhuaweicloud.com/kubeflow/viewer-crd-controller:0.1.16                                               </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4      swr.cn-north-5.myhuaweicloud.com/kubeflow/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4  </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/pytorch-operator:v0.5.0                                             swr.cn-north-5.myhuaweicloud.com/kubeflow/pytorch-operator:v0.5.0                                         </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd                 swr.cn-north-5.myhuaweicloud.com/kubeflow/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd             </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/tf_operator:v0.5.0                                                  swr.cn-north-5.myhuaweicloud.com/kubeflow/tf_operator:v0.5.0                                              </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/vizier-core:v0.1.2-alpha-156-g4ab3dbd                         swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core:v0.1.2-alpha-156-g4ab3dbd                     </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                    swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/centraldashboard:v0.5.0                                             swr.cn-north-5.myhuaweicloud.com/kubeflow/centraldashboard:v0.5.0                                         </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/jupyter-web-app:v0.5.0                                              swr.cn-north-5.myhuaweicloud.com/kubeflow/jupyter-web-app:v0.5.0                                          </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/katib-ui:v0.1.2-alpha-156-g4ab3dbd                            swr.cn-north-5.myhuaweicloud.com/kubeflow/katib-ui:v0.1.2-alpha-156-g4ab3dbd                        </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd     swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                     swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                 </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd                swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd            </span><br><span class="line">docker tag  gcr.io/kubeflow-images-public/katib/suggestion-random:v0.1.2-alpha-156-g4ab3dbd                   swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-random:v0.1.2-alpha-156-g4ab3dbd               </span><br><span class="line">docker tag  tensorflow/tensorflow:1.8.0                                                                       swr.cn-north-5.myhuaweicloud.com/kubeflow/tensorflow:1.8.0                 </span><br><span class="line">docker tag  mysql:5.6                                                                                         swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:5.6                                   </span><br><span class="line">docker tag  mysql:8.0.3                                                                                       swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:8.0.3                                 </span><br><span class="line">docker tag  quay.io/datawire/ambassador:0.37.0                                                                swr.cn-north-5.myhuaweicloud.com/kubeflow/ambassador:0.37.0          </span><br><span class="line">docker tag  argoproj/argoui:v2.2.0                                                                            swr.cn-north-5.myhuaweicloud.com/kubeflow/argoui:v2.2.0                      </span><br><span class="line">docker tag  argoproj/workflow-controller:v2.2.0                                                               swr.cn-north-5.myhuaweicloud.com/kubeflow/workflow-controller:v2.2.0         </span><br><span class="line">docker tag  metacontroller/metacontroller:v0.3.0                                                              swr.cn-north-5.myhuaweicloud.com/kubeflow/metacontroller:v0.3.0        </span><br><span class="line">docker tag  minio/minio:RELEASE.2018-02-09T22-40-05Z                                                          swr.cn-north-5.myhuaweicloud.com/kubeflow/minio:RELEASE.2018-02-09T22-40-05Z    </span><br></pre></td></tr></table></figure><p>再推到SWR之中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/api-server:0.1.16                                                          </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/persistenceagent:0.1.16                                                    </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/scheduledworkflow:0.1.16                                                   </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/frontend:0.1.16                                                            </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/viewer-crd-controller:0.1.16                                               </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4  </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/pytorch-operator:v0.5.0                                         </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd             </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/tf_operator:v0.5.0                                              </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core:v0.1.2-alpha-156-g4ab3dbd                     </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/centraldashboard:v0.5.0                                         </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/jupyter-web-app:v0.5.0                                          </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/katib-ui:v0.1.2-alpha-156-g4ab3dbd                        </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                 </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd            </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/suggestion-random:v0.1.2-alpha-156-g4ab3dbd               </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/tensorflow:1.8.0                 </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:5.6                                   </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/mysql:8.0.3                                 </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/ambassador:0.37.0          </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/argoui:v2.2.0                      </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/workflow-controller:v2.2.0         </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/metacontroller:v0.3.0        </span><br><span class="line">docker push  swr.cn-north-5.myhuaweicloud.com/kubeflow/minio:RELEASE.2018-02-09T22-40-05Z    </span><br></pre></td></tr></table></figure><blockquote><p>华为云有几个限制, 需要你注意:</p><ol><li>路径不能有多级, 即容器名字不能含有<code>/</code></li><li>路径不能含有点号<code>.</code>, 有部分镜像带有版本号, 需要修改原始名称, 但是tag里面又可以存在<code>.</code></li></ol><p>另外这部分镜像可能是不全的, 因为例如<code>model db</code>等组件是没有安装的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/api-server:0.1.16                                                          </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/persistenceagent:0.1.16                                                    </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/scheduledworkflow:0.1.16                                                   </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/frontend:0.1.16                                                            </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/viewer-crd-controller:0.1.16                                               </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/notebook-controller:v20190401-v0.4.0-rc.1-308-g33618cc9-e3b0c4  </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/pytorch-operator:v0.5.0                                         </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/studyjob-controller:v0.1.2-alpha-156-g4ab3dbd             </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/tf_operator:v0.5.0                                              </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/vizier-core:v0.1.2-alpha-156-g4ab3dbd                     </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/vizier-core-rest:v0.1.2-alpha-156-g4ab3dbd                </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/centraldashboard:v0.5.0                                         </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/jupyter-web-app:v0.5.0                                          </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/katib-ui:v0.1.2-alpha-156-g4ab3dbd                        </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-bayesianoptimization:v0.1.2-alpha-156-g4ab3dbd </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-grid:v0.1.2-alpha-156-g4ab3dbd                 </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-hyperband:v0.1.2-alpha-156-g4ab3dbd            </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/suggestion-random:v0.1.2-alpha-156-g4ab3dbd               </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/tensorflow:1.8.0                 </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/mysql:5.6                                   </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/mysql:8.0.3                                 </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/ambassador:0.37.0          </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/argoui:v2.2.0                      </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/workflow-controller:v2.2.0         </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/metacontroller:v0.3.0        </span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/minio:RELEASE.2018-02-09T22-40-05Z</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/volume-nfs:0.8</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/argoexec:v2.2.0</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/metrics-collector:v0.1.2-alpha-156-g4ab3dbd</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-artifact-store:kubeflow</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-backend:kubeflow</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-backend-proxy:kubeflow</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/mysql:5.7</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/modeldb-frontend:kubeflow</span><br><span class="line">docker push swr.cn-north-1.myhuaweicloud.com/hzw-kubeflow/serving:1.11.1</span><br></pre></td></tr></table></figure><p>这份长的整整有35个镜像之多</p></blockquote><h3 id="镜像替换"><a href="#镜像替换" class="headerlink" title="镜像替换"></a>镜像替换</h3><p>经过最终排查, 发现所有依赖镜像都在以下两个文件之中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim ks_app/vendor/kubeflow/pipeline/pipeline.libsonnet</span><br><span class="line">vim ks_app/components/params.libsonnet</span><br></pre></td></tr></table></figure><p>使用vim打开这两个文件, 并搜索<code>/Image\c</code>, 忽略大小搜索所有Image, 并将后面的值修改为SWR上的镜像地址</p><h3 id="怎么找到哪些镜像是你需要替换的镜像"><a href="#怎么找到哪些镜像是你需要替换的镜像" class="headerlink" title="怎么找到哪些镜像是你需要替换的镜像"></a>怎么找到哪些镜像是你需要替换的镜像</h3><p>有两种方法: </p><ol><li>搜索上面那两个文件, 获取其中的镜像列表</li><li>先安装容器, 然后再看什么镜像拉取不下来</li></ol><p>实际上使用第一种会更快一点, 基本上也就包含了所有的镜像</p><h2 id="安装kubeflow组件"><a href="#安装kubeflow组件" class="headerlink" title="安装kubeflow组件"></a>安装kubeflow组件</h2><p>执行一下命令, 在k8s里部署<code>kubeflow</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kfctl apply all -V</span><br></pre></td></tr></table></figure><p>至此, 你以为任务已经完成, 但是一到k8s的页面一看, 发现Pod无法启动, 我们再一个一个解决问题.</p><blockquote><p>kubeflow大多数是deployment, 所以工作负载-&gt;无状态页面上可以看到组件状态</p></blockquote><h3 id="镜像拉取权限"><a href="#镜像拉取权限" class="headerlink" title="镜像拉取权限"></a>镜像拉取权限</h3><p>遇到的第一个问题, 发现Pod的镜像一直无法拉取, 查了半天资料才发现华为云还有这么一个坑爹的限制: <a href="https://support.huaweicloud.com/cce_faq/cce_faq_00015.html">必须显式指定imagePullSecrets</a></p><p><strong>那就解决它吧</strong></p><p>首先执行以下命令获取全部的<code>serviceaccount</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get serviceaccount -n kubeflow</span><br></pre></td></tr></table></figure><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/2.png" alt=""></p><p>其次, 执行以下命令, 给每个<code>serviceaccount</code>添加默认的<code>imagePullSecrets</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kubectl -n kubeflow patch serviceaccount ambassador                             -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span>     </span><br><span class="line">kubectl -n kubeflow patch serviceaccount argo                                   -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span>  </span><br><span class="line">kubectl -n kubeflow patch serviceaccount argo-ui                                -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span>    </span><br><span class="line">kubectl -n kubeflow patch serviceaccount centraldashboard                       -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount default                                -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount jupyter-notebook                       -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount jupyter-web-app                        -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount katib-ui                               -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount meta-controller-service                -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount metrics-collector                      -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline                            -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-persistenceagent           -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-scheduledworkflow          -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-ui                         -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount ml-pipeline-viewer-crd-service-account -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount notebook-controller                    -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount pipeline-runner                        -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount pytorch-operator                       -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount studyjob-controller                    -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount tf-job-dashboard                       -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount tf-job-operator                        -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br><span class="line">kubectl -n kubeflow patch serviceaccount vizier-core                            -p <span class="string">&#x27;&#123;&quot;imagePullSecrets&quot;: [&#123;&quot;name&quot;: &quot;default-secret&quot;&#125;]&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>删除失败Pod, 过段时间后, 大部分的都正常了, 还有部分有问题.</p><h3 id="创建数据库PVC"><a href="#创建数据库PVC" class="headerlink" title="创建数据库PVC"></a>创建数据库PVC</h3><p>这个时候, 你应该能发现, mysql的相关负载一直是黄, 估计是存储有问题, 用下面命令查看一下pvc状态, 果然如你所料</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pvc -n kubeflow</span><br><span class="line">NAME             STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">katib-mysql      Pending                                                      12h</span><br><span class="line">minio-pvc        Pending                                                      12h</span><br><span class="line">mysql-pv-claim   Pending                                                      12h</span><br></pre></td></tr></table></figure><p>那么主动在管理页面上创建三个对应的pvc(图中多了一个notebook的pvc请忽略)</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/3.png" alt=""></p><p>由于CCE的PVC必然是带有<code>cce-sfs-</code>这类的前缀, 因此实际上PVC的名字发生了改变, 需要修改对应的相应的Deployment的yaml配置项</p><p>这点在华为云的页面上直接可以操作: 点开对应的Deployment的配置项, 找到<code>编辑YAML</code>完成任务修改.</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/4.png" alt=""></p><p>重启Deployment之后, 这几个负载也变绿了.</p><h3 id="修改vizier-db配置项"><a href="#修改vizier-db配置项" class="headerlink" title="修改vizier-db配置项"></a>修改vizier-db配置项</h3><p>但是还有一个负载顽强跑不起来, 查看事件发现是, <code>readinessProbe</code>有出错, 发现<code>initialDelaySeconds</code>默认为1s, 启动时间太短, 修改为15秒后, 负载正常启动.<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">readinessProbe:</span></span><br><span class="line">  <span class="attr">exec:</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">/bin/bash</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;-c&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;&quot;&quot;&quot;mysql -D $$MYSQL_DATABASE -p$$MYSQL_ROOT_PASSWORD -e &#x27;</span><span class="string">&#x27;SELECT 1&#x27;</span><span class="string">&#x27;&quot;&quot;&quot;&#x27;</span></span><br><span class="line">  <span class="attr">initialDelaySeconds:</span> <span class="string">**15**</span></span><br><span class="line">  <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">periodSeconds:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">successThreshold:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br></pre></td></tr></table></figure></p><p>至此,  所有Deployment都开始正常运行了</p><h2 id="完结撒花"><a href="#完结撒花" class="headerlink" title="完结撒花"></a>完结撒花</h2><p>最后放一张可视化的页面截图</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/kubeflow/5.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubeflow系列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[题型设计]复杂任务调度设计</title>
      <link href="/2019/06/29/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E8%AE%BE%E8%AE%A1/"/>
      <url>/2019/06/29/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%A4%8D%E6%9D%82%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E8%AE%BE%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近想优化一下业务代码之中的任务调度模块,  我把整体需求理了理, 准备重头开始设计一下.</p><p>请听需求:</p><p>在业务系统里面, 任务提交是一个非常常见的场景, 下面是我提炼的业务系统任务的几个特点:</p><ol><li>任务运行时间较长, 可能会超过一个小时, 肯定会有不同的状态</li><li>任务会有不同类型, 每个类型之间会有一定关联, 但是差别也很多</li><li>一个任务可能会有多个子任务: 子任务是不可再分的</li><li>每个子任务或者任务都可能会失败, 需要进行任务重试, 重试要遵循幂等设计</li><li>子任务会连接其他系统, 中间有可能会有超时异常, 要防止任务堆积</li><li>这里假设任务提交速率不高, 可以单机处理 </li></ol><p>问题如下, 请用伪代码描述:</p><ol><li>设计一个框架来实现该功能, 详细描述里面用到的设计模式</li><li>如果提交速率提高到100个请求/s, 需要分布式化, 请问如何设计</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TBD </tag>
            
            <tag> 面试题设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8s命令参数</title>
      <link href="/2019/06/29/K8s%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0/"/>
      <url>/2019/06/29/K8s%E5%91%BD%E4%BB%A4%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="从Docker说起"><a href="#从Docker说起" class="headerlink" title="从Docker说起"></a>从Docker说起</h2><p>在Docker之中有两个命令行入口定义: <code>ENTRYPOINT</code>和<code>CMD</code>. 两者都是定义容器的启动命令, 两个定义方式也是相同的, 都是一个字符串数组, 类似于</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;/opt/entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;/opt/entrypoint.sh&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>两者的区别在于, 执行<code>docker run</code>后面命令行的时候, <code>CMD</code>的命令直接被覆盖了, 而<code>ENTRYPOINT</code>还依然会被执行</p><blockquote><p>如果想覆盖<code>ENTRYPOINT</code>, 在docker run的时候需要显示指定<code>--entrypoint</code></p></blockquote><p>而且通常情况下, 我们会将<code>ENTRYPOINT</code>和<code>CMD</code>联合使用, 类似这样:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;/opt/entrypoint.sh&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;/home&quot;</span>, <span class="string">&quot;/opt&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>这种情况下, <code>DOCKERFILE</code>直接定义的入口脚本为<code>sh /opt/entrypoint.sh</code>, 但脚本的参数有<code>CMD</code>提供, 用户执行<code>docker run</code>的时候可以通过修改输入, 直接改变容器启动命令参数.</p><h2 id="K8s的命令参数"><a href="#K8s的命令参数" class="headerlink" title="K8s的命令参数"></a>K8s的命令参数</h2><p>在K8s之中, 是没有<code>ENTRYPOINT</code>或者<code>CMD</code>的,  来看一组K8s中典型的任务定义:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">command-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">purpose:</span> <span class="string">demonstrate-command</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">command-demo-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">debian</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;printenv&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&quot;HOSTNAME&quot;</span>, <span class="string">&quot;KUBERNETES_PORT&quot;</span>]</span><br></pre></td></tr></table></figure></p><p>在K8s有两个关键的词: <code>command</code>和<code>args</code>, 这两个函数和DockerFile之中的<code>ENTRYPOINT</code>和<code>CMD</code>的关系如下:</p><div class="table-container"><table><thead><tr><th>分类</th><th>有args</th><th>无args</th></tr></thead><tbody><tr><td>有command</td><td>覆盖Docker内的ENTRYPOINT和CMD</td><td>覆盖Docker内的ENTRYPOINT和CMD</td></tr><tr><td>无command</td><td>使用Docker内的ENTRYPOINT, 覆盖CMD</td><td>使用Docker默认ENTRYPOINT和CMD</td></tr></tbody></table></div><p><strong>推荐使用方式: </strong>  K8s之中不定义<code>command</code>只定义<code>args</code>, 这样做的好处是, K8s层不需要来感知容器内部的入口, 真正的入口由<strong>容器制作者</strong>来确定, 而<code>args</code>则是比较变化的, 就由用户在K8s层输入即可.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo使用技巧记录</title>
      <link href="/2019/06/15/Hexo%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E8%AE%B0%E5%BD%95/"/>
      <url>/2019/06/15/Hexo%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="新环境初始化"><a href="#新环境初始化" class="headerlink" title="新环境初始化"></a>新环境初始化</h2><blockquote><p>用于新环境的安装, 例如刚申请了一个电脑, 假设你配置完毕git和下载安装完毕<a href="http://nodejs.cn/download/">NodeJs</a></p></blockquote><p>执行下面的命令, 完成初始化命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@github.com:SaintBacchus/hexo.git</span><br><span class="line"><span class="built_in">cd</span> hexo</span><br><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure></p><h2 id="如何实现置顶功能"><a href="#如何实现置顶功能" class="headerlink" title="如何实现置顶功能"></a>如何实现置顶功能</h2><p>首先安装hexo插件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure><p>然后在文章的Front-matter头部加入:    <code>top: true</code></p><blockquote><p>出自<a href="http://wangwlj.com/2018/01/09/blog_pin_post/">博客</a></p></blockquote><h2 id="如何实现一个文章多个categories"><a href="#如何实现一个文章多个categories" class="headerlink" title="如何实现一个文章多个categories"></a>如何实现一个文章多个categories</h2><p>多个categories<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="string">Sports</span>]</span><br><span class="line">  <span class="bullet">-</span> [<span class="string">Baseball</span>]</span><br></pre></td></tr></table></figure></p><p>多级categories<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Sports</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Baseball</span></span><br></pre></td></tr></table></figure><br>或者<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span> [<span class="string">Sports</span>,<span class="string">Baseball</span></span><br></pre></td></tr></table></figure></p><p>组合使用:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">categories:</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="string">Sports</span>,<span class="string">Baseball</span>]</span><br><span class="line">  <span class="bullet">-</span> [<span class="string">Play</span>]</span><br></pre></td></tr></table></figure></p><blockquote><p>出自<a href="http://aiellochan.com/2018/02/13/hexo/Hexo-%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0%E5%A4%9A%E4%B8%AA-categories/">博客</a></p></blockquote><h2 id="文章加密"><a href="#文章加密" class="headerlink" title="文章加密"></a>文章加密</h2><p>使用如下命令安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-blog-encrypt</span><br></pre></td></tr></table></figure><p>启动插件, 在根<code>_config.yaml</code>设置:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Security</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="attr">encrypt:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>在每篇文章的Format里面设置:</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># password: 是该博客加密使用的密码</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">Mike</span></span><br><span class="line"><span class="comment"># abstract: 是该博客的摘要，会显示在博客的列表页</span></span><br><span class="line"><span class="attr">abstract:</span> <span class="string">Welcome</span> <span class="string">to</span> <span class="string">my</span> <span class="string">blog,</span> <span class="string">enter</span> <span class="string">password</span> <span class="string">to</span> <span class="string">read.</span></span><br><span class="line"><span class="comment"># message: 这个是博客查看时，密码输入框上面的描述性文字</span></span><br><span class="line"><span class="attr">message:</span> <span class="string">Welcome</span> <span class="string">to</span> <span class="string">my</span> <span class="string">blog,</span> <span class="string">enter</span> <span class="string">password</span> <span class="string">to</span> <span class="string">read.</span></span><br></pre></td></tr></table></figure><blockquote><p>出自<a href="https://github.com/MikeCoder/hexo-blog-encrypt/blob/master/ReadMe.zh.md">Github</a></p></blockquote><h2 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h2><blockquote><p>出自<a href="https://www.jianshu.com/p/514c7792ad10">简书</a></p></blockquote><h2 id="支持HTTPS认证"><a href="#支持HTTPS认证" class="headerlink" title="支持HTTPS认证"></a>支持HTTPS认证</h2><blockquote><p>出自<a href="https://molunerfinn.com/hexo-travisci-https/#%E5%8A%A0%E5%85%A5HSTS%E7%9A%84%E5%88%97%E8%A1%A8">博客</a></p></blockquote><h2 id="升级Hexo客户端"><a href="#升级Hexo客户端" class="headerlink" title="升级Hexo客户端"></a>升级Hexo客户端</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo -v</span><br><span class="line">npm update</span><br></pre></td></tr></table></figure><h2 id="升级依赖"><a href="#升级依赖" class="headerlink" title="升级依赖"></a>升级依赖</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">npm install -g npm-check</span><br><span class="line">npm-check</span><br><span class="line">npm install -g npm-upgrade</span><br><span class="line">npm-upgrade</span><br><span class="line">npm update -g</span><br><span class="line">npm update --save</span><br></pre></td></tr></table></figure><h2 id="图片居中显示"><a href="#图片居中显示" class="headerlink" title="图片居中显示"></a>图片居中显示</h2><p>在图谱插入的前面加入HTML前缀<code>&lt;div align=center&gt;</code>, 使用方式如下:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div align=center&gt;![](https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/aboutme/1.png)</span><br></pre></td></tr></table></figure></p><h2 id="修改默认Front-matter"><a href="#修改默认Front-matter" class="headerlink" title="修改默认Front-matter"></a>修改默认Front-matter</h2><p>在根目录下有<code>/scaffolds/post.md</code>的文件用来定义默认的文章格式</p><p>在此键入以下模板</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> &#123;&#123; <span class="string">title</span> &#125;&#125;</span><br><span class="line"><span class="attr">date:</span> &#123;&#123; <span class="string">date</span> &#125;&#125;</span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="attr">typora-root-url:</span> <span class="string">..</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[题型设计]字符串相似度匹配</title>
      <link href="/2019/06/15/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D/"/>
      <url>/2019/06/15/%E9%A2%98%E5%9E%8B%E8%AE%BE%E8%AE%A1-%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>近段时间接触了基因比对算法, 业界最常使用的是BWA算法, 我现在还不了解具体如何实现的, 这里只是准备把算法用计算机的语言描述一下.</p><p>请听题:</p><p>假设有一长字符串, 有以下特征:</p><ol><li>字符串长度大约为30亿</li><li>所有字符由以下五种组成: <code>ATGCN</code>, 前面为4钟碱基, 最后一个为未知碱基</li><li>该字符串为确定字符串, 即所有位置都已经正确</li></ol><p>另外有一短字符串, 短字符串有如下特性:</p><ol><li>长度为100或者150左右的固定长度字符</li><li>字符串也有<code>ATGC</code>组成</li><li>由实验误差原因, 已知该字符串每个位置都可能出现<code>p</code>的概率误差, 称之为<code>测序误差</code></li><li>由于生物学的缘由,  会出现字符出错, 也有出现字符丢失, 或者字符增加, 这种误差被称之为<code>突变误差</code></li></ol><p>此外, 还有一点:</p><ol><li><p>短字符串比较多, 可能长字符串同一个位置会有段短字符串匹配, 也可能没一个匹配到的</p></li><li><p>短序列的起始和结束位置是不固定,</p></li><li><p>目前的高级的测序仪器, 平均所有短序列的覆盖深度为50X, 即总共的字符为30亿*50, 如果短字符长度为100的话, 就有30亿*50/100 = 15亿条.</p></li></ol><p>问题:</p><ol><li>设计一个概念模型来计算字符的相似度, 并说明如何确定匹配阈值, 即达到什么概论, 可以认为没有匹配上.</li><li>设计一个算法能够尽快的计算出所有短序列在长序列之中位置</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TBD </tag>
            
            <tag> 算法 </tag>
            
            <tag> 面试题设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行计算模式</title>
      <link href="/2019/06/14/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%BC%8F/"/>
      <url>/2019/06/14/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>之前一直在做Spark相关工作, 主要就是做分布式计算相关的内容, 经常会听到一些<code>BSP</code>或者<code>MPP</code>等分布式模式的术语, 每次看过文章有些了解之后, 但是经常会忘记, 因此写个文章记录这些概念.</p><h2 id="BSP-ASP-SSP"><a href="#BSP-ASP-SSP" class="headerlink" title="BSP/ASP/SSP"></a>BSP/ASP/SSP</h2><p>首先要明确的是, 这三个概念主要是为了处理<code>机器学习领域迭代计算</code>提出来的</p><p><strong>BSP</strong>是指迭代过程之中, 必须等待前一轮的迭代全部才能进行下一轮, 每轮之间的等待,被称之为<code>Barrier</code>,  所以才叫做<code>Barrier Synchronous Parallel</code></p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/002.jpg" alt=""><br>而<code>ASP: Asynchronous Synchronous Parallel</code>是另外一个极端的方式, 任何一轮迭代绝对不会等待前面的迭代结果, 这个当然能解决<code>BSP</code>模型里面<strong>慢节点</strong>的问题, 但是它存在的问题就是<strong>速度不一,导致最后的梯度不收敛</strong></p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/001.jpg" alt=""></p><p>那么<code>SSP: Stale Synchronous Parallel</code>就是这两者的折中, 他有一个超参<code>s</code>, 表示最快的迭代和最慢的迭代之间的代差要小于等于S.</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/003.jpg" alt=""></p><p>我们经常听说<code>Spark</code>或者<code>MapReduce</code>是一个BSP模型, 原因在于Spark的实际计算只有一轮迭代, 一轮迭代就需要直接出最终结果, 那么只有BSP才能正确计算完毕.</p><p>所以说, Spark是BSP的一种, 这种说明既对, 他确实有栅栏, 但也不对, 在于它主要描述分布式机器学习流程.</p><p>至于为什么机器学习那种多轮迭代为什么可以进行<code>SSP</code>或者<code>ASP</code>计算, 具体的数学原理可以查看<a href="https://www.zhihu.com/question/264189719">如何理解随机梯度下降</a></p><blockquote><p>细节的内容可以看这篇<a href="https://zhuanlan.zhihu.com/p/29968773">博客</a>, 写的不错, 当然最好直接看<a href="http://papers.nips.cc/paper/4894-more-effective-distributed-ml-via-a-stale-synchronous-parallel-parameter-server.pdf">论文</a></p><h2 id="SMP-NUMA-MPP"><a href="#SMP-NUMA-MPP" class="headerlink" title="SMP/NUMA/MPP"></a>SMP/NUMA/MPP</h2></blockquote><p>这三个概念主要出现在<code>计算机体系结构</code>之中, 主要将多核CPU如何实现并行计算的.</p><ul><li><p>SMP：对称多处理器结构(Symmetric Multi-Processor)</p></li><li><p>NUMA：非一致存储访问结构(Non-Uniform Memory Access)</p></li></ul><p>这两个经常在课文里面见到, 大学里面也学过, 大致的体系结构如下图所示:</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/004.png" alt=""><br><code>SMP</code>和<code>NUMA</code>都是单机多核, <code>MPP</code>我理解就应该多台机器了</p><blockquote><p>MPP：和NUMA不同，MPP提供了另外一种进行系统扩展的方式，它由多个SMP服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个SMP服务器(每个SMP服务器称节点)通过节点互联网络连接而成，每个节点只访问自己的本地资源(内存、存储等)，是一种完全无共享(Share Nothing)结构，因而扩展能力最好，理论上其扩展无限制。</p></blockquote><p>可能的示意图如下所示:</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/005.png" alt=""></p><p>除了体系结构, 这三个概念还经常用于描述数据库的不同架构, 例如:</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/006.png" alt=""></p><p>SMP是Shared Everthting的方式</p><p>NUMA是Shared Storage的方式</p><p>MPP是Shared Nothing的方式</p><p>按照这类分法, Spark是数据Shared Storage的方式, 因此数据放在HDFS可以共享获取.</p><p><strong>MPP架构有如下特点：</strong></p><ul><li>Share Nothing、节点之间数据不共享，只有通过网络连接实现的协同</li><li>每个节点有独立的存储和内存</li><li>数据根据某种规则(如Hash)散布到各个节点</li><li>计算任务也是会发布到各个节点并行执行，最后再将结果聚合到整体返回</li><li>用户使用时会看做整体</li><li>MPP数据库（如GreePlum）往往优先考虑C一致性，然后是A可用性，最后考虑P分区容忍</li><li>MPP架构目前被并行数据库广泛采用，一般通过scan、sort和merge等操作符实时返回查询结果</li></ul><p><strong>MPP架构劣势</strong></p><ul><li>很难高可用 -&gt; 影响可用性和可靠性  因为数据按某种规则如HASH已经散布到了各个节点上。</li><li>节点数 =任务并行数 -&gt; 影响扩展性 一个作业提交时，每个节点都要执行相同任务。而不像MapReduce那样做了根据实际开销进行任务拆分后散发到有资源的几个节点上。这一点大大影响了MPP架构应用的可扩展性。</li><li>每个客户端同时连接所有节点通信 -&gt; 影响网络 MPP架构每个节点独立，所以客户端往往需要连接所有节点进行通信，这使得网络也成为瓶颈。</li><li>分区容错性差 前面提到过MPP主要考虑CA，最次才是P。那么一旦扩展节点太多后，元数据管理十分困难。</li></ul><p><strong>MPP 适用场景</strong></p><ul><li>集群规模100以内、并发小（50以下）</li><li>MPP架构目前被并行数据库广泛采用，一般通过scan、sort和merge等操作符实时返回查询结果</li></ul><p><strong>MPP的典型架构</strong></p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/parallel/007.png" alt=""></p><blockquote><p> 参考<a href="https://www.jianshu.com/p/720df3b85b3a">博客1</a>, <a href="https://blog.csdn.net/baichoufei90/article/details/84328666">博客2</a>, <a href="https://rainforc.iteye.com/blog/2217606">博客3</a></p></blockquote><h2 id="MPP数据库-Hadoop数据库"><a href="#MPP数据库-Hadoop数据库" class="headerlink" title="MPP数据库/Hadoop数据库"></a>MPP数据库/Hadoop数据库</h2><p>这个<a href="https://mp.weixin.qq.com/s/scXNfkpjktCZxBg3pYEUUA?">博客</a> 介绍了MPP和Spark数据库的差别, 并提出现在MPP和Batch类数据库的融合, 文章写的很流畅, 读就是了.</p><p>另外特别的要提一下<code>Impala</code>, 这个系统之前做Spark SQL的时候, 和它对标过: 之前团队想要使用Spark SQL替代Impala. 但效果肯定不理想, MPP数据库的时延确实比Spark好太多 了.</p><p>这篇介绍Impala的<a href="https://www.cnblogs.com/Rainbow-G/articles/4282444.html">文章</a>不错, 可以好好看一下</p><p><strong>这个章节, 回头再理理, 需要详细写一下</strong></p><h2 id="Poll-Push模式"><a href="#Poll-Push模式" class="headerlink" title="Poll/Push模式"></a>Poll/Push模式</h2><p>这个是<code>Shuffle</code>处理的概念, 经常出现在流处理系统的概念表里面, 例如Kafka/Flink.<br>在Spark之中, Poll是指ReduceTask主动去拉Shuffle的数据, 这种模式容错比较好处理, 数据丢失之后主要重试计算就好了.<br>Push模式是指将MapTask主动将数据push到ReduceTask的节点上,但是我们实际上在MapTask时候比较难以估计ReduceTask位置, 尤其在节点丢失情况下, 所以要实现push模式, 编程量会很多.</p><p>而在Kafka之中, Push和Poll区别主要在于Consumer是主动获取数据, 还是被动接收数据.</p><p>这个概念还是比较容易理解的.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总体的文章有点乱, 概念越看越模糊, 后续再优化吧.</p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TBD </tag>
            
            <tag> 分布式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>迭代器引导的序列化惨案</title>
      <link href="/2019/06/14/%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%BC%95%E5%AF%BC%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%83%A8%E6%A1%88/"/>
      <url>/2019/06/14/%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%BC%95%E5%AF%BC%E7%9A%84%E5%BA%8F%E5%88%97%E5%8C%96%E6%83%A8%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>今天2月份左右GATK的4.1版本发布, 经过测试目前版本的准确率相对于单机版本也有版本提高, 具体数值见下表</p><center>单机版GATK准确率表</center><div class="table-container"><table><thead><tr><th>单机GATK</th><th>Precision</th><th>Sensitivity</th><th>F-measure</th></tr></thead><tbody><tr><td>SNP</td><td>98.90%</td><td>99.84%</td><td>99.37%</td></tr><tr><td>INDEL</td><td>97.20%</td><td>96.22%</td><td>96.71%</td></tr></tbody></table></div><center>分布式版GATK准确率表</center><div class="table-container"><table><thead><tr><th>分布式GATK</th><th>Precision</th><th>Sensitivity</th><th>F-measure</th></tr></thead><tbody><tr><td>SNP</td><td>98.91%</td><td>99.84%</td><td>99.37%</td></tr><tr><td>INDEL</td><td>97.32%</td><td>96.84%</td><td>96.84%</td></tr></tbody></table></div><p>从图表上看, 虽然目前分布式GATK还是beta特性, 但是准确率已经和单机版很接近, 并且有部分还超越了.</p><center>单机版和分布式性能比较表</center><div class="table-container"><table><thead><tr><th>对比项</th><th>单机版本</th><th>分布式版本</th></tr></thead><tbody><tr><td>耗时</td><td>48H</td><td>3.5H</td></tr></tbody></table></div><blockquote><p>测试数据来源: <a href="http://smash.cs.berkeley.edu/datasets.html">http://smash.cs.berkeley.edu/datasets.html</a></p></blockquote><p>但是, 新版本的GAKT遇到一个严重的BUG, 使用<code>ReadsPipelineSpark</code>的时候, 如果使用<code>hg38</code>的Reference就必然出现<code>StackOverflowError</code></p><blockquote><p>Bug reported @ <a href="https://github.com/broadinstitute/gatk/issues/5869">issues-5869</a></p></blockquote><h2 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h2><p>这个问题有一个麻烦点就是, 从异常栈只能看出是序列化的时候出问题了, 但无法定位哪一行出现的问题, 因此需要首先定界出问题的代码行.</p><h3 id="问题定界"><a href="#问题定界" class="headerlink" title="问题定界"></a>问题定界</h3><p>一般来说, 定界问题有两种:  Debug大法和Print大法. </p><p>Debug大法适合你已经对代码有一定的了解, 并有相应的测试用例支持, 这样做比较事半功倍. </p><p>而Print大法比较适合现在这种情况, 对GATK的源码不是很熟悉, 而且不知道如何构造简单用例复现问题的时候, 这个时候就在<code>ReadsPipelineSpark</code>的代码路径里面, 打满日志, 根据报错前的日志, 定界出错代码位置.</p><p>经过打印了解到, <code>StackOverflowError</code>发生在这个代码段之中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Broadcast&lt;Supplier&lt;AssemblyRegionEvaluator&gt;&gt; <span class="title function_">assemblyRegionEvaluatorSupplierBroadcast</span><span class="params">(</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> JavaSparkContext ctx,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> HaplotypeCallerArgumentCollection hcArgs,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> SAMFileHeader header,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> String reference,</span></span><br><span class="line"><span class="params">        <span class="keyword">final</span> Collection&lt;Annotation&gt; annotations)</span> &#123;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Path</span> <span class="variable">referencePath</span> <span class="operator">=</span> IOUtils.getPath(reference);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">String</span> <span class="variable">referenceFileName</span> <span class="operator">=</span> referencePath.getFileName().toString();</span><br><span class="line">    <span class="keyword">final</span> <span class="type">ReferenceSequenceFile</span> <span class="variable">taskReferenceSequenceFile</span> <span class="operator">=</span> taskReferenceSequenceFile(referenceFileName);</span><br><span class="line">    <span class="keyword">final</span> <span class="type">VariantAnnotatorEngine</span> <span class="variable">annotatorEngine</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">VariantAnnotatorEngine</span>(annotations,  hcArgs.dbsnp.dbsnp, hcArgs.comps, hcArgs.emitReferenceConfidence != ReferenceConfidenceMode.NONE, <span class="literal">false</span>);</span><br><span class="line">    <span class="keyword">return</span> assemblyRegionEvaluatorSupplierBroadcastFunction(ctx, hcArgs, header, taskReferenceSequenceFile, annotatorEngine);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里的大致逻辑是比较清楚的, 就是<code>Driver</code>将一部分信息通过Spark的<code>广播机制</code>发布到<code>Executor</code>里面, 这个会有序列化的动作.</p><p>序列化的对象有: <code>hcArgs</code>,<code>header</code>,<code>taskReferenceSequenceFile</code>,<code>annotatorEngine</code>这四个, 具体是哪一个呢? </p><p>这时候祭出<code>Save-Load</code>大法, 将其中某个值设置null, 再一次次的尝试, 最后发现<code>taskReferenceSequenceFile</code>设置为null的时候, 代码能走过这段逻辑.</p><blockquote><p>当然SL大法在用的时候, 经常被自己的先验知识影响, 当时重点一直在怀疑<code>header</code>和<code>annotatorEngine</code>这两个字段, 一直没想到Reference会有问题, 绕了不少弯路, 因此SL一次的时间还是挺长的.</p></blockquote><h3 id="问题定位f"><a href="#问题定位f" class="headerlink" title="问题定位f"></a>问题定位f</h3><p>上面已经定位出来是<code>ReferenceSequenceFile</code>这个类导致的问题, 那么这个类的哪一部分出问题了呢? 这个时候就要用到Debug大法. </p><p>构造一次测试用例:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparkContext sc;</span><br><span class="line"><span class="type">ReferenceSequenceFile</span> <span class="variable">ref</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">UserDefinedReferenceSequenceFile</span>()</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    sc.broadcast(ref)</span><br><span class="line">&#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    t.printStackTrace();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在<code>UserDefinedReferenceSequenceFile</code>里面不断将其中的字段加入进去, 最后发现以下代码片段导致整个<code>StackOverflowError</code>的问题:</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/iterator_serializable/001.png" alt=""></p><p>就是这个<code>Iterator</code>在序列化的时候, 会不断的递归遍历, 导致栈溢出.<br><code>Iterator</code>的实现为<code>LinkedHashIterator</code>, 里面<code>LinkedHashMap.Entry</code>为一个二叉树, 因此, 需要真实序列化的, 就会不断去遍历整个二叉树, 导致问题整个.</p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/iterator_serializable/002.png" alt=""></p><p><div align=center><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/iterator_serializable/003.png" alt=""></p><h2 id="问题修复方案"><a href="#问题修复方案" class="headerlink" title="问题修复方案"></a>问题修复方案</h2><p>社区已经有解决的<a href="https://github.com/broadinstitute/gatk/pull/5950">方案</a>了,  总体的方式就是不要在<code>Driver</code>加载Reference文件, 而是放在<code>Executor</code>, 这样就能免去序列化的步骤. </p><blockquote><p>社区的问题目的是为了解决内存问题, 但实际上这个是序列化的问题. 此外实际运行的时候, 还有一个内存OMM的问题, 这个并不能够解决.</p></blockquote><h3 id="为什么这个问题值得记录"><a href="#为什么这个问题值得记录" class="headerlink" title="为什么这个问题值得记录"></a>为什么这个问题值得记录</h3><ul><li>首先, 迭代器模式竟然会出现<code>StackOverFlowError</code>, 这个真的没想到.</li><li>其次, 对于陌生代码的定位方式记录一下.</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[转]几道抛硬币问题</title>
      <link href="/2019/06/13/%E8%BD%AC-%E5%87%A0%E9%81%93%E6%8A%9B%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98/"/>
      <url>/2019/06/13/%E8%BD%AC-%E5%87%A0%E9%81%93%E6%8A%9B%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><blockquote><p>首先说明转载, 本文转自<a href="https://www.raychase.net/3144">这里</a></p></blockquote><h3 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h3><blockquote><p>1、平均需要抛掷多少次硬币，才会首次出现连续的两个正面？</p></blockquote><p>假设连续两个正面的期望是 E，那么，先看第一次抛硬币：</p><ol><li>如果抛到反面，那么还期望抛 E 次，因为抛到反面完全没用，总数就期望抛 E+1</li><li>如果抛到正面，那么要看下一次，如果下一次也是正面，那抛硬币就结束了，总数是 2；如果下一次是反面，那么相当于重头来过，总数就期望抛 E+2</li></ol><p>于是可以得到如下关系式：</p><blockquote><p>E = 0.5(E+1) + 0.25*2 + 0.25(E+2)</p></blockquote><p>得到所求期望 E=6</p><p>现在把题目拓展，不是说“连续两个正面”，而是“连续 n 个正面”呢？</p><p>这个问题 Matrix67 有非常有趣的解答 <a href="http://www.matrix67.com/blog/archives/3638">《用数学解赌博问题不稀奇，用赌博解数学问题才牛 B》</a>，下面我简述一下：</p><p>假设有一个赌场，赌博的方式就是猜正反，每来一个玩家来的时候都只带了 1 元，每次都会全部下注，然后赌正面，庄家抛硬币，如果猜错就是全部输掉，如果赢了就得到下注的两倍，玩家会一直玩一直玩直到钱输光；而赌场老板会看，如果有人赢到 2^n 元，就下令关闭赌场。</p><p>于是直到 n 次正面朝上的情况发生，赌场关闭，只有最后那 n 个人才赚到了钱，最后一人得到了 2 元（没算成本价 1 元），倒数第二人是 4 元……倒数第 n 人是 2^n 元，所以，一共得到（等比数列求和）：</p><blockquote><p>2+4+8+…+2^n = 2*(1-2^n)/(1-2) = 2^(n+1) – 2</p></blockquote><p>赌场有多少钱流入，自然就有多少钱流出，所以到赌场倒闭，玩家赢得的钱的总数，就应该等于赌场期望的收入。而因为每个人来的时候都只带了 1 元，因此这个数正好等于期望的人数。于是这就是最终答案。</p><h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><blockquote><p>2、一堆硬币，每天都随便捡一枚抛，如果抛到正面，就把它翻过来；如果抛到反面，就再抛一下，问很长很长时间以后，硬币正面和反面的比例会趋近于多少？</p></blockquote><p>假设正面的比例是 x，那么反面就是 1-x，对于任意一次操作：</p><ul><li>如果抛到正面，那么得到的就一定是反面了；</li><li>如果抛到反面，那么得到正面的可能性为 0.5，反面的也为 0.5。</li></ul><p>所以得到正面的综合起来的概率为：</p><blockquote><p>x<em>0 + (1-x)</em>0.5 = x</p></blockquote><p>所以 x = 1/3，因此硬币正面和反面的比例会趋近于 x/(1-x) = 1/2</p><h3 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h3><blockquote><p>3、连续抛硬币，直到第一次出现连续两次正面为止，恰好抛了 N 次的概率是多少？</p></blockquote><p>考虑“恰好”抛 N 次硬币，到底有多少种情况可以得出最后两次是连续出现了正面，而之前没有出现过连续正面。</p><ul><li>假设 f(x) 表示第一次出现连续正面的时候，已经抛了 x 次，并且整个过程的第一次抛出的结果是反面；</li><li>假设 g(x) 表示第一次出现连续正面的时候，已经抛了 x 次，并且整个过程的第一次抛出的结果是正面。</li></ul><p>所以 f(1)=f(2)=0，g(1)=0，g(2)=1，而当 x&gt;2，</p><ul><li>求 f(x+1)，因为第一次是反面，所以这新添加的第一次不影响结果，因此 f(x+1)=f(x)+g(x)</li><li>求 g(x+1)，因为第一次是正面，必须要保证第二次不能为正，所以 g(x+1)=f(x)</li></ul><p>于是得到：</p><blockquote><p>f(x+2)=f(x+1)+g(x+1)=f(x+1)+f(x)</p><p>g(x+1)=f(x)</p></blockquote><p>其中，求 f(x) 的递推式可以看出 f(x) 是斐波那契数列，根据它的通项公式：</p><p><img src="https://www.raychase.net/wp-content/uploads/2015/07/coin-tossFibonacci_number.png" alt="Fibonacci_number"></p><p>得到 f(N)，也就得到了 g(N)，而总抛的可能性共有 2^N 次方，因此，概率为：</p><blockquote><p>(f(N)+g(N))/2^N</p></blockquote><h3 id="问题四"><a href="#问题四" class="headerlink" title="问题四"></a>问题四</h3><blockquote><p>4、抛硬币 N 次，出现连续 M 次正面的概率是多少？</p></blockquote><p>这个问题也很常见，但是做起来没那么容易，这里有一个 <a href="http://bbs.emath.ac.cn/thread-667-1-1.html">非常详细的讨论过程（链接）</a>，我就不搬过来了。</p><p>结果公式为: $\frac  {F^M_N} {2^N}$</p><h3 id="问题五"><a href="#问题五" class="headerlink" title="问题五"></a>问题五</h3><blockquote><p>5、抛 N 次硬币，正反两面出现次数相同的概率是多少？</p></blockquote><p>其实就是从 N 个硬币的空位中，选出 N/2 个作为正面，余下 N/2 个作为反面，应用组合公式可得到：</p><blockquote><p>C(N,N/2)/2^N=N!/((N-N/2)!(N/2)!)/2^N</p></blockquote><p>继续，</p><blockquote><p>正面出现次数超过反面的概率？</p></blockquote><p>因为正反情况相同，因此正面次数超过反面的概率应当等于反面次数超过正面的概率，因此结果为 1 减去上面那一问的结果之后除以 2：</p><blockquote><p>(1-C(N,N/2)/2^N)/2</p></blockquote><h3 id="MathJax公式书写规范"><a href="#MathJax公式书写规范" class="headerlink" title="MathJax公式书写规范"></a>MathJax公式书写规范</h3><p>$\frac {1 -  \frac {C{^N_{N/2}}} {2^N}}  2 \text {，mathjax最后计算公式示例}$</p><p>参考这篇<a href="https://www.jianshu.com/p/16fbd768bfe7">博客</a> 或者 <a href="https://www.zybuluo.com/codeep/note/163962">这篇</a></p>]]></content>
      
      
      <categories>
          
          <category> 转载文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>加解密技术</title>
      <link href="/2019/06/05/%E5%8A%A0%E8%A7%A3%E5%AF%86%E6%8A%80%E6%9C%AF/"/>
      <url>/2019/06/05/%E5%8A%A0%E8%A7%A3%E5%AF%86%E6%8A%80%E6%9C%AF/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h2><p>本周在向组内培训安全编程规范的原则, 其中遇到了不少加解密算法相关的内容, 之前大致都了解过, 但是经常忘记, 因此今天特地写个博客把其中的内容都记录一下. </p><p>本片博客内容, 参考了不少<a href="https://songlee24.github.io/2015/05/03/public-key-and-private-key/">这篇文章</a>的内容,  有兴趣可以看原博客, 写的比我好.</p><h3 id="对称加密与非对称加密"><a href="#对称加密与非对称加密" class="headerlink" title="对称加密与非对称加密"></a>对称加密与非对称加密</h3><p><strong>对称加密</strong>使用同一个密钥进行加密和解密, 因此这个密钥是不能公开的, 那么密钥的分发就成了一个大问题, 如何在不受信任的网络之中分发密钥, 成为限制<strong>对称加密</strong>算法的痛点.</p><p>而<strong>非对称加密</strong>有两个密钥, 一个称之为<strong>公钥</strong>, 一个称之为<strong>私钥</strong>. <strong>公钥</strong>顾名思义可以在不受信任的网络之中分发, 无论攻击者是否获取了公钥都无法密文.</p><p><strong>非对称加密</strong>要实现这个能力, 需要有以下特点:</p><ol><li>公钥和私钥成对出现, 换而言之: <em>一把公钥有且只有一把私钥, 反之亦然</em></li><li>公钥加密的数据有且只能由对应的私钥解密, 反之亦然</li></ol><h3 id="非对称加密的传输过程"><a href="#非对称加密的传输过程" class="headerlink" title="非对称加密的传输过程"></a>非对称加密的传输过程</h3><ol><li><strong>接收方(解密者, 持有私钥)</strong> 生成一个密钥, 接收方将公钥广播给<strong>发送方(加密者, 持有公钥)</strong></li><li><strong>发送方</strong>将明文字符串安装某种字符串编码格式变成字节流</li><li><strong>发送方</strong>使用公钥对明文数据加密, 再将密文发送给<strong>接收方</strong></li><li><strong>接收方</strong>获得密文, 并使用私钥进行解密, 获得明文字节流</li><li><strong>接收方</strong>按照字符串编码格式加字节流解码为明文字符串</li></ol><p>整个步骤如下图所示(步骤一一对应): </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/encrypt/20150502122610368.jpg" alt=""></p><p>已知在网络公开的数据有两种: 公钥和密文, 由于非对称算法的特性, 公钥无法对密文进行解密,  这样就保证了数据传输的安全性.</p><h3 id="数字签名"><a href="#数字签名" class="headerlink" title="数字签名"></a>数字签名</h3><p>签名, 顾名思义就是告诉别人, 这就是我.  在刚才<strong>非对称加密</strong>传输的例子是公钥持有者发送数据给私钥持有者, 如果反过来, 私钥持有者将数据加密之后发送给公钥持有者, 公钥持有者使用公钥解密数据, 这里由于公钥和数据都是公开的, 那么明文数据是什么已经不重要了, 重要的是<strong>这个文件就是私钥持有者加密的</strong>, 这个就是数字签名. </p><p>它通过非对称加密的原理, 由公钥来验证私钥持有者的身份, 防止密文内容被串改.</p><blockquote><p>当然公钥的安全性是先验的, 也就是说验证者必须知道自己公钥是什么, 而且公钥内容不能被攻击者篡改, 不然给了假的公钥和假的密文, 就往验证者系统注入一个受信内容.</p></blockquote><h3 id="非对称加密算法的缺点"><a href="#非对称加密算法的缺点" class="headerlink" title="非对称加密算法的缺点"></a>非对称加密算法的缺点</h3><p><strong>非对称加密算法</strong>的执行效率比<strong>对称加密算法</strong>慢了2-3个数量级, 如果用非对称加密算法去加密数据性能不能接受, 因此在实际上, 这两者一般结合起来一起使用.</p><p><strong>对称加密算法</strong>的缺点就是<strong>无法安全的传输密钥</strong>, 那么就由<strong>非对称加密算法</strong>传输密钥就好了:</p><ol><li><strong>接收方(持有非对称算法的私钥, 解密者)</strong>通过公钥机制生成一对密钥，一个公钥，一个私钥, 并发送给<strong>发送方(持有对称算法的密钥和非对称算法的公钥, 加密者)</strong></li><li><strong>发送方</strong>将明文字符串安装某种字符串编码格式变成字节流</li><li><strong>发送方</strong>使用非对称算法公钥对对称算法的密钥加密, 获得<strong>加密后的对称密钥</strong></li><li><strong>发送方</strong>使用对称加密的密钥对字节流进行加密</li><li><strong>发送方</strong>将3和4步骤的数据集编码之后发送给<strong>接收方</strong></li><li><strong>接收方</strong>拆分掉这两部分数据: <strong>加密后的对称密钥</strong> 和 <strong>密文</strong></li><li><strong>接收方</strong>用私钥进行解密得到对称算法的密钥。</li><li><strong>接收方</strong>再在用对称加密的密钥对原始的密文进行解密</li><li><strong>接收方</strong>按照字符串编码格式加字节流解码为明文字符串</li></ol><p>整个步骤如下图所示(步骤一一对应): </p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/encrypt/20150502122733376.jpg" alt=""></p><blockquote><p>这个图中, <strong>发送者</strong>和<strong>发送者</strong>只有两次交互, 这应该只是为了画图方便, 实际场景之中, 没必要将密钥和密文一起发送, 密文的传输应该就在其中握手过程完成.</p></blockquote><h3 id="常见的对称和非对称加密算法"><a href="#常见的对称和非对称加密算法" class="headerlink" title="常见的对称和非对称加密算法"></a>常见的对称和非对称加密算法</h3><p><strong>对称加密</strong>最常见的是AES和DES, 但是DES是一种弱加密算法, 不推荐使用<br><strong>不对称加密</strong>常见的有RSA/DSA/SHA256</p><p>秘钥的长度最低为:<br>AES: 128位<br>RSA: 2048位<br>DSA: 1024位<br>SHA: 256位</p><h2 id="非对称加密的数学原理"><a href="#非对称加密的数学原理" class="headerlink" title="非对称加密的数学原理"></a>非对称加密的数学原理</h2><iframe src="//player.bilibili.com/player.html?aid=26639065&cid=45813166&page=1" scrolling="no" border="0" frameborder="no" framespacing="0"  width="640" height="420" allowfullscreen="true"> </iframe><p>直接丟一个B站的视频, 李永乐老师对RSA算法讲解的非常通熟易懂, 一定要看一遍.</p><h2 id="HTTPS认证过程"><a href="#HTTPS认证过程" class="headerlink" title="HTTPS认证过程"></a>HTTPS认证过程</h2><blockquote><p>TBD</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础技术 </tag>
            
            <tag> TBD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[2019-06-01]再看阿甘正传有感</title>
      <link href="/2019/06/01/2019-06-01-%E5%86%8D%E7%9C%8B%E9%98%BF%E7%94%98%E6%AD%A3%E4%BC%A0%E6%9C%89%E6%84%9F/"/>
      <url>/2019/06/01/2019-06-01-%E5%86%8D%E7%9C%8B%E9%98%BF%E7%94%98%E6%AD%A3%E4%BC%A0%E6%9C%89%E6%84%9F/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="缘由"><a href="#缘由" class="headerlink" title="缘由"></a>缘由</h2><p>前几天的时候, 看了&lt;阿甘正传&gt;的视频节目, 昨天晚上抽空又重温了一遍.  </p><p>说起来我已经有看过4遍的&lt;阿甘正传&gt;了:</p><ul><li>第一遍在高中时候看的, 那个时候觉得这个片子特别的爽, 毕竟是一个逆袭的故事嘛, 和大多数的网络小说的套路实际上类型. </li><li>第二遍是在大学的时候看的, 那个时候思绪关心政治历史哲学等内容, 特别关注于电影里面的历史进程, 觉得阿甘真的是历史见证者, 这也是另外一种意淫, 和大多数的穿越小说也类似.  </li><li>第三遍是在毕业之后看的, 那个时候关注电影的构建, 开始注意电影之中的人物塑造, 渐渐去了解了阿甘/珍妮/丹中尉/母亲等角色的性格</li><li>第四遍是昨天的看的, 发现了其中一个之前我一直没关注到的点: <strong>关于命运的思考</strong>. 在珍妮死去之后, 阿甘在墓前的那场戏, 实际上看着整部电影的文眼, 他让我思绪万千, 所以我就打算写个影评记录一下.</li></ul><h2 id="人物剖析"><a href="#人物剖析" class="headerlink" title="人物剖析"></a>人物剖析</h2><p>&lt;阿甘正传&gt;里面实际上描述的人物非常少,  因为整部电影都是以阿甘的视角展开的, 只有珍妮有一部分闪切的镜头来描述她的人生. 下面就依次描述一下对阿甘来说最重要的认.</p><h3 id="阿甘母亲"><a href="#阿甘母亲" class="headerlink" title="阿甘母亲"></a>阿甘母亲</h3><p>阿甘的口头禅是”妈妈曾经说过”, 说明他大部分的人生思考实际来自他妈妈,  她肯定是阿甘人生最重要的人.  她的形象应该是一个<strong>比较完美的乡下母亲</strong>, 当阿甘出生之后整个人生都围绕在阿甘身边:</p><ol><li><p>为让阿甘能上学而被潜规则</p></li><li><p>阿甘大学毕业之后, 以他为荣</p></li><li><p>尊重阿甘的人生选择, 并没有因为阿甘是低能儿而各种安排阿甘的人生.</p></li><li><p>耐心的教育阿甘, 特别是她死亡的时候那个场景, 能够那么平和的面对人生的结局, 并还在教育阿甘如何面对死亡/面对亲人的离去.  我自己有幻想今后如何教育自己的子女如何面对死亡, 因为这是一个无法逃避的问题, 我想阿甘母亲说的, 可能就是最好的注解了.</p></li></ol><p>阿甘的母亲在电影塑造上是非常 符号话的, 只有人物的形象, 并没有人物的血肉.</p><h3 id="丹中尉"><a href="#丹中尉" class="headerlink" title="丹中尉"></a>丹中尉</h3><p>这角色其实和阿甘母亲一样, 只是一个人物符号, 他是完全服务于电影另一个主题的表达: <strong>越战反思</strong>.<br>丹中尉出生一个电影的军人世家, 以为国牺牲为荣, 但是在越战之中受伤, 失去双腿, 身体残疾了; 国内反战严重, 不以军人为荣, 患上了PTSD, 精神也残疾了.<br>在现实之中, 他最大概率死于自杀, 但是由于&lt;阿甘正传&gt;属于正能量电影, 所以后面他正做起来了, 最后还娶了一个越南籍的老婆, 构成了对越战的反讽.</p><h3 id="珍妮"><a href="#珍妮" class="headerlink" title="珍妮"></a>珍妮</h3><p>珍妮是&lt;阿甘正传&gt;里面唯一有单独场景的人物(<em>其他人物都是和阿甘在一起的场景</em>) , 这个人物主要是为了与阿甘的人生做一个对比的, 象征着美国的两种完全不一样的思维方式.</p><p>阿甘是那种典型美国保守派的思路, 而珍妮是所谓的”垮掉的一代”的典型, 嬉皮士/反战/黑人运动, 哪儿有热闹就往哪儿钻. 她性解放/滥交/吸毒, 基本上所有传统道德反对的事情, 她都做.</p><p>但是最后阿甘和珍妮还是在一起了, 生下来了一个小阿甘, 因为阿甘和珍妮都是美国的一个象征, 一体两面. 最后以传统思维的胜利而结束, 因此珍妮就必须死去了, 但是留下了小阿甘, 代表了两种思潮合并后的现代美国, 继续生活下去了.</p><h3 id="阿甘"><a href="#阿甘" class="headerlink" title="阿甘"></a>阿甘</h3><p>和珍妮不同, 阿甘是所有美国保守派对于美好道德的憧憬: <strong>尽管人有缺陷, 但是道德是完美的</strong></p><p>阿甘在电影虽然戏份太多, 但是人物形象基本上很平, 首先是他是一个低能儿, 因此就不可能展现太多的思考, 其次电影太多的让阿甘的人生轨迹贴近历史了, 实际上对于他自身的心里描述很少. </p><p>前面的三个角色都是阿甘人生里面不可或缺的角色: </p><ol><li>母亲是阿甘人生的导师</li><li>丹中尉是阿甘的经济来源, 阿甘捕虾事业实际上由丹中尉再管理</li><li>珍妮是阿甘人生的追求和比对.</li></ol><h2 id="再论珍妮"><a href="#再论珍妮" class="headerlink" title="再论珍妮"></a>再论珍妮</h2><p>上面介绍珍妮的时候，完全是按照电影角色的角度去思考的，虽然我觉得编剧应该就是这么构想的，因为&lt;阿甘正传&gt;算商业电影，这么设计符合受众的口味．但是我决定还是<strong>过度解读</strong>一下, 以人的角度来描述. 珍妮就是珍妮, 阿甘就是阿甘, 他们是在那个时代阿拉巴马州的两个小镇孩子的缩影.</p><h3 id="少年期"><a href="#少年期" class="headerlink" title="少年期"></a>少年期</h3><p>阿甘是个笨蛋, 腿上又有点残疾, 但有一个非常爱他的母亲, 人虽然傻, 好在对世界的认识都很美好.</p><p>而珍妮是个正常人, 但出生在一个不正常家庭, 父亲小时候性侵她, 她最后报警搬到祖母的房车上居住, 她最大希望是逃离这个乡下, 逃离这个童年.</p><h3 id="青年期"><a href="#青年期" class="headerlink" title="青年期"></a>青年期</h3><p>阿甘凭借打橄榄球读了大学了, 并顺利毕业, 毕业后不知道去哪儿就去当兵, 然后去了越南打战.</p><p>珍妮在大学里开始就解放了天性, 梦想当一个歌手, 站在一个瞩目的位置, 但是命运弄人, 只能靠肉体在杂志里出位.</p><h3 id="越战期间"><a href="#越战期间" class="headerlink" title="越战期间"></a>越战期间</h3><p>阿甘就一路开挂, 而珍妮则在参加嬉皮士运动, 反战等, 这段期间可以珍妮已经放弃了人生, 随波逐流, 跟随那些进步青年的口号, 走上了国内战场.</p><h3 id="越战归来"><a href="#越战归来" class="headerlink" title="越战归来"></a>越战归来</h3><p>阿甘开始当上的捕虾船长, 而这个珍妮就真的堕落了, 开始吸毒当妓女, 生活一片黑暗, 甚至想寻死. 珍妮想跳楼的那场戏是塑造人物最重要的点, 让观众了解珍妮不只是享乐主义的人, 她有她自己痛苦, 而且这场戏也是她人生的一个转折点.</p><h3 id="母亲过世"><a href="#母亲过世" class="headerlink" title="母亲过世"></a>母亲过世</h3><p>阿甘母亲过世之后, 阿甘就又回归到小镇生活了, 而珍妮也厌倦了之前的生活, 回到阿甘身边生活了一段时间. 最后阿甘向珍妮求婚之后, 珍妮再一次离别.</p><h3 id="奔跑吧"><a href="#奔跑吧" class="headerlink" title="奔跑吧"></a>奔跑吧</h3><p>之后, 阿甘就开启了长跑之路, 而珍妮也开始回归正常生活, 当起来了单身母亲</p><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>最后珍妮得病, 把小阿甘托付之后, 和阿甘结婚, 马上就过世了. 阿甘就独立带孩子, 完成新的一个轮循环.</p><h3 id="三次离别"><a href="#三次离别" class="headerlink" title="三次离别"></a>三次离别</h3><p>珍妮有三次和阿甘分道扬镳, 一次在越战之前, 一次在越战之后, 最后一次在小镇生活过一段时间之后.</p><p>第一次的时候, 两个人的追求不同, 珍妮希望当明星, 而阿甘没有追求</p><p>第二次的时候, 两个人的人生不同, 珍妮显然看不上阿甘甘于寂寞的人生.</p><p>第三次的时候, 两个人的经历不同, 珍妮有点看不起自己, 另外不想住在greenblow</p><h3 id="为什么要奔跑"><a href="#为什么要奔跑" class="headerlink" title="为什么要奔跑"></a>为什么要奔跑</h3><p>阿甘跑遍美国的时候, 一直解释说自己没有目的, 他确实很没有目的, 但又有目的: 阿甘母亲过世之前, 她一直是阿甘的精神支柱; 她过世之后, 珍妮就成了他人生的唯一意义, 当珍妮来到greenblow的时候, 阿甘觉得是这辈子最幸福的时候, 但当珍妮再次离开的时候, 他觉得整个人生的意义就不存在了. </p><p>其中有一场戏, 阿甘看着空空荡荡的房间, 看见珍妮留在房间的总统勋章, 演技是真的很好: 有一种心死了的感觉. 换一个正常的人, 可能会想着寻死觅活, 而阿甘想到就是珍妮的那句话, 跑.</p><h3 id="珍妮的悲剧"><a href="#珍妮的悲剧" class="headerlink" title="珍妮的悲剧"></a>珍妮的悲剧</h3><p>珍妮和阿甘的悲剧最大的原因在于, 珍妮是比较了解阿甘的, 但是阿甘是完全不了解珍妮的.</p><p>珍妮知道阿甘的痛苦: 当在电视里面看到阿甘在跑的时候, 珍妮就知道阿甘非常痛苦, 她知道阿甘为什么痛苦, 所以准备回到阿甘身边(<em>写信</em>), 也开始关注阿甘的动态(<em>剪报</em>) .</p><p>而阿甘不知道珍妮的痛苦: 珍妮朝老房子扔石头的时候, 阿甘是不知道为什么的, 到最后只知道珍妮讨厌老房子, 就把她推平了</p><h3 id="珍妮的救赎"><a href="#珍妮的救赎" class="headerlink" title="珍妮的救赎"></a>珍妮的救赎</h3><p>珍妮的人生悲剧在于那个悲惨的童年, 她一直想逃离, 但一直都无法逃离. 最后一个离开阿甘的时候, 说道”我没有逃”. 但是我觉得她确实不想逃了, 但是并没有完全放下: 她并不想住在小时候的地方, 而阿甘却一直想住在童年的地方. 所以她第三次的时候, 又逃离了.</p><p>而真正让珍妮放下的是, 她有了下一代, 当人有下一代的时候, 人的责任心就不一样了.</p><p>之前只想着自己的生活, 之后大多数的考虑都围绕在孩子在周围.</p><p><strong>有了下一代的含义, 就是这一代结束了</strong></p><p>当珍妮在剪报的时候, 她已经完成了救赎, 她结束了自己的使命.</p><h2 id="关于命运"><a href="#关于命运" class="headerlink" title="关于命运"></a>关于命运</h2><p>之前说过, 阿甘在珍妮的墓前说:</p><blockquote><p>这一切是命中注定的, 还是命运无常, 他觉得两者都有.</p></blockquote><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gum/Selection_018.png" alt=""></p><p>这里主语不清楚, 我觉得他应该说的是他和珍妮的人生, 命中注定说的是结局, 命运无常说的是过程.</p><p>但我想说的是, 阿甘是命中注定, 而珍妮是命运无常.</p><p>阿甘虽然脑子不太好使, 但是有一个特别能跑的优点, 又有主角光环, 关键还有一个珍妮值得它去爱,  这三个优点无论去掉哪个, 阿甘的人生都暗淡的.</p><p>而珍妮虽然人长得漂亮, 但是出生于一个恶劣的家庭环境, 个人又没有明显的特长, 人生旅途之中又没有一个真正能拯救她的人, 唯一一个爱他的阿甘, 又根本无法了解她的内心, 可以说她一直都是孤独的.</p><p>站在珍妮的角度, 能死在阿甘的怀里, 可能已经是她最幸福的结局了. 她确实是一个可怜的人.</p><p><strong>命运是一种看不见摸不着的东西, 但你想摆脱它, 那又比登天一样难</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&lt;阿甘正传&gt;并非一部非常经得起分析的电影, 特别是关于人物方面的, 但是我还是推荐你们多去看几遍, 因为我知道大多数人是不会去看这种贴近历史的严肃文学的, 例如我看过的&lt;白鹿原&gt;, 也是以历史为背景介绍一个村子人的变化, 虽然着眼点很小, 但是里面的人物确实觉得是真实的人.</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GATK4.0 Spark性能分析</title>
      <link href="/2018/09/08/GATK4-0-Spark%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
      <url>/2018/09/08/GATK4-0-Spark%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇文章讲记录GATK4.0 Spark流程之中性能点的分析.<br>目前我们团队已经把整个参数调优已经做到了极致, 目前已经识别的这些性能优化点只能优化GATK代码方式去调优了, 这也是我们团队接下来的一个重要工作.</p><p>因此, 这篇文章讲包括:</p><ol><li>GATK步骤的主要瓶颈点</li><li>以及造成该瓶颈的原因</li><li>优化建议</li></ol><p>这篇文章不包括:</p><ol><li>具体的GATK调优的参数</li><li>具体的Spark调优的参数</li><li>GATK算法相关瓶颈点(目前能力不够, 以后有机会再分析)</li><li>GATK源码级调优</li></ol><h2 id="流程剖析"><a href="#流程剖析" class="headerlink" title="流程剖析"></a>流程剖析</h2><p>之前已经有介绍<a href="https://saintbacchus.github.io/2018/08/19/%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8CWGS%E5%88%86%E6%9E%90/">GATK4.0 Spark</a>的测序流程, 这里就按照里面的步骤一个一个分析吧.</p><p>分析的样本为NA12878的fastq.gz(98GB)文件, Ref使用HG38(3G), KnownSites使用GATK官网文件, 总大小为8G<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1000g_omni2.5.hg38.vcf</span><br><span class="line">1000g_phase1.snps.high_confidence.hg38.vcf</span><br><span class="line">dbsnp_144.hg38.vcf</span><br><span class="line">hapmap_3.3.hg38.vcf</span><br><span class="line">homo_sapiens_assembly38.known_indels.vcf</span><br><span class="line">mills_and_1000g_gold_standard.indels.hg38.vcf</span><br></pre></td></tr></table></figure></p><h3 id="FastqToSam"><a href="#FastqToSam" class="headerlink" title="FastqToSam"></a>FastqToSam</h3><p>由于FastQ文件是GZ格式的, 而Gzip格式在HDFS上是无法切分的, 因此无法用Spark加速, 所以实际上<code>FastqToSam</code>工具是单机的.<br>这个步骤主要耗时是花费在<code>fastq.gz</code>解压缩之中, 目前我们处理这个步骤需要花费90分钟, 而使用linux的命令解压缩单个文件大约就需要1个小时以上, 因此基本上只要解压缩完就能处理完毕</p><p><strong>现状瓶颈:</strong> fastq.gz的文件解压缩</p><p><strong>瓶颈原因:</strong> <code>FastqToSam</code>调用的是Java的GZip库进行解压的, 因此整个性能和linux gzip命令的性能一样. 而gzip解压缩最大的问题在于是单线程解压的, 因此性能一直不高, 而且机器的CPU利用率也非常低.</p><p><strong>优化建议:</strong> 实际上<code>fastq.gz</code>格式不是linux上那种通用的gzip格式, 而且专门为生物信息领域设计的gzip格式, 因此它实际上可以多线程解压的. 经过测试使用<a href="http://www.htslib.org/download/">bgzip</a>多线程解压, 性能可以从原有的1个小时, 提高到半个小时左右(4线程). 但是目前这个工具似乎只有C语言版本, <code>FastqToSam</code>是用java写的, 如果想利用这个步骤, 可能要用到JNI的技术来实现C语言的加载.</p><h3 id="BwaSpark"><a href="#BwaSpark" class="headerlink" title="BwaSpark"></a>BwaSpark</h3><p>首先看一样,<code>BwaSpark</code>的Spark的任务图<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/BwaSparkJobs.png" alt=""></p><p>总共有2个长时间的Job组成, 一个是<code>collect</code>,一个是<code>save</code></p><p>分析collect过程, 发现每个Task都很小, 绝大部分的时间加载Ref文件, 由于每个Executor都需要拉取5G左右的Ref文件, 当Executor达到50个以上的时候, 将近有250GB的数据需要在集群交互.</p><p>save过程, 是用BWA软件将每个小的Bam文件进行比对, 这个步骤目前无法优化.</p><p><strong>现状瓶颈:</strong> 获取HDFS数据慢</p><p><strong>瓶颈原因:</strong> 每个Executor都需要拉取5G左右的Ref, HDFS无法承受如此高的资源.</p><p><strong>优化建议:</strong> 提前加载Ref的Image的数据, 让整个流程快速开始计算.</p><h3 id="ReadsPipelineSpark"><a href="#ReadsPipelineSpark" class="headerlink" title="ReadsPipelineSpark"></a>ReadsPipelineSpark</h3><p>看一下, <code>ReadsPipelineSpark</code>的Spark的任务图<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/ReadsPipelineSpark_Jobs.png" alt=""></p><p>这个时间需要消耗在<code>BaseRecalibratorSpark</code>和<code>VariantsSparkSink</code>,前者是BQSR的计算流程, 后者是<code>HaplotypeCallerSpark</code>的流程.</p><p><code>HaplotypeCallerSpark</code>只消耗了32min符合预期, 但是BQSR流程耗时1.2小时, 时间慢的不正常.</p><p><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/BQSR_Jobs.png" alt=""><br>打开最慢的那个步骤的日志, 发现加载KnownSites文件将近耗费了25分钟, 整个流程被阻塞近半个小时.<br><img src="https://carlmartin-pic-1305764798.cos.ap-chengdu.myqcloud.com/hexo/images/gatk_perf/BQSR_Logs.png" alt=""></p><p><strong>现状瓶颈:</strong> KnownSites文件加载过慢</p><p><strong>瓶颈原因:</strong> 每个Exeuctor都要计算并加载整个KnownSites</p><p><strong>优化建议:</strong> 提前加载KnownSites的数据, 让整个流程快速开始计算.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前对于Spark WGS流程的优化点, 有以下两个:</p><ol><li>使用bgzip加速FastQ转化Bam</li><li>将整个GATK作为Service, 提前加载好所有的资源文件, 客户端提交工具请求, 服务端直接开始计算. 作为完全的Serviceless的服务.</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GATK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Spark进行WGS分析</title>
      <link href="/2018/08/19/%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8CWGS%E5%88%86%E6%9E%90/"/>
      <url>/2018/08/19/%E4%BD%BF%E7%94%A8Spark%E8%BF%9B%E8%A1%8CWGS%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="GATK简介"><a href="#GATK简介" class="headerlink" title="GATK简介"></a>GATK简介</h2><p>GATK全称叫做: Genome Analysis Toolkit. 是Broad Institute开发的用于二代重测序数据分析的一款软件.<br>目前主要用于人类的<strong>WGS</strong>以及<strong>WES</strong>基因测试流程, 具体流程介绍可以看官网的<a href="https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145">最佳实践</a>  </p><p>GATK3版本之前, 一直都是单机版本, 性能一直是瓶颈点, 做完一个WGS的流程大约需要3天时间. 因此在GATK4以后的版本之中, 引入Spark做分布式性能优化, GATK4.0版本可以讲整个WGS测序流程的时间压缩在半天之内, 性能提高将近10倍有余.</p><p>但是, 目前所有标注有Spark加速的工具都是<code>BETA Tool</code>, 虽然就我们测试来看敏感度和准确性都和单机版本没有太大区别, 但是由于整理功能开发阶段, 工具接口可能会调整, 因此如果想应用到生产系统上的话, 也请慎重选择.</p><blockquote><p>WGS: Whole Genome Sequencing 全基因组测序</p><p>WES: Whole Exome Sequencing  全外显子测测序</p><h2 id="WGS流程简介"><a href="#WGS流程简介" class="headerlink" title="WGS流程简介"></a>WGS流程简介</h2><p>在GATK的最佳实践里面, 有流程介绍, 也有<a href="https://github.com/gatk-workflows/gatk4-germline-snps-indels">样例程序</a>供大家参考</p></blockquote><p>但是如果大家之前没有接触过WGS的话, 看官网的介绍还是有点晕. 推荐看一下<strong>碱基矿工</strong>的<a href="http://www.huangshujia.me/2018/02/20/2018-02-20-WGS-Best-Practics.html">GATK4.0和全基因组数据分析实践</a></p><p>好了, 言归正传, 我在这儿简单总结一下WGS的流程:  </p><ol><li>获取数据 — 脱机数据转化成FastQ格式</li><li>数据质控 — 使用<code>Fastqc</code>工具过滤掉低质量的数据</li><li>比对排序 — 使用<code>Bwa + samtools</code>工具对FastQ进行比对排序, 并将格式转化为Bam格式</li><li>碱基去重 — 使用GATK的<code>MarkDuplicates</code>工具完成该步骤</li><li>碱基矫正 — 使用GATK的<code>BQSR</code>工具完成该步骤</li><li>变异检测 — 使用GATK的<code>HaplotypeCaller</code>工具完成该步骤</li><li>变异控制 — 使用GATK的<code>VQSR</code>工具完成该步骤</li></ol><p>最后我们实现的一个功能是将原始的FastQ个数的数据, 转化为VCF格式的数据, 完成整个WGS的流程.</p><blockquote><p>VCF(Variant Call Format): 是种文本格式变异数据的格式, 可以直接用文本编辑器查看里面的字段及数据, 字段含义<a href="http://samtools.github.io/hts-specs/VCFv4.2.pdf">如文所示</a></p></blockquote><h2 id="使用Spark进行WGS分析流程"><a href="#使用Spark进行WGS分析流程" class="headerlink" title="使用Spark进行WGS分析流程"></a>使用Spark进行WGS分析流程</h2><h3 id="流程介绍"><a href="#流程介绍" class="headerlink" title="流程介绍"></a>流程介绍</h3><p>使用Spark进行WGS的流程如下图所示: </p><ol><li>首先FastQ格式的原始数据,通过<code>FastqToSam</code>工具转化为<code>UBam</code>格式</li><li>接着使用<code>BwaSpark</code>方法进行比对, 输出经过比对的<code>Bam</code>格式数据</li><li>最后通过<code>ReadsPipelineSpark</code>进行变异检测,并将变异点输出为<code>VCF</code>格式</li></ol><div id="flowchart-0" class="flow-chart"></div><p><strong>下面讲详细介绍每个工具的用法及命令</strong></p><h3 id="FastqToSam"><a href="#FastqToSam" class="headerlink" title="FastqToSam"></a>FastqToSam</h3><p><em>官网简介</em><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Converts a FASTQ file to an unaligned BAM or SAM file.</span><br><span class="line"></span><br><span class="line">Output read records will contain the original base calls and quality scores will be translated depending on the base quality score encoding: FastqSanger, FastqSolexa and FastqIllumina.</span><br><span class="line"></span><br><span class="line">There are also arguments to provide values for SAM header and read attributes that are not present in FASTQ (e.g see RG or SM below).</span><br></pre></td></tr></table></figure></p><p>这个工具的作用就是做好格式转化, 并对BAM格式进行排序. <strong>这个工具是单机的, 无法使用Spark加速. 官方工具转化一个NA1278 30X的样本大致需要3-4个小时. 我们团队这个工具进行了优化, 使得转化时间提高到1.5小时,大大降低了样本转化的时间</strong></p><p><em>执行命令</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/gatk/gatk FastqToSam -F1 NA12878_1.fastq.gz -F2 NA12878_2.fastq.gz -O NA12878_unaligned.bam -SM SM1 -PL illumina -R hg19.fa</span><br></pre></td></tr></table></figure></p><p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_FastqToSam.php">FastqToSam文档</a></p><h3 id="BwaSpark"><a href="#BwaSpark" class="headerlink" title="BwaSpark"></a>BwaSpark</h3><p>简介<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Align reads to a given reference using BWA on Spark</span><br></pre></td></tr></table></figure></p><p>这个工具本质上是使用HDFS分片的能力, 让Spark对BWA软件分布化. Spark的每个Task都比对一个<em>块大小</em>的<code>uBam</code>. 每个块大小由参数<code>--bam-partition-size</code>指定, 默认值是使用Hadoop默认的块大小.<br><strong>通过Spark的分布式话, 原有的比对时间从4小时,可以降低到1个小时左右.</strong>  </p><p><em>执行命令</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/gatk/gatk BwaSpark -I hdfs://hacluster/gatk/NA12878_unaligned.bam -O hdfs://hacluster/gatk/NA12878_aligned.bam -R hdfs://hacluster/gatk/hg19.2bit --spark-runner SPARK --spark-master yarn-cluster</span><br></pre></td></tr></table></figure></p><p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_bwa_BwaSpark.php">BwaSpark文档</a></p><h3 id="ReadsPipelineSpark"><a href="#ReadsPipelineSpark" class="headerlink" title="ReadsPipelineSpark"></a>ReadsPipelineSpark</h3><p>简介<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Takes unaligned or aligned reads and runs BWA (if specified), MarkDuplicates, BQSR, and HaplotypeCaller to generate a VCF file of variants</span><br></pre></td></tr></table></figure></p><p>在之前的GATK版本之中, 每个命令都是单独的, 只能通过自己编写脚本的方式讲这些工具集串行起来.<br>而在GATK4.0之中, 想用<code>ReadsPipelineSpark</code>这一个工具, 将整个变异流程全部统一起来, 未来变异检测只要执行这个工具即可.   </p><blockquote><p>实际上<code>ReadsPipelineSpark</code>的流程能够包括<code>BwaSpark</code>的步骤, 但为什么还要特别分开呢? 原因在于: 目前<code>ReadsPipelineSpark</code>的实现之中没有缓存必要的RDD, 导致重计算, 整个性能没有分开计算好. 因此目前分成了两个步骤, 等社区解决了这个问题之后, 可以讲这两个步骤合并. 这也符合社区对于这个工具的定位.</p></blockquote><p><em>执行命令</em><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/gatk/gatk ReadsPipelineSpark -I hdfs://hacluster/gatk/NA12878_aligned.bam -O hdfs://hacluster/gatk/NA12878.vcf -R hdfs://hacluster/gatk/hg19.2bit --known-sites hdfs://hacluster/gatk_ref/dbsnp.vcf --spark-runner SPARK --spark-master yarn-cluster</span><br></pre></td></tr></table></figure></p><p><a href="https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pipelines_ReadsPipelineSpark.php">ReadsPipelineSpark文档</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>GATK4.0 在今年初引入了Spark进行分布式化性能优化, 整个WGS流程的性能由原有的<strong>天</strong>级别降低到<strong>小时</strong>级别, 性能得到极大优化.  </p><p>后续将从这个流程输出,分析一下目前使用Spark之中的性能瓶颈点以及源码级的分析</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="数据集获取"><a href="#数据集获取" class="headerlink" title="数据集获取"></a>数据集获取</h3><h4 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h4><p>输入的数据可以有两种, 一种是<a href="http://www.huangshujia.me/2017/08/12/2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html">FASTQ</a>格式, 另外一种是<a href="http://www.huangshujia.me/2017/11/27/2017-11-27-Begining-WGS-Data-Analysis-BAM-CRAM-And-SAM.html">BAM</a>格式</p><blockquote><p>NA12878是人类基因测试里面最常用的实验数据</p><h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.gz</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.fai</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.fasta.64.alt</span><br></pre></td></tr></table></figure></blockquote><h4 id="knownsites"><a href="#knownsites" class="headerlink" title="knownsites"></a>knownsites</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 获取SNP的Knownsites: 1000G_phase1.snps.high_confidence</span></span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi</span><br><span class="line"></span><br><span class="line"><span class="comment">## 获取Indel的Knownsites: Mills_and_1000G_gold_standard.indels</span></span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz</span><br><span class="line">wget ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Input(FastQ)e=>end: Output(VCF)op1=>operation: FastqToSamop2=>operation: BwaSparkop3=>operation: ReadsPipelineSparkst->op1->op2->op3->e</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script></p>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GATK </tag>
            
            <tag> SPARK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark Streaming Indroduce</title>
      <link href="/2015/10/30/Spark-Streaming-Indroduce/"/>
      <url>/2015/10/30/Spark-Streaming-Indroduce/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="面向人群"><a href="#面向人群" class="headerlink" title="面向人群"></a>面向人群</h3><ul><li>精通Java/Scala编程</li><li>Spark相关使用及编程经验</li><li>了解流应用架构</li></ul><hr><h3 id="简明介绍"><a href="#简明介绍" class="headerlink" title="简明介绍"></a>简明介绍</h3><p>Spark Streaming是一种基于微批量(<em>micro-batch</em>)方式计算和处理实时流数据执行框架。<br>Streaming依托于于Spark执行框架，将连续输入数据按批次切分，通过DStream(Discretized stream)来表征，然后按批次组装为Spark任务，放入Spark任务池中执行。<br><img src="http://spark.apache.org/docs/latest/img/streaming-flow.png" alt=""></p><center>图1 Streaming与Spark关系</center><p>因此基于<strong>micro-batch</strong>的Streaming必然会带有以下特征:</p><ul><li>高扩展性</li><li>高吞吐量</li><li>高可靠性</li><li>高延时性</li></ul><p>目前Streaming已经内置以下多种数据源和输出源的适配器，其中数据源使用比较多的是Kafka和HDFS，输出源一般都为HDFS。<br><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt="Streming输入输出"></p><center>图2 Streming输入输出</center><p>Streaming目前使用的案例并不是特别多，<a href="http://www.infoq.com/cn/news/2014/04/spark-streaming-bidding">Sharethrough</a>和<a href="http://www.infoq.com/cn/news/2015/04/pinterest-memsql-spark-streaming">Pinterest</a>是比较明确Streaming的使用者:).</p><h3 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h3><!-- 架构层设计 --><h4 id="micro-batch"><a href="#micro-batch" class="headerlink" title="micro-batch"></a>micro-batch</h4><p><img src="http://image.slidesharecdn.com/apachestormvs-140811162542-phpapp01/95/apache-storm-vs-spark-streaming-11-638.jpg?cb=1425908462" alt=""></p><center>图3 micro-batch与流与批的关系</center><p>Wiki上并没有关于<code>micro-batch</code>的介绍，甚至在Streaming的paper中也没有提出这个名词，只能从网上别人总结的图来说明。<br>用这个概念能很好的总结Spark Streaming的执行流程，也能很直观的与类似Storm这类<code>Record By Record</code>类型的流系统区别开来。所谓的<code>micro-batch</code>，它的本质是批处理，因此Streaming的执行层是Spark——一个批处理系统：</p><blockquote><p>Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.</p></blockquote><p>因此，Streaming会天然继承Spark的优点：<em>高扩展性</em>、<em>高吞吐量</em>和<em>高可靠性</em>。<br>但是Streaming能够处理处理流式数据的原因在与它的batch相对于正常的Spark应用是非常小的，而且是严格按照时间切分批次。<br><strong>定义：</strong></p><blockquote><p>Streaming将流式数据分为多个<strong>固定时间窗口</strong>中的<strong>单批数据</strong>，并根据<strong>处理逻辑</strong>封装成Spark任务<strong>持续不断</strong>放入Spark中执行。  </p></blockquote><h4 id="JobGenerator"><a href="#JobGenerator" class="headerlink" title="JobGenerator"></a>JobGenerator</h4><p>Streaming需要提供按照<strong>固定时间</strong>产生的<strong>持续不断</strong>的Spark任务，因此在它的实现里，必然会有一个定时器，该定时器每隔<strong>固定时间</strong>生成一个处理该时间窗口数据的Spark任务，我们称这个组件为JobGenerator。<br>由于一些Streaming任务前后数据之间没有关联性，因此在JobGenerator中必须提供一个Spark的任务池，用于多线程的执行Spark的任务。</p><h4 id="Receiver"><a href="#Receiver" class="headerlink" title="Receiver"></a>Receiver</h4><p>Streaming还需要负责接收<strong>固定时间</strong>产生的流式数据，并将这种数据封装为JobGenerator产生Spark任务的输入数据，我们称之为Receiver。<br>由于<em>micro-batch</em>方式，Streaming可以同时处理批数据和流式数据，因此也会存在两种组织形式的Receiver。</p><ul><li>流式数据<br>以SocketReceiver为代表，通过单节点接受流式数据，将数据按批组装为任务输入数据，包括KafkaReceiver、FlumeReceiver等接收型数据。</li><li>批数据<br>以HDFS接口为代表，本身底层数据系统即是分布式数据，数据不需要组装，可以直接被RDD表征，在Streaming主要包括HDFS文件以及DirectKafkaAPI。</li></ul><h4 id="DStreamGraph"><a href="#DStreamGraph" class="headerlink" title="DStreamGraph"></a>DStreamGraph</h4><p>Streaming的API设计与<code>RDD</code>接口相似，RDD通过<code>dependencies_</code>存储自己的处理逻辑，并通过<code>DAGScheduler</code>分解出RDD整个Spark执行的逻辑，而相对应的<code>DStream</code>需要将自己的逻辑<em>翻译</em>为RDD原语，这个翻译过程被称为<code>DStreamGraph</code>。</p><p><img src="http://images.cnitblog.com/blog/287057/201404/231432212018837.jpg" alt=""></p><center>图4 DStream转化RDD</center><!-- API层设计 --><h4 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Iter</span></span><br><span class="line"><span class="type">Source</span>.fromFile(<span class="string">&quot;&quot;</span>).getLines().flatMap(_.split(<span class="string">&quot;,&quot;</span>)).map(x =&gt; (x, <span class="number">1</span>)).foreach(println)</span><br><span class="line"><span class="comment">// RDD</span></span><br><span class="line"><span class="type">SpoarkContext</span>.textFile(<span class="string">&quot;&quot;</span>).flatMap(_.split(<span class="string">&quot;,&quot;</span>)).map(x =&gt; (x, <span class="number">1</span>)).foreach(println)</span><br><span class="line"><span class="comment">// DStream</span></span><br><span class="line"><span class="type">StreamingContext</span>.textFileStream(<span class="string">&quot;&quot;</span>).flatMap(_.split(<span class="string">&quot;,&quot;</span>)).map(x =&gt; (x, <span class="number">1</span>)).foreach(println)</span><br></pre></td></tr></table></figure><p>从上诉代码上看，DStream和RDD的接口都参考Scala的集合API设计，我们可以将迭代器理解为单机上表征数据以及数据转化方式的对象，那么从RDD的定义和实现来看，RDD是在分布式维度上表征数据及数据转化方式的对象。<br><strong>RDD定义：</strong></p><blockquote><p>Resilient Distributed Datasets (RDDs) are fault-tolerant, parallel data structures that let users explicitly persist intermediate results in memory, control their partitioning to optimize data placement, and manipulate them using a rich set of operators.</p></blockquote><p>而DStream可以理解为在时间维度上表征分布式数据及数据转化方式的对象：</p><blockquote><p>A discretized stream or D-Stream groups together <strong>a series of RDDs</strong> and lets the user manipulate them to through various operators. D-Streams provide both stateless operators, such as map, which act independently on each time interval, and stateful operators, such as aggregation over a sliding window, which operate on multiple intervals and may produce intermediate RDDs as state.   </p></blockquote><p>当然，DStream除了上述和scala集合操作对应的API，DStream还包括一些流应用特有的操作，例如</p><ul><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#updatestatebykey-operation">updateStateByKey</a></li><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations">Window Operations</a></li><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#transform-operation">transform operation</a></li></ul><h3 id="实现浅析"><a href="#实现浅析" class="headerlink" title="实现浅析"></a>实现浅析</h3><h3 id="流式语义"><a href="#流式语义" class="headerlink" title="流式语义"></a>流式语义</h3><p>Streaming的语义在<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#definitions">官网</a>已经有所介绍，总结而言：</p><ul><li>数据接收阶段，对于批数据已经实现<code>Exactly once</code>语义，对于流式数据在Spark-1.2以后引入<code>WAL</code>技术，可以保证<code>at-least once</code>语义</li><li>数据转化阶段，依托RDD的可靠性保证，Streaming能保证<code>Exactly once</code>语义</li><li>数据输出阶段，默认的语义只为<code>at-least once</code>，需要用户自己实现<code>Exactly once</code>语义</li></ul><h3 id="简单案例"><a href="#简单案例" class="headerlink" title="简单案例"></a>简单案例</h3><h4 id="HdfsWordCount"><a href="#HdfsWordCount" class="headerlink" title="HdfsWordCount"></a>HdfsWordCount</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;HdfsWordCount&quot;</span>)</span><br><span class="line"><span class="comment">// Create the context</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create the FileInputDStream on the directory and use the</span></span><br><span class="line"><span class="comment">// stream to count words in new files created</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.textFileStream(args(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><h4 id="DirectKafkaWordCount"><a href="#DirectKafkaWordCount" class="headerlink" title="DirectKafkaWordCount"></a>DirectKafkaWordCount</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;DirectKafkaWordCount&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create direct kafka stream with brokers and topics</span></span><br><span class="line"><span class="keyword">val</span> topicsSet = topics.split(<span class="string">&quot;,&quot;</span>).toSet</span><br><span class="line"><span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">&quot;metadata.broker.list&quot;</span> -&gt; brokers)</span><br><span class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](</span><br><span class="line">  ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get the lines, split them into words, count the words and print</span></span><br><span class="line"><span class="keyword">val</span> lines = messages.map(_._2)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>L)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Start the computation</span></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><p>Streaming应用一般有以下步骤：</p><ol><li>根据不同数据源接口获取<code>InputDStream</code>，对于Kafka即<code>KafkaUtils.createDirectStream</code>，对于HDFS即：<code>ssc.textFileStream</code></li><li>通过<code>DStream</code>接口根据业务对<code>InputDStream</code>进行转化：<code>flatMap(_.split(&quot; &quot;)).map(x =&gt; (x, 1L)).reduceByKey(_ + _)</code></li><li><code>DStream</code>输出结果，<code>print</code>和<code>saveAsTextFiles</code>等，如果需要自定义的输出结果，可以使用<code>foreachRDD</code>算子</li><li>调用<code>ssc.start()</code>和<code>ssc.awaitTermination()</code>，启动Streaming的计算</li></ol><p><strong>任务提交</strong><br>使用常规的Spark任务提交应用，例如<code>HdfsWordCount</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/run-example streaming.HdfsWordCount /streaming</span><br></pre></td></tr></table></figure><br>启动任务后，可以在本地上传文本文件到HDFS指定目录下，这时在日志界面上就能看到统计出来的单词条数<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/hadoop fs -put textFie /streaming</span><br></pre></td></tr></table></figure></p><h3 id="实际案例"><a href="#实际案例" class="headerlink" title="实际案例"></a>实际案例</h3><p>公司案例</p><h3 id="学习建议"><a href="#学习建议" class="headerlink" title="学习建议"></a>学习建议</h3><ol><li>首先仔细浏览官网上<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Streaming编程指南</a>，学习Streaming相关概念</li><li>通过官方<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming">样例工程</a>熟悉API接口，并编写编写简单的应用，尝试在集群中运行。</li><li>学有余力的可以开始阅读Streaming源代码，并通过对比代码尝试定位运行过程中出现的问题。</li><li>学习<a href="http://kafka.apache.org/">Kafka</a>相关概念与架构以及其他流式组件</li><li>学习<a href="http://storm.apache.org/">其他流式</a>组件，集思广益</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol><li><a href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf">Spark Paper</a></li><li><a href="https://people.csail.mit.edu/matei/papers/2012/hotcloud_spark_streaming.pdf">Spark Streaming Paper</a></li><li><a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Streaming Programming Guide</a></li><li><a href="http://stanford.edu/~rezab/sparkclass/slides/td_streaming.pdf">Indroduce By Tathagata</a></li><li><a href="http://www.csdn.net/article/2014-01-27/2818282-Spark-Streaming-big-data">大规模流式数据处理的新贵</a></li><li><a href="http://www.slideshare.net/ptgoetz/apache-storm-vs-spark-streaming">Storm与Spark Streaming比较</a></li><li><a href="http://www.cnblogs.com/shenh062326/p/3530092.html">Spark Streaming实时计算框架介绍</a></li><li><a href="http://blog.csdn.net/anzhsoft/article/details/38168025">从Storm和Spark 学习流式实时分布式计算的设计</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术文章 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPARK </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
